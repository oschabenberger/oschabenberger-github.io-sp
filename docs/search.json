[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Programming",
    "section": "",
    "text": "Preface\nWelcome to this short treatise about statistical programming and statistical program packages. This is online material in a series of collections used in teaching data science and statistics at the graduate level. This material on statistical programming accompanies\n\nFoundations of Data Science–Beyond the Numbers (Foundations) and\nStatistical Learning (StatLearning).\n\nFoundations covers fundamental concepts in data science and the data science project life cycle. StatLearning is a comprehensive collection of modules on supervised and unsupervised learning from a statistical perspective.\nAll three “books” were written in Quarto because it combines prose with math and supports multiple programming languages and kernels within the same framework. To learn more about Quarto books visit https://quarto.org/docs/books.\n\nIn statistical programming it is not possible to completely separate concept—validating an algorithm with matrix/vector algebra, for example—from implementation—in R or Python or SAS or SPSS or … . Tools used in statistical programming have changed dramatically over the last decades. When I was in graduate school and when I first taught statistics, SAS was the undisputed king of the hill—with the exception of one course, every graduate course that touched on computing with data was using SAS.\nI joined SAS in 2001 and contributed to the development of statistical algorithms for many years, maintaining and authoring statistical tools such as the MIXED, NLMIXED, GLIMMIX, NLIN, FMM, PLM procedures and working on subsystems used across many procedures.\nSine then, the world of statistical programming has changed—a lot! Open source statistical languages like R and open source programming languages such as Python are leading the way. There is still some room for proprietary languages such as SAS, STATA, SPSS, and tools like JMP (a SAS business unit), Minitab, and others. But that room continues to shrink. Data scientists today predominantly work in R or Python. The modules that follow are using primarily R. The difficulty with presenting multi-lingual programming material is to use the languages fully for all the material, or to decide on certain tools and languages for certain material, which leaves you wanting if the chosen mix does not match your inclination. Fortunately, the basic principles of statistical programming apply regardless of your choice of tool or language. Principles of data visualization and summarization, reproducibility, and version control do not depend on the tool. Once you understand the grammar of graphics, whether you use `ggplot2 in R or the plotnine library in Python is secondary.\nIs R better than Python or is Python better than R? As always, it depends. R was developed as a statistical programming language, not unlike SAS. This is evident in the way it manipulates data, invokes algorithms, formulates statistical models, handles factors, etc. As someone once joked\n\nR is what happens when statisticians design a programming language.\n\nIt should be added that the author of this quote is a committed Pythonista.\nThe same could be said about SAS or SPSS. And that is not all bad. R is designed for statistical programming and is excellent for that purpose.\nPython is a general purpose scripting language that has been fortified to perform domain-specific tasks through libraries. My experience has been that those who come to data science from a statistical background are versed in R, those that come from a non-statistical background are more familiar with Python. You can stick with what you are comfortable with. The concepts in statistical programming are the same, their implementations might differ depending on the tool.\nOne of the more vexing issues I have with Python for statistical programming is that the origin of many popular libraries lies in machine learning. Not that there is anything wrong with it, but the approach to data analytics in statistical modeling and in machine learning is different. Statistical modeling is based on the notion of a data-generating mechanism, the data at hand is a realization of a stochastic process. Statistical properties of the estimated quantities derive from the underlying random process and the particulars of the estimation principle. Machine learning does not appeal to a data-generating mechanism, although it has the concept of “errors”, deviations between prediction and ground truth, which are treated as random variables. The standard output from Python libraries such as scikit-learn often does not produce the quantities statisticians are looking for, sometimes they are not produced by any combination of methods or options. Not having access to standard errors for estimated quantities is a head-scratcher for statisticians.\nOn the other hand, statistics relies heavily on asymptotic properties of estimators, what happens when sample sizes grow to infinity. Machine learning is more concerned with inferences for the data set at hand, not for some imaginary size. That is a healthy approach in my opinion.\n\nOne of the most instructive and insightful things to do in statistics is to compute the quantities you see on software output from scratch. This often uses matrix-vector operations, random number generation, simulation. Your implementation is probably not as efficient as the code in the software library and that is OK. Well-written software rarely takes formulas straight from the paper. The best way of communicating mathematics for human interpretation is not the best way of communicating the finite precision operations to a machine. For example, the eigenvalue decomposition of a symmetric matrix is a straightforward mathematical concept. The matrices of eigenvectors and eigenvalues are extremely important to distill properties of the data, for example in a Principal Component Analysis (PCA). Performing the eigendecomposition in a computer is fraught with precision issues and a more stable approach numerically is to go through a singular value decomposition instead.\nNevertheless, working through the eigendecomposition for a well-behaved data set and comparing the results to say, the princomp function in R is a great exercise and deepens your understanding of PCA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Statistical Computing and Computational Statistics\nThe terms statistical computing and computational statistics are often used interchangeably. Trying to separate them seems like semantic hair splitting. But we shall try.\nThe field of statistics can broadly be categorized into mathematical statistics and computational statistics. The former derives results based on the mathematical-statistical properties of quantities derived from data. The central limit theorem, for example, tells us that if \\(Y_1,\\cdots,Y_n\\) are a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the distribution of \\[\nZ = \\frac{\\overline{Y}-\\mu}{\\sigma/\\sqrt{n}}\n\\] approaches that of a standard Gaussian (\\(G(0,1)\\)) as \\(n\\rightarrow \\infty\\). That is a result in mathematical statistics. We also know that if the \\(Y_i\\) are \\(\\textit{iid } G(\\mu,\\sigma^2)\\), then \\(Z\\) is exactly \\(G(0,1)\\) distributed (for any \\(n\\)) and in that case \\[\nT = \\frac{\\overline{Y}-\\mu}{s/\\sqrt{n}}\n\\tag{1.1}\\]\nfollows a \\(t_{n-1}\\) distribution with \\(n-1\\) degrees of freedom. The quantity \\(s\\) in Equation 1.1 is the standard deviation of the sample, \\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n \\left(Y_i - \\overline{Y}\\right)^2}\n\\]\nTo evaluate the hypothesis \\(H:\\mu = 4\\) based on a random sample from a Gaussian distribution, you apply statistical computing to the mathematical statistical result and evaluate the quantities with the help of a computer and software. The software implementation of the \\(t\\) test needs to compute \\(\\overline{y}\\) and \\(s\\) based on the sample. There are multiple ways of doing that. For example, on a single pass through the data the code might compute: \\[\nS_1 = \\sum_{i=1}^{n^*} y_i \\qquad S_2 = \\sum_{i=1}^{n^*} y_i^2\n\\] for the \\(n^*\\) observations without missing values for \\(y\\). \\(n^*\\), the number of valid samples, is a by-product of this pass. The sample mean and standard deviation are then computed as \\[\n\\overline{y} = \\frac{S_1}{n^*} \\qquad s = \\sqrt{\\frac{1}{n^*-1} \\left(S_2 - S_1^2/n^*\\right)}\n\\] A second approach would be to perform two passes through the data, computing \\(\\overline{y}\\) on the first pass and the sum of the squared differences from the sample mean on the second pass. The third method uses the knowledge that the least-squares estimate in an intercept-only model is the sample mean and uses the lm function in R to fit a model. The three approaches, including the result from the built-in method, are shown in the following code snippet.\nset.seed(1335)\ny &lt;- rnorm(100,mean=3,sd=2)\nn &lt;- length(y)\n\n# Method 1\nS1 &lt;- sum(y)\nS2 &lt;- sum(y^2)\nybar &lt;- S1/n\nsd &lt;- sqrt((S2-S1^2/n)/(n-1))\ncat(\"Sample mean: \", ybar, \" Sample std. dev: \",sd,\"\\n\")\n\nSample mean:  2.913484  Sample std. dev:  1.809064 \n\n# Method 2\nybar &lt;- sum(y)/n\nsd &lt;- sqrt(sum((y-ybar)^2)/(n-1))\ncat(\"Sample mean: \", ybar, \" Sample std. dev: \",sd,\"\\n\")\n\nSample mean:  2.913484  Sample std. dev:  1.809064 \n\n# Method 3\nsummary(lm(y ~ 1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5917 -1.4713  0.0674  0.9708  5.0478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9135     0.1809    16.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.809 on 99 degrees of freedom\nThe Std. Error shown in the summary from lm is the standard deviation of the Estimate, which in turn is \\(\\overline{y}\\). The standard error reported there is thus an estimate of \\[\n\\sqrt{\\text{Var}[\\overline{Y}]} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n# Built-in methods\nmean(y)\n\n[1] 2.913484\n\nsd(y)\n\n[1] 1.809064\nComputational statistics, compared to mathematical statistics, uses computational methods to solve statistical problems. Rather than relying on asymptotic distributional properties of estimators, computational statistics uses the power of computers to derive the actual distribution of estimators. A famous example of a computational statistical method is the bootstrap, a resampling procedure that draws \\(B\\) random samples of size \\(n\\) with replacement from the sample data. You calculate the value of the statistic of interest in each bootstrap sample and build up the distribution of the statistic. At the end of the bootstrap procedure you can summarize this distribution any way you wish, using statistical computing. The next code snippet calculates the bootstrap estimate of the sample mean and reports the center and standard deviation of its distribution after 10, 100, and 1000 bootstrap samples. We know from basic statistics that the theoretical values are \\[\n\\text{E}[\\overline{Y}] = \\mu = 3 \\qquad \\sqrt{\\text{Var}[\\overline{Y}]} = \\frac{\\sigma}{\\sqrt{n}}=0.2\n\\]\nbootstrap &lt;- function(y,B=500) {\n    n &lt;- length(y)\n    S1 &lt;- 0\n    S2 &lt;- 0\n    for (i in 1:B) {\n        bs &lt;- sample(n,n,replace=TRUE)\n        stat &lt;- mean(y[bs])\n        S1 &lt;- S1 + stat\n        S2 &lt;- S2 + stat^2\n    }\n    ybar &lt;- S1/B\n    sd &lt;- sqrt((S2-S1^2/B)/(B-1))\n    return(list(ybar=ybar,sd=sd))\n}\n\nset.seed(6543)\nbootstrap(y,10)\n\n$ybar\n[1] 2.836697\n\n$sd\n[1] 0.1959311\n\nbootstrap(y,100)\n\n$ybar\n[1] 2.950894\n\n$sd\n[1] 0.2039954\n\nbootstrap(y,1000)\n\n$ybar\n[1] 2.910958\n\n$sd\n[1] 0.1813154\nStatistical methods that rely on random number generators to derive statistical distributions belong to computational statistics. Cross-validation, bootstrapping, bagging, and Markov chain Monte Carlo methods, are examples. Computational statistics also includes computer-intensive methods that cannot be solved without the assistance of computers. Examples are numerical optimization for nonlinear problems, artificial neural networks, support vector machines, gradient boosting, and so on.\nBy applying computer science to data, machine learning has dramatically increased the number of techniques that rely on computational methods over theory. One of the most influential papers in the last decade, entitled “Attention is all you need”, by Vaswani et al. (2017), introduced us to the multi-head attention mechanism in encoder-decoder models and laid the foundation for the large-language model revolution that led to GPT, Gemini and others. The paper has been cited over 127,000 times (as of June 2024) and does not contain a single theorem or proof—it is a clinic in applying computational concepts to data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#languages-and-tools",
    "href": "intro.html#languages-and-tools",
    "title": "1  Introduction",
    "section": "1.2 Languages and Tools",
    "text": "1.2 Languages and Tools\nJust like general software and application development, statistical data processing can be done with no-code, low-code, or high-code tools/environments. A high-code environment is a traditional programming environment where developers sling code. A low-code environment allows the assembly of software from pre-built, reusable components using visual drag-and-drop interfaces. A no-code environment allows users with no programming skills to build applications. While low-code platforms require some programming skills to stitch together components, no-code environments do not require any coding skills. This comes at the expense of rigid templates and built-ins that often do not offer enough opportunities for customization.\nThe traditional way to perform statistical analysis is through statistical programming using an integrated development environment (IDE). Some products offer programmatic (high-code) and visual (no/low-code) interfaces (SAS, JMP, Stata, IBM SPSS); they have their own IDEs.\nOver the last decades open-source tools have pushed proprietary tools and solutions toward the edges of the data analytics market. You cannot measure this effect in terms of market share, as this metric is based on revenue numbers, but it is clearly seen in user engagement, community activity, and the anemic growth numbers of some of the commercial alternatives for statistical and machine learning software. If the market grows by 10% year-over-year and your growth is flat, you are losing share of the market.\nThe big winners in this transition are R and Python. The former is a statistical programming language based originally on the S language. The otherwise capable commercial S-Plus product was doomed once open-source R took hold. You can still see references to original S implementations in R documentations today. Popular packages such as MASS (Modern Applied Statistics with S) started as S modules.\nWhile R was designed as a domain-specific programming language for statistical applications, Python is a general-purpose language. Through libraries such as pandas, numpy, scipy, polars, statsmodels, scikit-learn, keras, matplotlib, seaborn and many others, Python has evolved into a highly capable language for data processing. In the area of deep learning and artificial intelligence, Python has become the standard. As you progress on the analytic journey you will find R to be a great language for many statistical needs. As you move into modern machine learning and artificial intelligence you will transition toward Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#statistical-programming-and-software-engineering",
    "href": "intro.html#statistical-programming-and-software-engineering",
    "title": "1  Introduction",
    "section": "1.3 Statistical Programming and Software Engineering",
    "text": "1.3 Statistical Programming and Software Engineering\nA statistical program is a piece of software, but statistical programming is not bona fide software engineering. Data scientists and statistical programmers are sometimes compared to software developers. Yes, they do share certain traits; both are using tools, languages, and frameworks to build complex systems with software. And because the result is software, statistical programmers need to know the principles of good software engineering and how to apply these in the context of a statistical program:\n\nModularity: Separate software into components according to functionality and responsibility (functions, modules, public and private interfaces)\nSeparation of Concerns: Human beings have a limited capacity to manage contexts. Breaking down a larger task into units and abstractions that you can deal with one at a time is helpful. Interface and implementation are separate concerns. Data quality and data modeling are separate concerns. Not in the sense that they are unrelated, low quality data leads to low quality models—garbage in, garbage out. But in the sense that you can deal with data quality prior to the modeling task. Code efficiency (runtime performance) is sometimes listed as an example for separating concerns: write the code first to meet criteria such as correctness and robustness, then optimize the code for efficiency, focusing on the parts of the code the run spends most time in.\nAbstraction: Separate the behavior of software components from their implementation. Look at each component from two points of views: what it does and how it does it. A client-facing API (Application Programming Interface) specifies what a module does. It does not convey the implementation details. By looking at the function interface of prcomp and princomp in R, you cannot tell that one function is based on singular-value decomposition and the other is based on eigenvalue decomposition.\nGenerality: Software should be free from restrictions that limit its use as an automated solution for the problem at hand. Limiting supported data types to doubles and fixed-size strings is convenient, but not sufficiently general to deal with today’s varied data formats (unstructured text, audio, video, etc.). The “Year 2000” issue is a good example of lack of generality that threatened the digital economy: to save memory, years were represented in software products as two-digit numbers, causing havoc when 1999 (“99”) rolled over to “00” on January 1, 2000.\nAnticipation of Change: Software is an automated solution. It is rarely finished on the first go-around; the process is iterative. Starting from client requirements the product evolves in a back and forth between client and developer, each side refining their understanding of the process and the product at each step. Writing software that can easily change is important and often difficult. When software components are tightly coupled and depend on each other, it is unlikely that you can swap out for another without affecting both.\nConsistency: It is easier to do things within a familiar context. Consistent layout of code and user interfaces helps the programmer as well as the user as well as the next programmer. Consistency in code formatting, comments, naming conventions, variable assignments, etc. makes it easier to read and modify code and helps to prevent errors. When you are consistent in initializing all local variables in C functions, you will never have uninitialized variable bugs.\n\nBut there are important differences between statistical programming and general software engineering. These stem to a large part from the inherent uncertainty and unpredictability of the data, the raw material of a statistical program.\n\nInput inherently unpredictable and uncertain. Statistical code is different from non-analytic code in that it is processing an uncertain input. A JSON parser also processes variability, each JSON document is different from the next. Does it not also deal with uncertain input? If the parser is free of bugs, the result of parsing is known with certainty. For example, we are convinced that the sentence “this book is certainly concerned with uncertainty” has been correctly extracted from the JSON file. Assessing the sentiment of the sentence, however, is a data science task: a sentiment model is applied to the text and returns a set of probabilities indicating how likely the model believes the sentiment of the text is negative, neutral, or positive. Subsequent steps taken in the software are based on interpreting what is probable.\nUncertainty about methods. Whether a software developer uses a quicksort or merge sort algorithm to order an array has impact on the performance of the code but not on the result. Whether you choose a decision tree or a support vector machine to classify the data in the array impacts the performance and the result of the code. A chosen value for a tuning parameter, e.g., the learning rate, can produce stable results with one data set and highly volatile results with another.\nRandom elements in code. Further uncertainty is introduced through analytic steps that are themselves random. Splitting data into training and test data sets, creating random folds for cross-validation, drawing bootstrap samples in random forests, random starting values in clustering or neural networks, selecting the predictors in random forests, Monte Carlo estimation, are some examples where data analysis involves drawing random numbers. The statistical programmer needs to ensure that random number sequences that create different numerical results do not affect the quality of the answers. The results are frequently made repeatable by fixing the seed or starting value of the random number generator. While this makes the program flow repeatable, it is yet another quantity that affects the numerical results. It is also a potential source for misuse: “let me see if another seed value produces a smaller prediction error.”\nData are messy. Data contains missing values and can be full of errors. There is uncertainty about how disparate data sources represent a feature (a customer, a region, a temperature) that affects how you integrate the data sources. These sources of uncertainty can be managed through proper data quality and data integration. As a data scientist you need to be aware and respectful of these issues; they can doom a project if not properly addressed. In an organization without a dedicated data engineering team resolving data quality issues might fall on your shoulders. If you are lucky to work with a data engineering team you still need to be mindful of these challenges and able to confirm that they have been addressed or deal with some of them (missing values).\n\nOther differences between statistical programming and software engineering are\n\nThe use of high-level languages. statistical programming uses languages like R and Python. The software is written at a high level of abstraction, calling into existing packages and libraries. Rather than writing your own implementation of a random forest, you use someone else’s implementation. Instead, your concern shifts to how to use the hyperparameters of the random forest to the greatest effect for the particular data set. You can perform statistical programming in C, C++, or Rust. These system-level languages are best for implementing efficient algorithms, that are then called from a higher-level interface in R or Python.\nThe length of the programs. Statistical programs are typically short, a few hundred to a few thousands lines long. While a thousand lines of Python code may sound like much, it is not much compared to the size of large software engineering projects.\nThe programs are often standalone. A single file or module can contain all the code you need for a statistics project. That is good and bad. Good because it is easy to maintain. Bad because we often skip steps of good software hygiene such as documentation and source control.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-in-this-book",
    "href": "intro.html#what-is-in-this-book",
    "title": "1  Introduction",
    "section": "1.4 What is in this Book?",
    "text": "1.4 What is in this Book?\nThis book is a crash course in statistical programming based primarily on R and Python. It is not a treatise on computational statistics or statistical computing. It is also not an introductory text on R or Python. There are many excellent resources available for free on both languages. Below is a list of resources I have consulted.\nThe approach will be to solve practical problems in statistical programming and introduce concepts as we go along. Rather than dwelling on data types, operators, flow control, basic functions, and so on, the examples in the text will cover some of these concepts implicitly.\n\nR Resources\n\nR for Data Science, 2nd ed. (Wickham, Cetinkaya-Rundel, and Grolemund 2023).\nAdvanced R (Wickham 2019).\nModern Data Science with R, 2nd ed. (Baumer, Kaplan, and Horton 2021).\nR Graphics Cookbook: Practical Recipes for Visualizaing Data, 2nd ed. (Chang 2018)\nR Markdown: The Definite Guide (Xie, Allaire, and Grolemund 2019)\nR Markdown Cookbook (Xie, Dervieux, and Riederer 2021)\nMastering Software Development in R (Peng, Kross, and Anderson 2020)\n\n\nBasic packages\n\nFind an R Package\ntidyverse\n\ndplyr\nggplot2\nstringr\nreadr\nforcats\n\n\nTable 1.1 lists some commonly used packages for statistical work.\n\n\n\nTable 1.1: Analytic packages commonly used in statistical programming\n\n\n\n\n\n\n\n\n\nPackage Name\nNotes\n\n\n\n\nada\nThe R Package Ada for Stochastic Boosting\n\n\narules\nMining Association Rules and Frequent Itemsets\n\n\nbetareg\nBeta Regression\n\n\nbiclust\nBiCluster Algorithms\n\n\nBMA\nBayesian Model Averaging\n\n\nboot\nBootstrap Functions\n\n\nclass\nFunctions for Classification\n\n\ncluster\n“Finding Groups in Data”: Cluster Analysis Extended Rousseeuw et al.\n\n\ncaret\nClassification and Regression Training\n\n\ndata.table\nExtension of data.frame\n\n\nDBI\nR Database Interface\n\n\ndbplyr\nA ‘dplyr’ Back End for Databases\n\n\ne1071\nMisc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien\n\n\nfactoextra\nExtract and Visualize the Results of Multivariate Data Analyses\n\n\ngam\nGeneralized Additive Models\n\n\ngbm\nGeneralized Boosted Regression Models\n\n\nglmnet\nLasso and Elastic-Net Regularized Generalized Linear Models\n\n\nkeras\nR Interface to ‘Keras’\n\n\nKernSmooth\nFunctions for Kernel Smoothing Supporting Wand & Jones (1995)\n\n\nkohonen\nSupervised and Unsupervised Self-Organising Maps\n\n\nlattice\nTrellis Graphics for R\n\n\nleaps\nRegression Subset Selection\n\n\nlmtest\nTesting Linear Regression Models\n\n\nlubridate\nMake Dealing with Dates a Little Easier\n\n\nMASS\nSupport Functions and Datasets for Venables and Ripley’s MASS\n\n\nMatrix\nSparse and Dense Matrix Classes and Methods\n\n\nmclust\nGaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation\n\n\nmgcv\nMixed GAM Computation Vehicle with Automatic Smoothness Estimation\n\n\nmlogit\nMultinomial Logit Models\n\n\nnlme\nLinear and Nonlinear Mixed Effects Models\n\n\nnls2\nNon-Linear Regression with Brute Force\n\n\nplotly\nCreate Interactive Web Graphics via ‘plotly.js’\n\n\npROC\nDisplay and Analyze ROC Curves\n\n\nrandomForest\nBreiman and Cutler’s Random Forests for Classification and Regression\n\n\nReinforcementLearning\nModel-Free Reinforcement Learning\n\n\nreticulate\nInterface to Python\n\n\nRfast\nA Collection of Efficient and Extremely Fast R Functions\n\n\nrpart\nRecursive Partitioning and Regression Trees\n\n\nSHAPforxgboost\nSHAP Plots for XGBoost\n\n\nstatmod\nStatistical Modeling\n\n\nspatial\nFunctions for Kriging and Point Pattern Analysis\n\n\nsplines\nRegression Spline Functions and Classes\n\n\ntensorflow\nR Interface to TensorFlow\n\n\ntree\nClassification and Regression Trees\n\n\ntseries\nTime Series Analysis and Computational Finance\n\n\nxgboost\nExtreme Gradient Boosting\n\n\n\n\n\n\n\n\n\nPython Resources\n\nPython for Data Analysis: Data Wrangling with pandas, NumPy and Jupyter, 3rd ed. (McKinney 2022)\nPython Data Science Handbook (VanderPlas 2016)\n\n\nBasic libraries\n\nNumPy User’s Guide\npandas documentation\npolars documentation\n\n\n\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021. Modern Data Science with r, 2nd Ed. Chapman & Hall/CRC Press. https://mdsr-book.github.io/mdsr3e/.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy and Jupyter, 3rd Ed. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook. O’Reilly Media. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd Ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown: The Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R Markdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "2.1 Getting Started with R\nTo get started with R as a statistical programming language you need access to R itself and a development environment from which to submit R code.\nDownload R for your operating system from the CRAN site. CRAN is the “Comprehensive R Archive Network” and also serves as the package management system to add new packages to your installation.\nIf you use VS Code as a development environment, add the “R Extension for Visual Studio” to your environment. We are focusing on RStudio as a development environment here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-getting-started-R",
    "href": "getting_started.html#sec-getting-started-R",
    "title": "2  Getting Started",
    "section": "",
    "text": "TL;DR What you Need\n\n\n\n\n\nTo work with R in this course, you need to be able to run R code, mix it with prose and formulas in a notebook-style environment, and turn program and output into pdf and html files. To accomplish this you will need\n\nR. Download from CRAN\nRStudio. Download RStudio Desktop from Posit\nLaTex. TinyTeX is a small distribution based on Tex Live that works well with R and can be manipulated through the tinytex R package.\n\nYou can skip R and RStudio installs if you do the work in a Posit Cloud account. These are available for free here.\n\n\n\n\n\n\n\nPosit Cloud\nIn today’s cloud world, you can get both through Posit Cloud. Posit is the company behind RStudio, Quarto, and other cool tools. Their cloud offering gives you access to an RStudio instance in the cloud. You can sign up for a free account here. The only drawback of the free account is its limitations in terms of RAM, CPU, execution time, etc. For the work you will be doing in this course, and probably many other courses, you will not exceed the limitations of the free account.\nOnce you have created an account, the workspace is organized the same way as a RStudio session on your desktop.\n\n\nR and RStudio\nRStudio is an integrated development environment (IDE) for R, but supports other languages as well. For example, using Quarto in RStudio, you can mix R, Python, and other code within the same document. Download Rstudio Desktop here.\nThe RStudio IDE is organized in panes, each pane can have multiple tabs (Figure 2.1). The important panes are\n\nSource. The files you edit. These can be R files (.R), Rmarkdown (.Rmd), Quarto (.qmd), or any other text files.\nConsole. Here you can enter R commands directly at the command prompt “&gt;”. This pane also has a Terminal tab for an OS terminal and a Background Jobs tab. The latter is important when you knit documents into pdf or html format.\nEnvitonment. Displays information about the objects created in the R session. You can click on an object for a more detailed look at it in the Viewer.\nHelp. This pane contains many useful tabs, such as a File browse, package information, access to the documentation and help system. Plots generated from the Console or from an R script are displayed in the Plots tab of this pane.\n\n\n\n\n\n\n\nFigure 2.1: RStudio IDE\n\n\n\n\n\nPackage Management\nThe R installation comes with attached base packages, you do not need to install or load those. Any other packages are enabled in a two-step process:\n\nInstall the package\nLoad the package in your R session with the library() command.\n\nInstalling the package is done once, this step adds the package to your system. Loading the library associated with the package needs to be done in every R session. Without loading the library, R cannot find the functions exported by the library.\n\nInstalling standard packages\nA standard R package is made available through the CRAN (Comprehensive R Archive Network) repositories. To install package “foo” from CRAN use\n\ninstall.packages(\"foo\")\n\nTo install multiple packages, specify them as a character vector:\n\ninstall.packages(c(\"foo\",\"bar\",\"foobar\"))\n\nTo uninstall (remove) one or more packages from a system, use the\n\nremove.packages(c(\"foo\",\"bar\"))\n\ncommand.\nPackages are installed by default into the directory given as the first element of the .libPaths() function. On my Mac this is\n\n.libPaths()[1]\n\n[1] \"/Users/olivers/Library/R/arm64/4.3/library\"\n\n\nIf you wish to install a package in a different location, provide the location in the lib=\"\" argument of install.packages(). Note that if you use a non-default location for the package install you need to specify that location when you load the library with the library command.\nTo make the functionality in a package available to your R session, use the library command. For example, the following statements make the dplyr and Rfast functions available.\n\nlibrary(\"dplyr\")\nlibrary(\"Rfast\")\n\nLibraries export functions into the R name space and sometimes these can collide. For example, the Rfast package exports functions knn and knn.cv for \\(k\\)-nearest neighbor and cross-validated \\(k\\)-nearest neighbor analysis. Functions by the same name also exist in the class package. To make it explicit which function to use, prepend the function name with the package name:\n\nRfast::knn()\n\nclass::knn.cv()\n\nTo load a library from a non-standard location, for example, when you installed the package in a special directory by using lib= on install.packages(), you need to specify the lib.loc=\"\" option in the library command.\n\ninstall.packages(\"some_package_name\", lib=\"/custom_path/to/packages/\")\n\nlibrary(\"some_package_name\", lib.loc=\"/custom_path/to/packages/\")\n\nAll available packages in your R environment can be seen with the\n\nlibrary() \n\ncommand.\nLibraries have dependencies and if you want to install all libraries that a given one depends on, choose dependencies=TRUE in the install.packages() call:\n\ninstall.packages(\"randomForest\", dependencies=TRUE)\n\n\n\nInstalling non-standard packages\nA package that is not served by the CRAN repository cannot be installed with install.packages(). The need for this might arise when you want to install a developer-modified version of a package before it lands on CRAN. This can be accomplished with the devtools package. The following statements install “some_package” from GitHub.\n\nlibrary(\"devtools\")\ndevtools::install_github(\"some_package\")\n\nOnce a non-standard package is installed you load it into a session in the same way as a standard package, with the library command.\nYou can see all packages installed on your system with\n\nas.vector(installed.packages()[,\"Package\"])\n\nand the packages loaded into your workspace with\n\n(.packages())\n\n [1] \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"       \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n\nA more detailed breakdown of the packages in groups, along with other information about the session, is available from sessionInfo().\nAs you write more R code and add packages to your system, you will ask yourself “Did I not install that previously?” The following code snippet helps to install only those packages from a list that are not already installed.\n\nlibs_to_load &lt;- c(\"dplyr\", \"readr\", \"magrittr\",\"reshape2\",\"ggplot2\")\nlibs_to_install &lt;- libs_to_load[!libs_to_load %in% installed.packages()]\nfor (lib in libs_to_install) install.packages(lib, dependencies=TRUE)\nsapply(libs_to_load, library, character=TRUE)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\n$dplyr\n [1] \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"       \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n$readr\n [1] \"readr\"        \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"        \n [6] \"dplyr\"        \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"       \n[11] \"datasets\"     \"methods\"      \"base\"        \n\n$magrittr\n [1] \"magrittr\"     \"readr\"        \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\"\n [6] \"Rcpp\"         \"dplyr\"        \"stats\"        \"graphics\"     \"grDevices\"   \n[11] \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n$reshape2\n [1] \"reshape2\"     \"magrittr\"     \"readr\"        \"Rfast\"        \"RcppParallel\"\n [6] \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"        \"stats\"        \"graphics\"    \n[11] \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n$ggplot2\n [1] \"ggplot2\"      \"reshape2\"     \"magrittr\"     \"readr\"        \"Rfast\"       \n [6] \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"        \"stats\"       \n[11] \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"     \n[16] \"base\"        \n\n\n\n\nUnloading a library\nThe easiest way to unload the libraries you loaded in an R session is to restart the session. 😊\nTo unload a library from an R session you can use the detach function with the unload=TRUE option. For example, to remove the randomForest library without restarting the session:\n\ndetach(\"package:randomForest\",unload=TRUE)\n\n\n\n\nSession Information\nIt is a good practice to add at the end of R programs a listing of the environment in which the program executed. This will show others what packages were loaded and their version. If you use the RNG=TRUE option, the random number generators are also reported, more on this in Chapter 8.\nFor this session, the info is as follows:\n\nsinfo &lt;- sessionInfo()\nprint(sinfo,RNG=T)\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nRandom number generation:\n RNG:     Mersenne-Twister \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.4      reshape2_1.4.4     magrittr_2.0.3     readr_2.1.4       \n[5] Rfast_2.1.0        RcppParallel_5.1.7 RcppZiggurat_0.1.6 Rcpp_1.0.11       \n[9] dplyr_1.1.4       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      jsonlite_1.8.8    compiler_4.3.0    tidyselect_1.2.1 \n [5] stringr_1.5.0     parallel_4.3.0    scales_1.3.0      fastmap_1.1.1    \n [9] R6_2.5.1          plyr_1.8.9        generics_0.1.3    knitr_1.45       \n[13] htmlwidgets_1.6.2 tibble_3.2.1      munsell_0.5.0     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.3       utf8_1.2.4        stringi_1.7.12   \n[21] xfun_0.39         cli_3.6.2         withr_2.5.2       digest_0.6.33    \n[25] grid_4.3.0        hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_0.22     glue_1.7.0        fansi_1.0.6       colorspace_2.1-0 \n[33] rmarkdown_2.25    tools_4.3.0       pkgconfig_2.0.3   htmltools_0.5.5  \n\n\nYou can drill down into the details of the information, for example,\n\nsinfo$loadedOnly$rmarkdown\n\nType: Package\nPackage: rmarkdown\nTitle: Dynamic Documents for R\nVersion: 2.25\nAuthors@R: c( person(\"JJ\", \"Allaire\", , \"jj@posit.co\", role = \"aut\"),\n        person(\"Yihui\", \"Xie\", , \"xie@yihui.name\", role = c(\"aut\",\n        \"cre\"), comment = c(ORCID = \"0000-0003-0645-5666\")),\n        person(\"Christophe\", \"Dervieux\", , \"cderv@posit.co\", role =\n        \"aut\", comment = c(ORCID = \"0000-0003-4474-2498\")),\n        person(\"Jonathan\", \"McPherson\", , \"jonathan@posit.co\", role =\n        \"aut\"), person(\"Javier\", \"Luraschi\", role = \"aut\"),\n        person(\"Kevin\", \"Ushey\", , \"kevin@posit.co\", role = \"aut\"),\n        person(\"Aron\", \"Atkins\", , \"aron@posit.co\", role = \"aut\"),\n        person(\"Hadley\", \"Wickham\", , \"hadley@posit.co\", role = \"aut\"),\n        person(\"Joe\", \"Cheng\", , \"joe@posit.co\", role = \"aut\"),\n        person(\"Winston\", \"Chang\", , \"winston@posit.co\", role = \"aut\"),\n        person(\"Richard\", \"Iannone\", , \"rich@posit.co\", role = \"aut\",\n        comment = c(ORCID = \"0000-0003-3925-190X\")), person(\"Andrew\",\n        \"Dunning\", role = \"ctb\", comment = c(ORCID =\n        \"0000-0003-0464-5036\")), person(\"Atsushi\", \"Yasumoto\", role =\n        c(\"ctb\", \"cph\"), comment = c(ORCID = \"0000-0002-8335-495X\", cph\n        = \"Number sections Lua filter\")), person(\"Barret\", \"Schloerke\",\n        role = \"ctb\"), person(\"Carson\", \"Sievert\", role = \"ctb\",\n        comment = c(ORCID = \"0000-0002-4958-2844\")), person(\"Devon\",\n        \"Ryan\", , \"dpryan79@gmail.com\", role = \"ctb\", comment = c(ORCID\n        = \"0000-0002-8549-0971\")), person(\"Frederik\", \"Aust\", ,\n        \"frederik.aust@uni-koeln.de\", role = \"ctb\", comment = c(ORCID =\n        \"0000-0003-4900-788X\")), person(\"Jeff\", \"Allen\", ,\n        \"jeff@posit.co\", role = \"ctb\"), person(\"JooYoung\", \"Seo\", role\n        = \"ctb\", comment = c(ORCID = \"0000-0002-4064-6012\")),\n        person(\"Malcolm\", \"Barrett\", role = \"ctb\"), person(\"Rob\",\n        \"Hyndman\", , \"Rob.Hyndman@monash.edu\", role = \"ctb\"),\n        person(\"Romain\", \"Lesur\", role = \"ctb\"), person(\"Roy\",\n        \"Storey\", role = \"ctb\"), person(\"Ruben\", \"Arslan\", ,\n        \"ruben.arslan@uni-goettingen.de\", role = \"ctb\"),\n        person(\"Sergio\", \"Oller\", role = \"ctb\"), person(given = \"Posit\n        Software, PBC\", role = c(\"cph\", \"fnd\")), person(, \"jQuery UI\n        contributors\", role = c(\"ctb\", \"cph\"), comment = \"jQuery UI\n        library; authors listed in inst/rmd/h/jqueryui/AUTHORS.txt\"),\n        person(\"Mark\", \"Otto\", role = \"ctb\", comment = \"Bootstrap\n        library\"), person(\"Jacob\", \"Thornton\", role = \"ctb\", comment =\n        \"Bootstrap library\"), person(, \"Bootstrap contributors\", role =\n        \"ctb\", comment = \"Bootstrap library\"), person(, \"Twitter, Inc\",\n        role = \"cph\", comment = \"Bootstrap library\"),\n        person(\"Alexander\", \"Farkas\", role = c(\"ctb\", \"cph\"), comment =\n        \"html5shiv library\"), person(\"Scott\", \"Jehl\", role = c(\"ctb\",\n        \"cph\"), comment = \"Respond.js library\"), person(\"Ivan\",\n        \"Sagalaev\", role = c(\"ctb\", \"cph\"), comment = \"highlight.js\n        library\"), person(\"Greg\", \"Franko\", role = c(\"ctb\", \"cph\"),\n        comment = \"tocify library\"), person(\"John\", \"MacFarlane\", role\n        = c(\"ctb\", \"cph\"), comment = \"Pandoc templates\"), person(,\n        \"Google, Inc.\", role = c(\"ctb\", \"cph\"), comment = \"ioslides\n        library\"), person(\"Dave\", \"Raggett\", role = \"ctb\", comment =\n        \"slidy library\"), person(, \"W3C\", role = \"cph\", comment =\n        \"slidy library\"), person(\"Dave\", \"Gandy\", role = c(\"ctb\",\n        \"cph\"), comment = \"Font-Awesome\"), person(\"Ben\", \"Sperry\", role\n        = \"ctb\", comment = \"Ionicons\"), person(, \"Drifty\", role =\n        \"cph\", comment = \"Ionicons\"), person(\"Aidan\", \"Lister\", role =\n        c(\"ctb\", \"cph\"), comment = \"jQuery StickyTabs\"), person(\"Benct\n        Philip\", \"Jonsson\", role = c(\"ctb\", \"cph\"), comment =\n        \"pagebreak Lua filter\"), person(\"Albert\", \"Krewinkel\", role =\n        c(\"ctb\", \"cph\"), comment = \"pagebreak Lua filter\") )\nMaintainer: Yihui Xie &lt;xie@yihui.name&gt;\nDescription: Convert R Markdown documents into a variety of formats.\nLicense: GPL-3\nURL: https://github.com/rstudio/rmarkdown,\n        https://pkgs.rstudio.com/rmarkdown/\nBugReports: https://github.com/rstudio/rmarkdown/issues\nDepends: R (&gt;= 3.0)\nImports: bslib (&gt;= 0.2.5.1), evaluate (&gt;= 0.13), fontawesome (&gt;=\n        0.5.0), htmltools (&gt;= 0.5.1), jquerylib, jsonlite, knitr (&gt;=\n        1.22), methods, stringr (&gt;= 1.2.0), tinytex (&gt;= 0.31), tools,\n        utils, xfun (&gt;= 0.36), yaml (&gt;= 2.1.19)\nSuggests: digest, dygraphs, fs, rsconnect, downlit (&gt;= 0.4.0), katex\n        (&gt;= 1.4.0), sass (&gt;= 0.4.0), shiny (&gt;= 1.6.0), testthat (&gt;=\n        3.0.3), tibble, vctrs, cleanrmd, withr (&gt;= 2.4.2)\nVignetteBuilder: knitr\nConfig/Needs/website: rstudio/quillt, pkgdown\nEncoding: UTF-8\nRoxygenNote: 7.2.3\nSystemRequirements: pandoc (&gt;= 1.14) - http://pandoc.org\nNeedsCompilation: no\nPackaged: 2023-09-15 16:52:22 UTC; yihui\nAuthor: JJ Allaire [aut], Yihui Xie [aut, cre]\n        (&lt;https://orcid.org/0000-0003-0645-5666&gt;), Christophe Dervieux\n        [aut] (&lt;https://orcid.org/0000-0003-4474-2498&gt;), Jonathan\n        McPherson [aut], Javier Luraschi [aut], Kevin Ushey [aut], Aron\n        Atkins [aut], Hadley Wickham [aut], Joe Cheng [aut], Winston\n        Chang [aut], Richard Iannone [aut]\n        (&lt;https://orcid.org/0000-0003-3925-190X&gt;), Andrew Dunning [ctb]\n        (&lt;https://orcid.org/0000-0003-0464-5036&gt;), Atsushi Yasumoto\n        [ctb, cph] (&lt;https://orcid.org/0000-0002-8335-495X&gt;, Number\n        sections Lua filter), Barret Schloerke [ctb], Carson Sievert\n        [ctb] (&lt;https://orcid.org/0000-0002-4958-2844&gt;), Devon Ryan\n        [ctb] (&lt;https://orcid.org/0000-0002-8549-0971&gt;), Frederik Aust\n        [ctb] (&lt;https://orcid.org/0000-0003-4900-788X&gt;), Jeff Allen\n        [ctb], JooYoung Seo [ctb]\n        (&lt;https://orcid.org/0000-0002-4064-6012&gt;), Malcolm Barrett\n        [ctb], Rob Hyndman [ctb], Romain Lesur [ctb], Roy Storey [ctb],\n        Ruben Arslan [ctb], Sergio Oller [ctb], Posit Software, PBC\n        [cph, fnd], jQuery UI contributors [ctb, cph] (jQuery UI\n        library; authors listed in inst/rmd/h/jqueryui/AUTHORS.txt),\n        Mark Otto [ctb] (Bootstrap library), Jacob Thornton [ctb]\n        (Bootstrap library), Bootstrap contributors [ctb] (Bootstrap\n        library), Twitter, Inc [cph] (Bootstrap library), Alexander\n        Farkas [ctb, cph] (html5shiv library), Scott Jehl [ctb, cph]\n        (Respond.js library), Ivan Sagalaev [ctb, cph] (highlight.js\n        library), Greg Franko [ctb, cph] (tocify library), John\n        MacFarlane [ctb, cph] (Pandoc templates), Google, Inc. [ctb,\n        cph] (ioslides library), Dave Raggett [ctb] (slidy library),\n        W3C [cph] (slidy library), Dave Gandy [ctb, cph]\n        (Font-Awesome), Ben Sperry [ctb] (Ionicons), Drifty [cph]\n        (Ionicons), Aidan Lister [ctb, cph] (jQuery StickyTabs), Benct\n        Philip Jonsson [ctb, cph] (pagebreak Lua filter), Albert\n        Krewinkel [ctb, cph] (pagebreak Lua filter)\nRepository: CRAN\nDate/Publication: 2023-09-18 09:30:02 UTC\nBuilt: R 4.3.1; ; 2023-09-18 12:03:34 UTC; unix\n\n-- File: /Users/olivers/Library/R/arm64/4.3/library/rmarkdown/Meta/package.rds \n\n\n\n\nLaTeX (\\(\\LaTeX\\))\n\\(\\LaTeX\\) (pronounced “LAY-tek” or “LAH-tek”) is a high-quality typesetting system; it includes features designed for the production of technical and scientific documents. \\(\\LaTeX\\) is the de facto standard for the communication and publication of scientific documents and is available for free from here.\nIf you are working in mathematics or statistics, you will be producing \\(\\LaTeX\\) documents. You can write equations with other authoring tools as well—even the Microsoft Equation Editor has improved greatly over the years, in part because it now accepts \\(\\LaTeX\\) syntax! \\(\\LaTeX\\) is not a WYSIWYG—what you see is what you get—environment. Instead, you write a plain text file where text is interspersed with \\(\\LaTeX\\) commands. The document is processed (“compiled”) into an output file (usually pdf) by running it through a TeX engine. In other words, you focus on writing the contents of the document with \\(\\LaTeX\\) commands and let the Tex engine take care of converting the commands into a visual appearance.\nRStudio, Rmarkdown, and Quarto support \\(\\LaTeX\\) natively and this makes it very easy to combine text, code, and formulas. For example, to show the probability density function of a G(0,1) random variable in this Quarto document, I typed the \\(\\LaTeX\\) instructions\n$$\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp \n       \\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\n$$\nin the editor. When the document is rendered, these instructions produce \\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\n\\]\nRStudio does not add a \\(\\LaTeX\\) system to your computer, so you need to do that yourself. If you are planning to use \\(\\LaTeX\\) outside of R and RStudio, I recommend installing a full distribution. If you just want to get by with the minimal \\(\\LaTeX\\) needed to add formulas to html and pdf files created from RStudio, then tinytex will suffice.\n\nMacTex: This \\(\\LaTeX\\) distribution contains everything you need for MacOS.\nMicTex: For Windows, Linux, and MacOS\nTex Live: A basic Tex distribution for Windows, Linux, and MacOS.\nTinyTex: A small \\(\\LaTeX\\) distribution based on Tex Live that works well with R. The R package tinytex provides helper functions to work with TinyTex from R/RStudio. If you want to use TinyTex in R, first install the tinytex package\n\ninstall.packages(\"tinytex\")\nand then download and install TinyTex with\ntinytex::install_tinytex()\nBy default, install_tinytex() will fail the install if another \\(\\LaTeX\\) distribution is detected (you can overwrite this behavior with the force= argument of the function).\nYou can check if RStudio/R uses tinytex by executing this command at the prompt:\n\ntinytex::is_tinytex()\n\n[1] FALSE\n\n\nTo author pure \\(\\LaTeX\\) documents on MacOS, I use TexShop from the University of Oregon, available here. TexShop comes with a Tex Live distribution, so installing TexShop is one method of adding LaTeX to your system.\nIf you are new to \\(\\LaTeX\\), the online LaTeX editor Overleaf has excellent tutorials and documentation. For example, this \\(\\LaTeX\\) in 30-minutes tutorial.\nWhen you use \\(\\LaTeX\\) commands in an Rmarkdown or Quarto document, you do not need to start the document with a preamble (\\documentclass() …) or wrap the commands into a \\begin{document} \\end{document} block. You can enter \\(\\LaTeX\\) commands immediately. The most important application of using \\(\\LaTeX\\) with R is to add mathematical expressions to your document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-getting-started-Python",
    "href": "getting_started.html#sec-getting-started-Python",
    "title": "2  Getting Started",
    "section": "2.2 Getting Started with Python",
    "text": "2.2 Getting Started with Python\nTo get started with statistical programming in Python, you need the following:\n\nAccess to a version of Python, typically installed on your computer\nA development environment to write, interpret, and execute Python code. This is frequently some form of notebook interface, for example Jupyter Notebook or Google Colab.\nA package management system to add/update/remove Python libraries on your system.\n\nYou can download any version of Python from here. The latest version as of this writing is Python 3.12.4. Some organizations still use Python 2; because of breaking changes between Python 2 and Python 3 they might not have updated to Python 3. Moving from Python 2 code to Python 3 is time consuming. Running Python 2 these days is a serious red flag. Python 2 has been sunset since January 1, 2020, meaning that there will be no bug fixes, not even for security bugs.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt is a great question to ask a potential employer: what version of Python are you running and how do you manage your default stack of Python libraries?\nIt is very telling if they are still running Python 2 and have not upgraded to Python 3. This organization does not know how to handle technical debt—run like it is the plague.\n\n\n\n\npyenv Version Management\npyenv is a version management tool for Python. It makes it particularly easy to work with multiple Python versions on the same system. The Python ecosystem moves very quickly and you will find yourself in a situation where a particular library requires a different version of Python from the one installed. Running different Python kernels for different projects is an unfortunate reality for many Python developers. With pyenv you can install/uninstall Python versions, you can switch versions globally, per shell or locally (in certain directories), and create virtual environments.\nThe instructions to install pyenv on your system are here. Pay attention to also update shell configurations when you install pyenv. For example, my system uses zsh and my .zshrc file contains the lines (straight from the GitHub documentation)\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\nThe most common pyenv commands I use are\n\npyenv install to install a Python version on the system. For example, pyenv install 3.11.4 will install Python 3.11.4.\npyenv version to see the currently active version of Python\npyenv local ... to set a local (application-specific) version of Python, for example pyenv local 3.9 makes Python 3.9 the version in the applications started from the current (local) directory. Similarly, pyenv shell ... sets the Python version for the shell instance and pyenv global ... sets the Python version globally. You see that the global version of Python can be different from the version active in a particular shell or a directory.\npyenv --help to get help for the pyenv commands\npyenv help commnand_name to get help for a specific pyenv command, for example pyenv help local\n\n\n\nPackage Management\nThe most common management tools used with Python are conda and pip. The two are often seen as equivalent, but they serve different purposes. pip is a Python package manager, you use it to add/update/remove packages from your Python installation. conda is a system package manager that handles much more than Python libraries. You can manage entire development stacks with conda, but not with pip.\nFor example, to add jupyter to your system with conda use\nconda install jupyter\nand with pip use\npip install jupyter\nThere is a misconception that conda and pip cannot be used together on the same system. You can use them together, in fact a great way to manage your environment is to first install and set up conda for your project and to install the packages you need from conda channels. With conda activated, you can use the version of pip that is included with conda to install any required pip dependencies. The important point is that once conda is activated, you use its version of pip.\nCheck\nwhich pip\nto see which version of pip will be called.\nI personally use pip to manage Python packages, but it is not without issues. Managing the dependencies between Python libraries is a special kind of suffering. You install a new package A that happens to have a dependency on an earlier version of package B, which it downgrades upon installation to the earlier version. This can break code that depends on the newer version of package B. Once you realize this you upgrade B to the newer version, making A fail.\n\n\n\nFigure 2.1: RStudio IDE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "reading_data.html",
    "href": "reading_data.html",
    "title": "3  Reading Data",
    "section": "",
    "text": "3.1 Tabular Data\nThe basic function to read tabular data into R is read.table. read.csv is a special case of read.table for CSV files with defaults such as sep=\",\" that make sense for CSV (comma-separated values) files.\nHere is an example of reading a CSV file into R. The stringsAsFactors=TRUE argument requests that all character variables are converted into factors.\npisa &lt;- read.csv(file=\"data/pisa.csv\",stringsAsFactors=TRUE)\npisa[1:10,]\n\n                Country MathMean MathShareLow MathShareTop ReadingMean\n1        Shanghai-China      613          3.8         55.4         570\n2             Singapore      573          8.3         40.0         542\n3  Hong Kong SAR, China      561          8.5         33.7         545\n4        Chinese Taipei      560         12.8         37.2         523\n5                 Korea      554          9.1         30.9         536\n6      Macao SAR, China      538         10.8         24.3         509\n7                 Japan      536         11.1         23.7         538\n8         Liechtenstein      535         14.1         24.8         516\n9           Switzerland      531         12.4         21.4         509\n10          Netherlands      523         14.8         19.3         511\n   ScienceMean      GDPp  logGDPp HighIncome\n1          580   6264.60  8.74267      FALSE\n2          551  54451.21 10.90506       TRUE\n3          555  36707.77 10.51074       TRUE\n4          523        NA       NA         NA\n5          538  24453.97 10.10455       TRUE\n6          521  77145.04 11.25344       TRUE\n7          547  46701.01 10.75152       TRUE\n8          525 149160.76 11.91278       TRUE\n9          515  83208.69 11.32911       TRUE\n10         522  49474.71 10.80922       TRUE\nNote that there are missing values for Chinese Taipei in row 4. This is the result of correctly specifying the contents of the comma-separated file. For this row the entries of the CSV files read\nThe sequence of commas indicates that the values for the corresponding variables are unobserved and will be set to missing.\nBy default, read.csv looks for information about the column names in the first row of the CSV file. If names are not available in the first row, add the header=FALSE option to the function call. Other important options for read.csv and read.table are",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#tabular-data",
    "href": "reading_data.html#tabular-data",
    "title": "3  Reading Data",
    "section": "",
    "text": "On CSV Files\n\n\n\nOK, time for a rant. CSV files are ubiquitous and going through college one could get the impression that most data is stored in CSV format. First, that does not hold for the real world. Second, CSV is a horrible format for data. It does have some advantages\n\nUbiquitous: every data tool can read and write CSV files. It is thus a common format to exchange (export/import) data between tools and applications.\nHuman readable: since the column names and values are stored in plain text, it is easy to look at the contents of a CSV file. When data are stored in binary form, you need to know exactly how the data are laid out in the file to access it.\nCompression: it is easy to compress CSV files.\nExcel: CSV files are easily exported from and imported to Microsoft Excel.\nSimple: the structure of the files is straightforward to understand and can represent tabular data well if the data types can be converted to text characters.\n\nThere are some considerable disadvantages of CSV files, however:\n\nHuman readable: To prevent exposing the contents of the file you need to use access controls and/or encryption. It is not a recommended file format for sensitive data.\nSimple structure: Complex data types such as documents with multiple fields and sub-fields cannot be stored in CSV files.\nPlain text: Some data types cannot be represented as plain text, for example, images, audio, and video. If you kluge binary data into a text representation the systems writing and reading the data need to know how to kluge and un-kluge the information—it is not a recommended practice.\nEfficiency: much more efficient formats for storing data exist, especially for large data sets.\nBroken: CSV files can be easily broken by applications. Examples include inserting line breaks, limiting line width, not handling embedded quotes correctly, blank lines.\nMissing values (NaNs): The writer and reader of CSV files need to agree how to represent missing values and values representing not-a-number. Inconsistency between writing and reading these values can have disastrous consequences For example, it is a bad but common practice to code missing values with special numbers such as 99999 (called “sentinel values”). How does the application reading the file know this is the code for a missing value?\nEncodings: When CSV files contain more than plain ASCII text, for example, emojis or Unicode characters, the file cannot be read without knowing the correct encoding (UTF-8, UTF-16, EBCDIC, US-ASCII, etc.). Storing encoding information in the header section of the CSV file throws off CSV reader software that does not anticipate the extra information.\nMetadata: The only metadata supported by the CSV format are the column names in the first row of the file. This information is optional and you will find CSV files without column names. Additional metadata common about columns in a table such as data types, format masks, number-to-string maps, cannot be stored in a CSV file.\nData Types: Data types need to be inferred by the CSV reader software when scanning the file. There is no metadata in the CSV header to identify data types, only column names.\nLoss of Precision: Floating point numbers are usually stored in CSV files with fewer decimal places than their internal representation in the computer. A double-precision floating point number occupies 64-bits (8 bytes) and has 15 digits of precision. Although it is not necessary, floating-point numbers are often rounded or truncated when they are converted to plain text.\n\nDespite these drawbacks, CSV is one of the most common file formats. It is the lowest common denominator format to exchange data between disparate systems.\n\n\n\n\n\n\"Chinese Taipei\",560,12.8,37.2,523,523,,,\n\n\n\nsep: specifies the character that separates the values of the columns, for read.csv this defaults to sep=\",\". For read.table the separator defaults to sep=\"\".\nskip: how many lines to skip at the top of the file before reading data. This is useful if the CSV file has a comment section at the top.\nnrow: how many rows of data to read. Useful if you want to import only a portion of the file.\ndec: specifies the character that indicates a decimal point.\nna.strings: a vector of character values that are interpreted as missing (NA) values. I hope you do not need to specify those—using sentinel values to indicate missing values is very dangerous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#excel-files",
    "href": "reading_data.html#excel-files",
    "title": "3  Reading Data",
    "section": "3.2 Excel Files",
    "text": "3.2 Excel Files\nThe readxl library is part of the tidyverse.\n\nlibrary(readxl)\ndf_tesla &lt;- read_excel(\"data/TeslaDeaths.xlsx\", sheet=1)\ndf_tesla[1:10,]\n\n# A tibble: 10 × 26\n   `Case #`  Year Date  Country State Description          Deaths `Tesla driver`\n      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;         \n 1     426   2024 45457 USA     CA    Scooter hit               1 -             \n 2     425   2024 45454 USA     CA    Tesla flips               3 -             \n 3     424   2024 45452 USA     CA    Pedestrian hit            1 -             \n 4     423.  2024 45451 USA     FL    Motorcycle hits Tes…      1 -             \n 5     423   2024 45446 USA     TX    Tesla hits parked t…      1 1             \n 6     422   2024 45444 USA     MD    Tesla hit motorcycle      1 -             \n 7     421   2024 45441 USA     WA    Tesla runs red ligh…      1 -             \n 8     420   2024 45438 USA     FL    Tesla crashes into …      1 1             \n 9     419   2024 45436 Germany -     Tesla crashes under…      1 1             \n10     418   2024 45435 UK      -     Three-way crash           1 1             \n# ℹ 18 more variables: `Tesla occupant` &lt;chr&gt;, `Other vehicle` &lt;chr&gt;,\n#   `Cyclists/ Peds` &lt;chr&gt;, `TSLA+cycl / peds` &lt;chr&gt;, Model &lt;chr&gt;,\n#   `Autopilot claimed` &lt;chr&gt;, `Reported in NHTSA SGO` &lt;chr&gt;,\n#   `Verified Tesla Autopilot Deaths` &lt;chr&gt;,\n#   `Excerpt Verifying Tesla Autopilot Deaths` &lt;chr&gt;,\n#   `Verified FSD Beta Death` &lt;lgl&gt;, ...19 &lt;chr&gt;, ...20 &lt;chr&gt;, Source &lt;chr&gt;,\n#   Note &lt;chr&gt;, `Deceased 1` &lt;chr&gt;, `Deceased 2` &lt;chr&gt;, `Deceased 3` &lt;chr&gt;, …",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#sec-json-files",
    "href": "reading_data.html#sec-json-files",
    "title": "3  Reading Data",
    "section": "3.3 JSON Files",
    "text": "3.3 JSON Files\nJSON stands for JavaScript Object Notation, and although it was borne out of interoperability concerns for JavaScript applications, it is a language-agnostic data format. Initially used to pass information in human readable form between applications over APIs (Application Programmer Interfaces), JSON has grown into a general-purpose format for text-based, structured information. It is the standard for communicating data on the web. The correct pronunciation of JSON is like the name “Jason”, but “JAY-sawn” has become common.\nIn contrast to CSV, JSON is not based on rows of data but three basic data elements:\n\nValue: a string, number, reserved word, or one of the following:\nObject: a collection of name—value pairs similar to a key-value store.\nArray: An ordered list of values\n\nAll modern programming languages support key—values and arrays, they might be calling it by different names (object, record, dictionary, struct, list, sequence, map, hash table, …). This makes JSON documents highly interchangeable between programming languages—JSON documents are easy to parse (read) and write by computers. Any modern data processing system can read and write JSON data, making it a frequent choice to share data between systems and applications.\nA value in JSON can be a string in double quotes, a number, true, false, or null, an object or an array (Figure 3.1). An array is an ordered collection of values. Objects are unordered collection of name—value pairs. Since values can contain objects and arrays, JSON allows highly nested data structures that do not fit the tabular row–column structure of CSV files.\n\n\n\n\n\n\nFigure 3.1: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\n\n\n\nJSON documents are self-describing, the schema to make the data intelligible is built into the structures. It is also a highly flexible format that does not impose any structure on the data, except that it must comply with the JSON rules and data types.\n\n\n\n\n\n\nFigure 3.2: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\n\n\n\nAs a human-readable, non-binary format, JSON shares some of the advantages and disadvantages with CSV files. You do not want to pass sensitive information in JSON format without encryption. The level of human readability is lower for JSON files. The format is intended to make algorithms interoperable, not to make human interpretation simple.\nSince so much data is stored in JSON format, you need to get familiar and comfortable with working with JSON files. Data science projects are more likely consumers of JSON files rather than producer of files.\n\nThere are multiple packages for working with JSON files in R, for example, jsonlite and rjson. Here we use the jsonlite package. The toJSON and fromJSON functions are used to convert R objects to/from JSON. read_json and write_json read and write JSON files. They are similar to fromJSON and toJSON but recognize a file path as input.\nThe following example is taken from Datacarpentry.org. SAFI (Studying African Farmer-Led Irrigation) is a study of farming and irrigation methods in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017.\nIf you use read_json with the default settings, the JSON document is converted into a list (simplifyVector=FALSE). To convert JSON format into vectors and data frames, use simplifyVector=TRUE.\n\nlibrary(jsonlite)\njson_data &lt;- read_json(\"data/SAFI.json\", simplifyVector=TRUE)\n\nTo take a look at the data frame created with read_json we use the glimpse function from tidyverse. It works a bit like the str function in base R but has a more compact display for this case and shows more data points.\n\nlibrary(tidyverse)\nglimpse(json_data)\n\nRows: 131\nColumns: 74\n$ C06_rooms                      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 1, 3, 1, …\n$ B19_grand_liv                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ A08_ward                       &lt;chr&gt; \"ward2\", \"ward2\", \"ward2\", \"ward2\", \"wa…\n$ E01_water_use                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"y…\n$ B18_sp_parents_liv             &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"…\n$ B16_years_liv                  &lt;int&gt; 4, 9, 15, 6, 40, 3, 38, 70, 6, 23, 20, …\n$ E_yes_group_count              &lt;chr&gt; NA, \"3\", NA, NA, NA, NA, \"4\", \"2\", \"3\",…\n$ F_liv                          &lt;list&gt; [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[3 …\n$ `_note2`                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ instanceID                     &lt;chr&gt; \"uuid:ec241f2c-0609-46ed-b5e8-fe575f6ce…\n$ B20_sp_grand_liv               &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"…\n$ F10_liv_owned_other            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `_note1`                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ F12_poultry                    &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"…\n$ D_plots_count                  &lt;chr&gt; \"2\", \"3\", \"1\", \"3\", \"2\", \"1\", \"4\", \"2\",…\n$ C02_respondent_wall_type_other &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ C02_respondent_wall_type       &lt;chr&gt; \"muddaub\", \"muddaub\", \"burntbricks\", \"b…\n$ C05_buildings_in_compound      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, …\n$ `_remitters`                   &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 …\n$ E18_months_no_water            &lt;list&gt; &lt;NULL&gt;, &lt;\"Aug\", \"Sept\"&gt;, &lt;NULL&gt;, &lt;NULL…\n$ F07_use_income                 &lt;chr&gt; NA, \"AlimentaÃ§Ã£o e pagamento de educa…\n$ G01_no_meals                   &lt;int&gt; 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, …\n$ E17_no_enough_water            &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"yes\", \"yes\"…\n$ F04_need_money                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ A05_end                        &lt;chr&gt; \"2017-04-02T17:29:08.000Z\", \"2017-04-02…\n$ C04_window_type                &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"n…\n$ E21_other_meth                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ D_no_plots                     &lt;int&gt; 2, 3, 1, 3, 2, 1, 4, 2, 3, 2, 2, 2, 4, …\n$ F05_money_source               &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ A07_district                   &lt;chr&gt; \"district1\", \"district1\", \"district1\", …\n$ C03_respondent_floor_type      &lt;chr&gt; \"earth\", \"earth\", \"cement\", \"earth\", \"e…\n$ E_yes_group                    &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[3 …\n$ A01_interview_date             &lt;chr&gt; \"2016-11-17\", \"2016-11-17\", \"2016-11-17…\n$ B11_remittance_money           &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no…\n$ A04_start                      &lt;chr&gt; \"2017-03-23T09:49:57.000Z\", \"2017-04-02…\n$ D_plots                        &lt;list&gt; [&lt;data.frame[2 x 8]&gt;], [&lt;data.frame[3 …\n$ F_items                        &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[2 …\n$ F_liv_count                    &lt;chr&gt; \"1\", \"3\", \"1\", \"2\", \"4\", \"1\", \"1\", \"2\",…\n$ F10_liv_owned                  &lt;list&gt; \"poultry\", &lt;\"oxen\", \"cows\", \"goats\"&gt;, …\n$ B_no_membrs                    &lt;int&gt; 3, 7, 10, 7, 7, 3, 6, 12, 8, 12, 6, 7, …\n$ F13_du_look_aftr_cows          &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no…\n$ E26_affect_conflicts           &lt;chr&gt; NA, \"once\", NA, NA, NA, NA, \"never\", \"n…\n$ F14_items_owned                &lt;list&gt; &lt;\"bicycle\", \"television\", \"solar_panel…\n$ F06_crops_contr                &lt;chr&gt; NA, \"more_half\", NA, NA, NA, NA, \"more_…\n$ B17_parents_liv                &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ G02_months_lack_food           &lt;list&gt; \"Jan\", &lt;\"Jan\", \"Sept\", \"Oct\", \"Nov\", \"…\n$ A11_years_farm                 &lt;dbl&gt; 11, 2, 40, 6, 18, 3, 20, 16, 16, 22, 6,…\n$ F09_du_labour                  &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"no\", \"yes\", …\n$ E_no_group_count               &lt;chr&gt; \"2\", NA, \"1\", \"3\", \"2\", \"1\", NA, NA, NA…\n$ E22_res_change                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ E24_resp_assoc                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, NA, \"yes\", NA…\n$ A03_quest_no                   &lt;chr&gt; \"01\", \"01\", \"03\", \"04\", \"05\", \"6\", \"7\",…\n$ `_members`                     &lt;list&gt; [&lt;data.frame[3 x 12]&gt;], [&lt;data.frame[7…\n$ A06_province                   &lt;chr&gt; \"province1\", \"province1\", \"province1\", …\n$ `gps:Accuracy`                 &lt;dbl&gt; 14, 19, 13, 5, 10, 12, 11, 9, 11, 14, 1…\n$ E20_exper_other                &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"yes\", \"yes\"…\n$ A09_village                    &lt;chr&gt; \"village2\", \"village2\", \"village2\", \"vi…\n$ C01_respondent_roof_type       &lt;chr&gt; \"grass\", \"grass\", \"mabatisloping\", \"mab…\n$ `gps:Altitude`                 &lt;dbl&gt; 698, 690, 674, 679, 689, 692, 709, 700,…\n$ `gps:Longitude`                &lt;dbl&gt; 33.48346, 33.48342, 33.48345, 33.48342,…\n$ E23_memb_assoc                 &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"no\", \"yes\",…\n$ E19_period_use                 &lt;dbl&gt; NA, 2, NA, NA, NA, NA, 10, 10, 6, 22, N…\n$ E25_fees_water                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ C07_other_buildings            &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"ye…\n$ observation                    &lt;chr&gt; \"None\", \"Estes primeiros inquÃ©ritos na…\n$ `_note`                        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ A12_agr_assoc                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ G03_no_food_mitigation         &lt;list&gt; &lt;\"na\", \"rely_less_food\", \"reduce_meals…\n$ F05_money_source_other         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `gps:Latitude`                 &lt;dbl&gt; -19.11226, -19.11248, -19.11211, -19.11…\n$ E_no_group                     &lt;list&gt; [&lt;data.frame[2 x 6]&gt;], [&lt;data.frame[0 …\n$ F14_items_owned_other          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ F08_emply_lab                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ `_members_count`               &lt;chr&gt; \"3\", \"7\", \"10\", \"7\", \"7\", \"3\", \"6\", \"12…\n\n\nBecause of the deeply nested structure of JSON documents, flattening the data into a two-dimensional data frame can go only so far. Several of the columns are lists and some are lists of data frames.\n\njson_data %&gt;%\n    select(where(is.list)) %&gt;%\n    glimpse()\n\nRows: 131\nColumns: 14\n$ F_liv                  &lt;list&gt; [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[3 x 2]&gt;], …\n$ `_remitters`           &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 x 0]&gt;], …\n$ E18_months_no_water    &lt;list&gt; &lt;NULL&gt;, &lt;\"Aug\", \"Sept\"&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL…\n$ F05_money_source       &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ E_yes_group            &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[3 x 14]&gt;],…\n$ D_plots                &lt;list&gt; [&lt;data.frame[2 x 8]&gt;], [&lt;data.frame[3 x 8]&gt;], …\n$ F_items                &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[2 x 3]&gt;], …\n$ F10_liv_owned          &lt;list&gt; \"poultry\", &lt;\"oxen\", \"cows\", \"goats\"&gt;, \"none\", …\n$ F14_items_owned        &lt;list&gt; &lt;\"bicycle\", \"television\", \"solar_panel\", \"tabl…\n$ G02_months_lack_food   &lt;list&gt; \"Jan\", &lt;\"Jan\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"&gt;, &lt;…\n$ E22_res_change         &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ `_members`             &lt;list&gt; [&lt;data.frame[3 x 12]&gt;], [&lt;data.frame[7 x 12]&gt;]…\n$ G03_no_food_mitigation &lt;list&gt; &lt;\"na\", \"rely_less_food\", \"reduce_meals\", \"day_…\n$ E_no_group             &lt;list&gt; [&lt;data.frame[2 x 6]&gt;], [&lt;data.frame[0 x 0]&gt;], …\n\n\nYou access the data frames stored within the lists simply as any other list element in R.\n\nstr(json_data$F_liv[[2]])\n\n'data.frame':   3 obs. of  2 variables:\n $ F11_no_owned: int  4 3 4\n $ F_curr_liv  : chr  \"oxen\" \"cows\" \"goats\"\n\njson_data$F_liv[[2]]\n\n  F11_no_owned F_curr_liv\n1            4       oxen\n2            3       cows\n3            4      goats",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#parquet-files",
    "href": "reading_data.html#parquet-files",
    "title": "3  Reading Data",
    "section": "3.4 Parquet Files",
    "text": "3.4 Parquet Files\nThe Apache Parquet open-source file format is a binary format—data are not stored in plain text but in binary form. Originally conceived as a column-based file format in the Hadoop ecosystem, it has become popular as a general file format for analytical data inside and outside of Hadoop and its file system HDFS: for example, as an efficient analytic file format for data exported to data lakes or in data processing with Spark. Many organizations have switched to storing their data in Parquet files; loading Parquet files from AWS S3 buckets or from Google Cloud Storage or Microsoft Azure Blob storage has become a common access pattern.\nWorking with Parquet files for large data is an order of magnitude faster than working with CSV files. The drawbacks of CSV files discussed previously all melt away with Parquet files.\nParquet was designed from the ground up with complex data structures and read-heavy analytics in mind. It uses principally columnar storage but does it cleverly by storing chunks of columns in row groups rather than entire columns.\n\n\n\n\n\n\nFigure 3.3: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\n\n\n\nThis hybrid storage model is very efficient when queries select specific columns and filter rows at the same time; a common pattern in data science: compute the correlation between homeValue and NumberOfRooms for homes where ZipCode = 24060.\nParquet stores metadata about the row chunks to speed access to rows, the metadata tells the reader which row chunks to skip. Also, a single write to the Parquet format can generate multiple .parquet files. The total data is divided into multiple files collected within a folder. Like NoSQL and NewSQL databases, data are partitioned, but since Parquet is a file format and not a database engine, the partitioning results in multiple files. This is advantageous for parallel processing frameworks like Spark that can work on multiple partitions (files) concurrently.\nParquet uses several compression techniques to reduce the size of the files such as run-length encoding, dictionary encoding, Snappy, GZip, LZO, LZ4, ZSTD. Because of columnar storage, compression methods can be specified on a per-column basis; Parquet files compress much more than text-oriented CSV files.\nBecause of its complex file structure, Parquet files are relatively slow to write. The file format is optimized for the WORM paradigm: write-once, read many times.\n\nComparison of popular file formats in data science.\n\n\n\n\n\n\n\n\n\nCSV\nJSON\nParquet\n\n\n\n\nColumnar\nNo\nNo\nYes\n\n\nCompression\nYes\nYes\nYes\n\n\nHuman Readable\nYes\nYes\nNo\n\n\nNestable\nNo\nYes\nYes\n\n\nComplex Data Structures\nNo\nYes\nYes\n\n\nNamed Columns\nYes, if in header\nBased on scan\nYes, metadata\n\n\nData Types\nBased on scan\nBased on scan\nYes, metadata\n\n\n\nTo read a file in Parquet format into an R session, install and load the arrow package. Apache Arrow is an open-source development platform for in-memory analytics that supports many programming environments, including R and Python. The arrow libraries support reading and writing of the important file formats in this ecosystem.\n\n#install.packages(\"arrow\")\nlibrary(arrow)\nread_parquet(\"somefile.parquet\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#working-with-a-database",
    "href": "reading_data.html#working-with-a-database",
    "title": "3  Reading Data",
    "section": "3.5 Working with a Database",
    "text": "3.5 Working with a Database\nYou interface with a database from R with the DBI package. It provides a common syntax and functions to connect to supported databases, and to send queries to the database. In addition to the DBI package you need the driver packages specific to the database you are working with (RMySQL for MySQL, RSQLite for SQLite, and so on).\nMy favorite database for analytic work is duckDB, a highly efficient, embedded database designed for processing data analytically. The file ads.ddb, contains the database tables used in this and other courses in duckDB format. To add the duckDB driver package to your system, use\n\ninstall.packages(\"duckdb\")\n\nTo read a table from that database into an R data frame, follow the steps in the next code snippet:\n\n1library(\"duckdb\")\n2con &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\n3fit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n4dbDisconnect(con)\n\n\n1\n\nLoad the duckdb library\n\n2\n\nMake a connection to duckDB through the DBI interface, specifying the database you want to work with, here ads.ddb\n\n3\n\nSend a SELECT * FROM query to read the contents of the target table and assign the result to a dataframe in R\n\n4\n\nClose the connection to the database when you are done querying it.\n\n\n\n\nWe can now work with the fitness data set as a dataframe in R:\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\nsummary(fit)\n\n      Age            Weight          Oxygen         RunTime     \n Min.   :38.00   Min.   :59.08   Min.   :37.39   Min.   : 8.17  \n 1st Qu.:44.00   1st Qu.:73.20   1st Qu.:44.96   1st Qu.: 9.78  \n Median :48.00   Median :77.45   Median :46.77   Median :10.47  \n Mean   :47.68   Mean   :77.44   Mean   :47.38   Mean   :10.59  \n 3rd Qu.:51.00   3rd Qu.:82.33   3rd Qu.:50.13   3rd Qu.:11.27  \n Max.   :57.00   Max.   :91.63   Max.   :60.05   Max.   :14.03  \n   RestPulse        RunPulse        MaxPulse    \n Min.   :40.00   Min.   :146.0   Min.   :155.0  \n 1st Qu.:48.00   1st Qu.:163.0   1st Qu.:168.0  \n Median :52.00   Median :170.0   Median :172.0  \n Mean   :53.45   Mean   :169.6   Mean   :173.8  \n 3rd Qu.:58.50   3rd Qu.:176.0   3rd Qu.:180.0  \n Max.   :70.00   Max.   :186.0   Max.   :192.0  \n\n\nThe connection to the database can be left open until you are done working with the database. If you use the database to import tables at the beginning of a statistical program, it is recommended to close the connection as soon as that step is complete.\nBecause we are reading many tables from the ads.ddb database, you can write a function to wrap the database operations. The following function loads the duckdb library unless it is already loaded, and reads a table, possibly selecting rows with a WHERE filter, and returns the R dataframe.\n\nduckload &lt;- function(tableName, whereClause=NULL, dbName=\"ads.ddb\") {\n    if (!is.null(tableName)) {\n        if (!(\"duckdb\" %in% (.packages()))) {\n            suppressWarnings(library(\"duckdb\"))\n            message(\"duckdb library was loaded to execute duckload().\")\n\n        }\n        con &lt;- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)\n        query_string &lt;- paste(\"SELECT * from \", tableName)\n        if (!is.null(whereClause)) {\n            query_string &lt;- paste(query_string, \" WHERE \", whereClause)\n        }\n        df_ &lt;- dbGetQuery(con, query_string)\n        dbDisconnect(con)\n        return (df_)\n    } else {\n        return (NULL)\n    }\n}\n\n\n# Load the entire table\nfit &lt;- duckload(\"fitness\")\n\n# Load only the records where Age &gt; 50\nfit2 &lt;- duckload(\"fitness\",whereClause=\"Age &gt; 50\")\n\n# Load only the records where Age = 40\nfit3 &lt;- duckload(\"fitness\",\"Age = 40\")\n\nYou can modify the function to load a projection of columns from the database table by replacing the * in the SELECT query with the selected columns.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#saving-and-loading-rdata",
    "href": "reading_data.html#saving-and-loading-rdata",
    "title": "3  Reading Data",
    "section": "3.6 Saving and Loading RData",
    "text": "3.6 Saving and Loading RData\nR has its own format (binary or ASCII) to read and write R objects. This is convenient to save a data frame to disk and later load it back into an R session.\nTo save any R object, use the save or saveRDS function, to load it into an R session use the load function. save can save one or more objects to a .RData file, while saveRDS creates an .RDS file from a single object.\n\na &lt;- matrix(rnorm(200),nrow=50,ncol=4)\nb &lt;- crossprod(a)\ncor_mat &lt;- cor(a)\nsave(a,b,cor_mat,file=\"data/matrixStuff.RData\")\n\n\nload(\"data/matrixStuff.RData\")\n\nWhen you load R objects from a .RData file, you do not assign the result of the function. The objects are created in the work environment with their original names.\n\n\n\n\n\n\nCaution\n\n\n\nThe load operation will overwrite any R objects in your environment by the same name as those in the RData file.\n\n\n.RData files can contain many objects, for example, all the objects in your environment when it is saved upon closing an R session. If you load from an .RDS file, you can assign the result to an R object which helps avoid name collisions.\nIn general, it is better coding practice to use saveRDS and readRDS with single objects. The code is cleaner and easier to follow if single objects are saved/loaded and these are explicitly named in the code.\na = rnorm(100)\nsaveRDS(a, file = \"stuff.RDS\") \n\nnormal_rvs &lt;- readRDS(\"stuff.RDS\")\n\n\n\nFigure 3.1: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\nFigure 3.2: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\nFigure 3.3: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html",
    "href": "wrangling_data.html",
    "title": "4  Wrangling Data",
    "section": "",
    "text": "4.1 Introduction\nWrangling data refers to the steps to organize the data in a structured format that is suitable for analytic processing. Typically, the result of data wrangling is data in a row-column layout where each analysis variable is in its own column and each observation is in its own row. Data often do not start out that way.\nWe saw a bit of data wrangling in the previous chapter (@#sec-json-files) when a nested JSON structure was flattened into a row-column format.\nIn addition to structuring the data, wrangling includes the following steps:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#introduction",
    "href": "wrangling_data.html#introduction",
    "title": "4  Wrangling Data",
    "section": "",
    "text": "Cleaning involves removing or correcting inaccurate data, handling duplicates, and addressing anomalies that could impact the reliability of analyses. The focus is on enhancing data accuracy and reliability for analytic processing.\nEnriching involves creating additional variables, incorporating external data, and combining the data with other data sources. Any new data sources you bring needs to be structured and cleaned as well.\nValidation checks for inconsistencies and verifies data integrity. Your organization will have standards, for example how regions, customer information, names, ages, etc. are represented in data sets, and this step ensures that the standards are met.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#tidyverse",
    "href": "wrangling_data.html#tidyverse",
    "title": "4  Wrangling Data",
    "section": "4.2 tidyverse",
    "text": "4.2 tidyverse\nIn this chapter we concentrate on structuring, enriching, and combining data with the libraries in the tidyverse. This is an “opinionated” collection of R packages for data science, including dplyr, ggplot2, tibble, tidyr, purr, stringr, and readr. dplyr, tidyr, and ggplot2 are arguably the most important packages in the collection (that is my “opinionated” view). For data wrangling we rely on dplyr and tidyr.\nYou can install the packages individually or grab all the tidyverse packages with\n\ninstall.packages(\"tidyverse\")\n\nA great cheat sheet for wrangling data in R with dplyr and tidyr can be found here. This chapter draws on the contents of this cheat sheet.\n\nPiping\nThe tidyverse packages share a common philosophy; that makes it easy to use code across multiple packages and to combine them. An example of this common philosophy is piping. The following pipeline starts with the iris data frame. The data is piped to the filter function with the pipe operator %&gt;%. The result of the filter operation is piped to the summarize function to compute the average petal width of the plants with sepal length greater than 6:\n\nlibrary(tidyverse)\niris %&gt;% \n    filter(Sepal.Length &gt; 6) %&gt;%\n    summarize(average=mean(Petal.Width))\n\n   average\n1 1.847541\n\n\nBy default the argument on the left side of the pipe operator is passed as the first argument to the function on the right side. The filter function call is really dplyr::filter(iris,Species==\"virginica). You can also pass the argument on the left of the pipe to a different argument on the right side of the pipe by using a dot:\nx %&gt;% f(y) is the same as f(x,y)\nx %&gt;% f(y, ., z) is the same as f(y,x,z)\nPiping not only shows analytic operations as a sequence of steps, but also reduces the amount of code and makes it generally more readable. Learning to write good pipelines.\nData wrangling with dplyr and tidyr can be organized into the following steps:\n\nShaping the data\nSubsetting observations and/or variables\nCreating new variables\nCombining data sets\nSummarization\n\nSummarizing data is discussed as a separate step in Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#shaping-data-into-tabular-form",
    "href": "wrangling_data.html#shaping-data-into-tabular-form",
    "title": "4  Wrangling Data",
    "section": "4.3 Shaping Data into Tabular Form",
    "text": "4.3 Shaping Data into Tabular Form\nThe goal of shaping the data is to change its structure into a tabular row-column form where each variable is in a separate column and each observation is in its own row.\nConsider the classic airline passenger time series data from Box, jenkins, and Reinsel (1976), showing monthly totals of international airline passengers between 1994 and 1960 (in thousands).\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe data are arranged very neatly, each year in a separate row, each month in a separate column. This is a great layout for time series analysis but maybe not the best layout for general analytic processing. The analysis variables are year, month, and the passenger count. Restructuring the AirPassengers data into a data frame with variables for year and count is easy because this data set is stored as an R time series.\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\n\ndat &lt;- data.frame(count=as.matrix(AirPassengers), \n                  time=time(AirPassengers),\n                  year=round(time(AirPassengers),0))\nhead(dat)\n\n  count     time year\n1   112 1949.000 1949\n2   118 1949.083 1949\n3   132 1949.167 1949\n4   129 1949.250 1949\n5   121 1949.333 1949\n6   135 1949.417 1949\n\n\n\nGapminder Data\nIn general, the structure of a non-tabular format is not known to R and must be wrangled by the user. Consider the famous Gapminder dataset. The Gapminder Foundation is a Swedish non-profit that promotes the United Nations sustainability goals through use of statistics. The Gapminder data set tracks economic and social indicators like life expectancy and the GDP per capita of countries over time.\nA wide format of the Gapminder data is available on GitHub here. We are following the material in this excellent online resource in wrangling the data in the wide format.\n\n\ngap_wide &lt;- read.csv(file=\"data/gapminder_wide.csv\")\nstr(gap_wide)\n\n'data.frame':   142 obs. of  38 variables:\n $ continent     : chr  \"Africa\" \"Africa\" \"Africa\" \"Africa\" ...\n $ country       : chr  \"Algeria\" \"Angola\" \"Benin\" \"Botswana\" ...\n $ gdpPercap_1952: num  2449 3521 1063 851 543 ...\n $ gdpPercap_1957: num  3014 3828 960 918 617 ...\n $ gdpPercap_1962: num  2551 4269 949 984 723 ...\n $ gdpPercap_1967: num  3247 5523 1036 1215 795 ...\n $ gdpPercap_1972: num  4183 5473 1086 2264 855 ...\n $ gdpPercap_1977: num  4910 3009 1029 3215 743 ...\n $ gdpPercap_1982: num  5745 2757 1278 4551 807 ...\n $ gdpPercap_1987: num  5681 2430 1226 6206 912 ...\n $ gdpPercap_1992: num  5023 2628 1191 7954 932 ...\n $ gdpPercap_1997: num  4797 2277 1233 8647 946 ...\n $ gdpPercap_2002: num  5288 2773 1373 11004 1038 ...\n $ gdpPercap_2007: num  6223 4797 1441 12570 1217 ...\n $ lifeExp_1952  : num  43.1 30 38.2 47.6 32 ...\n $ lifeExp_1957  : num  45.7 32 40.4 49.6 34.9 ...\n $ lifeExp_1962  : num  48.3 34 42.6 51.5 37.8 ...\n $ lifeExp_1967  : num  51.4 36 44.9 53.3 40.7 ...\n $ lifeExp_1972  : num  54.5 37.9 47 56 43.6 ...\n $ lifeExp_1977  : num  58 39.5 49.2 59.3 46.1 ...\n $ lifeExp_1982  : num  61.4 39.9 50.9 61.5 48.1 ...\n $ lifeExp_1987  : num  65.8 39.9 52.3 63.6 49.6 ...\n $ lifeExp_1992  : num  67.7 40.6 53.9 62.7 50.3 ...\n $ lifeExp_1997  : num  69.2 41 54.8 52.6 50.3 ...\n $ lifeExp_2002  : num  71 41 54.4 46.6 50.6 ...\n $ lifeExp_2007  : num  72.3 42.7 56.7 50.7 52.3 ...\n $ pop_1952      : num  9279525 4232095 1738315 442308 4469979 ...\n $ pop_1957      : num  10270856 4561361 1925173 474639 4713416 ...\n $ pop_1962      : num  11000948 4826015 2151895 512764 4919632 ...\n $ pop_1967      : num  12760499 5247469 2427334 553541 5127935 ...\n $ pop_1972      : num  14760787 5894858 2761407 619351 5433886 ...\n $ pop_1977      : num  17152804 6162675 3168267 781472 5889574 ...\n $ pop_1982      : num  20033753 7016384 3641603 970347 6634596 ...\n $ pop_1987      : num  23254956 7874230 4243788 1151184 7586551 ...\n $ pop_1992      : num  26298373 8735988 4981671 1342614 8878303 ...\n $ pop_1997      : num  29072015 9875024 6066080 1536536 10352843 ...\n $ pop_2002      : int  31287142 10866106 7026113 1630347 12251209 7021078 15929988 4048013 8835739 614382 ...\n $ pop_2007      : int  33333216 12420476 8078314 1639131 14326203 8390505 17696293 4369038 10238807 710960 ...\n\n\nThe data are stored in wide format, annual values for the variables GDP, life expectancy, and population appear in separate columns. This is not unlike how the AirPassengers data is displayed, but gap_wide is not a time series object. The desired (“tidy”) way of structuring the data, where each variable is a column and each observation is a row is a tabular structure with variables\n\nContinent\nCountry\nYear\nGDP\nLife Expectancy\nPopulation\n\nFor each combination of continent and country there will be 12 observations, one for each year.\n\nGather columns\nTo move from wide to the desired long format, we use the dplyr::gather function. To do the opposite, moving from long to wide format, use the dplyr::spread function. To restructure the Gapminder data set from wide format into the desired format takes several steps.\nThe first step is to gather the columns that contain the values for GDP, life expectancy, and population into separate rows. Those are all columns in gap_wide except continent and country, so we can exclude those from the gathering operation with -c(continent, country).\n\ngap_step1 &lt;- gap_wide %&gt;% \n    gather(key=vartype_year,\n          value=values,\n          -c(continent, country))\nhead(gap_step1)\n\n  continent      country   vartype_year    values\n1    Africa      Algeria gdpPercap_1952 2449.0082\n2    Africa       Angola gdpPercap_1952 3520.6103\n3    Africa        Benin gdpPercap_1952 1062.7522\n4    Africa     Botswana gdpPercap_1952  851.2411\n5    Africa Burkina Faso gdpPercap_1952  543.2552\n6    Africa      Burundi gdpPercap_1952  339.2965\n\ntail(gap_step1)\n\n     continent        country vartype_year   values\n5107    Europe         Sweden     pop_2007  9031088\n5108    Europe    Switzerland     pop_2007  7554661\n5109    Europe         Turkey     pop_2007 71158647\n5110    Europe United Kingdom     pop_2007 60776238\n5111   Oceania      Australia     pop_2007 20434176\n5112   Oceania    New Zealand     pop_2007  4115771\n\n\n\n\nSeparating character variables\nThe gather function turns all variables into key-value pairs; the key is the name of the variable. The column that contains the names of the variables after the gathering is called vartype_year. In the next step dplyr::separate is called to split the character column vartype_year into two columns, one for the variable type and one for the year. The convert=TRUE option attempts to convert the data type of the new columns from character to numeric data—this will fail for the vartype column but succeed for the year column.\n\ngap_step2 &lt;- gap_wide %&gt;% \n    gather(key =vartype_year,\n          value=values,\n          -c(continent, country)) %&gt;%\n    separate(vartype_year,\n           into   =c('vartype','year'),\n           sep    =\"_\",\n           convert=TRUE)\nstr(gap_step2)\n\n'data.frame':   5112 obs. of  5 variables:\n $ continent: chr  \"Africa\" \"Africa\" \"Africa\" \"Africa\" ...\n $ country  : chr  \"Algeria\" \"Angola\" \"Benin\" \"Botswana\" ...\n $ vartype  : chr  \"gdpPercap\" \"gdpPercap\" \"gdpPercap\" \"gdpPercap\" ...\n $ year     : int  1952 1952 1952 1952 1952 1952 1952 1952 1952 1952 ...\n $ values   : num  2449 3521 1063 851 543 ...\n\nhead(gap_step2)\n\n  continent      country   vartype year    values\n1    Africa      Algeria gdpPercap 1952 2449.0082\n2    Africa       Angola gdpPercap 1952 3520.6103\n3    Africa        Benin gdpPercap 1952 1062.7522\n4    Africa     Botswana gdpPercap 1952  851.2411\n5    Africa Burkina Faso gdpPercap 1952  543.2552\n6    Africa      Burundi gdpPercap 1952  339.2965\n\ntail(gap_step2)\n\n     continent        country vartype year   values\n5107    Europe         Sweden     pop 2007  9031088\n5108    Europe    Switzerland     pop 2007  7554661\n5109    Europe         Turkey     pop 2007 71158647\n5110    Europe United Kingdom     pop 2007 60776238\n5111   Oceania      Australia     pop 2007 20434176\n5112   Oceania    New Zealand     pop 2007  4115771\n\n\n\n\nSpreading rows into columns\nWe are almost there, but not quite. We now have columns for continent, country, and year, but the values column contains values of different types: 1,704 observations for GDP, 1704 observations for life expectancy, and 1,704 observations for population. To create separate columns from the rows we can reverse the gather operation with the spread function—it splits key-value pairs across multiple columns. The entries in the vartype column are used by the spreading operation as the names of the new columns.\n\ngapminder &lt;- \n    gap_wide %&gt;% \n    gather(key =vartype_year,\n          value=values,\n          -c(continent, country)) %&gt;%\n    separate(vartype_year,\n           into   =c('vartype','year'),\n           sep    =\"_\",\n           convert=TRUE) %&gt;%\n    spread(vartype, values)\n\nhead(gapminder)\n\n  continent country year gdpPercap lifeExp      pop\n1    Africa Algeria 1952  2449.008  43.077  9279525\n2    Africa Algeria 1957  3013.976  45.685 10270856\n3    Africa Algeria 1962  2550.817  48.303 11000948\n4    Africa Algeria 1967  3246.992  51.407 12760499\n5    Africa Algeria 1972  4182.664  54.518 14760787\n6    Africa Algeria 1977  4910.417  58.014 17152804\n\ntail(gapminder)\n\n     continent     country year gdpPercap lifeExp     pop\n1699   Oceania New Zealand 1982  17632.41  73.840 3210650\n1700   Oceania New Zealand 1987  19007.19  74.320 3317166\n1701   Oceania New Zealand 1992  18363.32  76.330 3437674\n1702   Oceania New Zealand 1997  21050.41  77.550 3676187\n1703   Oceania New Zealand 2002  23189.80  79.110 3908037\n1704   Oceania New Zealand 2007  25185.01  80.204 4115771\n\n\nIn summary, we used gather to create one key-value pair of the variable_year columns and values in step 1, separated out the year in step 2, and spread the remaining variable types back out into columns in step 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#subsetting",
    "href": "wrangling_data.html#subsetting",
    "title": "4  Wrangling Data",
    "section": "4.4 Subsetting",
    "text": "4.4 Subsetting\nA subset operation reduces the number of observations (rows) or variables (columns) of data set. While filter is the most important operation to subset rows, there are a number of other functions in dplyr that reduce the rows. On the other hand, to subset columns there is only one function, dplyr::select.\n\nSubsetting Rows\nThe following statements create a data frame of dimension 100 x 2, containing two columns of integers randomly selected from 1–10 with replacement:\n\nset.seed(76)\ndat &lt;- data.frame(\n  x = sample(10, 100, rep = TRUE),\n  y = sample(10, 100, rep = TRUE)) %&gt;% \n    glimpse()\n\nRows: 100\nColumns: 2\n$ x &lt;int&gt; 5, 1, 2, 10, 7, 5, 6, 10, 8, 6, 4, 6, 3, 1, 8, 5, 7, 6, 4, 1, 3, 10,…\n$ y &lt;int&gt; 3, 5, 9, 8, 7, 10, 4, 1, 10, 6, 9, 6, 2, 9, 3, 8, 9, 7, 4, 3, 2, 3, …\n\n\n\nfilter\nfilter extracts rows that meet a logical condition. The following statement selects the rows for which \\(x \\in (1,5)\\) and \\(y \\in (3,8)\\).\n\ndat %&gt;% filter(x %in% c(1,5) & y %in% c(3,8))\n\n  x y\n1 5 3\n2 5 8\n3 1 3\n4 5 8\n5 5 8\n6 5 3\n\n\nYou can also list the conditions that are combined with logical “and”, separated with commas:\n\ndat %&gt;% filter(x %in% c(1,5), y %in% c(3,8))\n\n  x y\n1 5 3\n2 5 8\n3 1 3\n4 5 8\n5 5 8\n6 5 3\n\n\nThe following statement extracts the rows where the value of y exceeds 3.5 times its standard deviation:\n\ndat %&gt;% filter(y &gt; 3.5*sd(y))\n\n   x  y\n1  5 10\n2  8 10\n3  7 10\n4  1 10\n5  8 10\n6  1 10\n7  9 10\n8  9 10\n9  6 10\n10 7 10\n11 4 10\n12 5 10\n\n\nAnother subsetting operation is to remove duplicate observations from a data set with distinct. When applied to a subset of the columns, distinct returns the combinations of their unique values in the data.\n\n\ndistinct\n\ndat %&gt;% distinct(x)\n\n    x\n1   5\n2   1\n3   2\n4  10\n5   7\n6   6\n7   8\n8   4\n9   3\n10  9\n\n\nIf you specify .keep_all=TRUE, all other variables in the data set are kept as well. If multiple combinations occur, the function retains the first occurence.\n\ndat %&gt;% distinct(x, .keep_all=TRUE)\n\n    x  y\n1   5  3\n2   1  5\n3   2  9\n4  10  8\n5   7  7\n6   6  4\n7   8 10\n8   4  9\n9   3  2\n10  9  3\n\n\nTo remove all duplicates in a data frame, simply call distinct on the data frame.\n\ndat %&gt;% distinct() %&gt;% summarize(count=n())\n\n  count\n1    65\n\n\nThere are 62 unique rows of data in the dat data frame.\n\n\nOther subsetting functions\nOther functions subsetting rows in dplyr are\n\ndplyr::sample_frac: randomly select a proportion of the rows\ndplyr::sample_n: randomly select a fixed number of rows\ndplyr::slice: select rows by position, for example slice(dat,4:10) extracts rows 4–10\ndplyr::slice_head: selects the first rows\ndplyr::slice_tail: selects the last rows\ndplyr:slice_min: select rows with the smallest values\ndplyr:slice_max: select rows with the largest values\n\nFor example, the next statement selects the observation with the five largest values for the Sepal.Width variable in the iris data set.\n\niris %&gt;% slice_max(Sepal.Width, n=5)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.7         4.4          1.5         0.4  setosa\n2          5.5         4.2          1.4         0.2  setosa\n3          5.2         4.1          1.5         0.1  setosa\n4          5.8         4.0          1.2         0.2  setosa\n5          5.4         3.9          1.7         0.4  setosa\n6          5.4         3.9          1.3         0.4  setosa\n\n\n\n\n\nSubsetting (selecting) Columns\n\nselect\nTo subset columns there is only one statement in dplyr, the select statement. However, it is very versatile because of its many helper functions.\nThe basic usage is to list the column names being selected:\n\ngapminder %&gt;% select(gdpPercap, lifeExp) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\nYou can also specify an exclusion with a negative selection\n\ngapminder %&gt;% select(-country, -continent, -pop) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\nor\n\ngapminder %&gt;% select(-c(country, continent, pop)) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\n\n\nHelper functions\nHere are important helper functions for dplyr::select\n\nselect( contains(\".\")): select columns whose name contains a character string\nselect( ends_with(\"Length\")): select columns whose name ends in the specified string\nselect( starts_with(\"Sepal\")): select columns whose name starts with the specified string\nselect( everything()): select all columns\nselect( matches(\".t.\")): select the columns whose name matches a regular expression\nselect( num_range(\"x\",1:5)): select the columns named x1, x2, …, x5\nselect( one_off(\"Species\",\"Genus\")): select columns whose names are in the specified group of names\nselect( Sepal.Length:Petal.Width): Select all columns between Sepal.Length and Petal.Width, including those columns",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#creating-new-variables",
    "href": "wrangling_data.html#creating-new-variables",
    "title": "4  Wrangling Data",
    "section": "4.5 Creating New Variables",
    "text": "4.5 Creating New Variables\nThe principal function to create new variables in a data frame is dplyr::mutate. Variations are mutate_each which applies a function to every column and transmute which drops the original columns.\nThe following statements compute the GPD as the product of per-capita GDP and population size and finds the top-5 countries by GDP in Asia in 2007:\n\ngapminder %&gt;%\n    filter(continent == \"Asia\", year == 2007) %&gt;%\n    mutate(GDP = gdpPercap * pop) %&gt;%\n    select(country, year, gdpPercap, GDP) %&gt;%\n    slice_max(GDP,n=5)\n\n     country year gdpPercap          GDP\n1      China 2007  4959.115 6.539501e+12\n2      Japan 2007 31656.068 4.035135e+12\n3      India 2007  2452.210 2.722925e+12\n4 Korea Rep. 2007 23348.140 1.145105e+12\n5       Iran 2007 11605.714 8.060583e+11\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe variable GDP created in this operation is transient. It is instantiated for the duration of the pipeline and is not added to the gapminder data frame. If you want to add a new variable to an existing data frame you need to assign the result to a return object.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#combining-data-sets",
    "href": "wrangling_data.html#combining-data-sets",
    "title": "4  Wrangling Data",
    "section": "4.6 Combining Data Sets",
    "text": "4.6 Combining Data Sets\nThe process of combining tables is based on joins, set operations, or bindings. A join uses the values in specific columns of one data set to match records with another data set. A set operation is a merging of columns without considering the values in the columns. Appending rows rows of one table to another table or columns of one table to another table are binding operations. What happens to columns that exist in one data set but not in the other during the append depends on the implementation. Similarly, what happens to columns that share the same name when tables are merged horizontally depends on the implementation.\nTo show the various join and set operations, let’s create three small data frames of cities and weather information.\n\ncapitals1 &lt;- data.frame(city=c('Amsterdam','Berlin'),\n                       country=c('NL','Germany'))\n\ncapitals2 &lt;- data.frame(city=c('Amsterdam','Washington, D.C.'),\n                       country=c('NL','U.S.A.'))\n\nweather1 &lt;- data.frame(city=c('Amsterdam','Seattle'),\n                      degrees=c(10,8),\n                      date=c(\"2022-10-14\",\"2022-10-12\"))\n\ncapitals1\n\n       city country\n1 Amsterdam      NL\n2    Berlin Germany\n\ncapitals2 \n\n              city country\n1        Amsterdam      NL\n2 Washington, D.C.  U.S.A.\n\nweather1\n\n       city degrees       date\n1 Amsterdam      10 2022-10-14\n2   Seattle       8 2022-10-12\n\n\n\nSet Operations\nWe distinguish three set operations: the intersection of rows that appear in two data sets, the union of the rows, and the set difference of the rows. dplyr supports set operations with the following functions:\n\nintersect(x, y): finds all rows in both x and y.\nunion(x, y): finds all rows in either x or y, excluding duplicates.\nunion_all(x, y): finds all rows in either x or y, including duplicates.\nsetdiff(x, y): finds all rows in x that are not in y.\nsymdiff(x, y): computes the symmetric difference, i.e. all rows in x that are not in y and all rows in y that are not in x.\nsetequal(x, y): returns TRUE if x and y contain the same rows (ignoring order).\n\nNote that except for union_all the functions that return rows remove duplicates in x and y.\nFor the two data frames of capitals, here are the results of the various set operations.\n\ndplyr::intersect(capitals1, capitals2)\n\n       city country\n1 Amsterdam      NL\n\ndplyr::setdiff(capitals1, capitals2)\n\n    city country\n1 Berlin Germany\n\n\n\ndplyr::union(capitals1, capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3 Washington, D.C.  U.S.A.\n\ndplyr::union_all(capitals1, capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3        Amsterdam      NL\n4 Washington, D.C.  U.S.A.\n\n\n\ndplyr::symdiff(capitals1, capitals2)\n\n              city country\n1           Berlin Germany\n2 Washington, D.C.  U.S.A.\n\n\n\n\nJoins\nSet operations combine or reduce rows of data (vertically). Join operations combine data sets horizontally. Joins typically are based on the values in columns of the data sets to find matches.\nJoins are categorized as outer or inner joins depending on whether rows with matches are returned. An outer join returns rows that do not have any matches whereas the inner join returns only rows that get paired. The two data frames in a join are called the left and right sides of the relation and outer joins are further classified as\n\nLeft outer join: all rows from the left side of the relation appear at least once.\nRight outer join: all rows from the right side of the relation appear at least once.\nFull outer join: all rows from both sides of the relation appear at least once.\n\nTo demonstrate the joins in dplyr let’s set up some simple tables:\n\nweather &lt;- data.frame(name=c('San Francisco','San Francisco','Hayward'),\n                      temp_lo=c(46,43,37),\n                      temp_hi=c(50,57,54),\n                      prcp=c(0.25,0.0,NA),\n                      date=c('1994-11-27','1994-11-29','1994-11-29'))\n\ncities &lt;- data.frame(name=c('San Francisco'),\n                     lat=c(-194.0),\n                     lon=c(53.0))\n\n\nweather\n\n           name temp_lo temp_hi prcp       date\n1 San Francisco      46      50 0.25 1994-11-27\n2 San Francisco      43      57 0.00 1994-11-29\n3       Hayward      37      54   NA 1994-11-29\n\ncities\n\n           name  lat lon\n1 San Francisco -194  53\n\n\nAn inner join between the data frames on the columns that contain the city names will match the records for San Francisco:\n\ndplyr::inner_join(weather,cities,by=\"name\")\n\n           name temp_lo temp_hi prcp       date  lat lon\n1 San Francisco      46      50 0.25 1994-11-27 -194  53\n2 San Francisco      43      57 0.00 1994-11-29 -194  53\n\n\nNote that the values for lat and lon are repeated for every row in the weather table that matches the join in the relation. Because this is an inner join and the weather table had no matching row for city Hayward, this city does not appear in the join result. We can change that by modifying the type of join to a left outer join:\n\ndplyr::left_join(weather,cities,by=\"name\")\n\n           name temp_lo temp_hi prcp       date  lat lon\n1 San Francisco      46      50 0.25 1994-11-27 -194  53\n2 San Francisco      43      57 0.00 1994-11-29 -194  53\n3       Hayward      37      54   NA 1994-11-29   NA  NA\n\n\nBecause the join is an outer join, rows that do not have matches in the relation are returned. Because the outer join is a left join, every row on the left side of the relation is returned (at least once). If you change the left- and right-hand side of the relation you can achieve the same result by using a right outer join:\n\ndplyr::right_join(cities,weather,by=\"name\")\n\n           name  lat lon temp_lo temp_hi prcp       date\n1 San Francisco -194  53      46      50 0.25 1994-11-27\n2 San Francisco -194  53      43      57 0.00 1994-11-29\n3       Hayward   NA  NA      37      54   NA 1994-11-29\n\n\nThe left join retains all observations in the left data frame (first argument). The right join retains all observations in the right data frame (second argument).\nNow let’s add another record to the cities data frame without a matching record in the weather data frame:\n\ncities &lt;- dplyr::bind_rows(cities,data.frame(name=\"New York\",\n                                             lat=40.7,\n                                             lon=-73.9))\ncities \n\n           name    lat   lon\n1 San Francisco -194.0  53.0\n2      New York   40.7 -73.9\n\n\nA full outer join between the cities and weather data frames ensures that rows from both sides of the relation appear at least once:\n\ndplyr::full_join(cities,weather,by=\"name\")\n\n           name    lat   lon temp_lo temp_hi prcp       date\n1 San Francisco -194.0  53.0      46      50 0.25 1994-11-27\n2 San Francisco -194.0  53.0      43      57 0.00 1994-11-29\n3      New York   40.7 -73.9      NA      NA   NA       &lt;NA&gt;\n4       Hayward     NA    NA      37      54   NA 1994-11-29\n\n\n\n\nBindings\nFor data scientists working with rectangular data frames in which observations have a natural order, merging data horizontally is a standard operation. Observations are matched by position and not according to the values in key columns. In the world of relational databases, such a merge is called a positional join and a somewhat unnatural operation because relational tables do not work from a natural ordering of the data, they are based on keys and indices.\nWhen working with statistical data sets merging by position is not uncommon as data sets do not have keys. The positional join—or column binding—matches data frames row-by-row such that rows from both tables appear at least once:\n\ndplyr::bind_cols(capitals1,weather1)\n\nNew names:\n• `city` -&gt; `city...1`\n• `city` -&gt; `city...3`\n\n\n   city...1 country  city...3 degrees       date\n1 Amsterdam      NL Amsterdam      10 2022-10-14\n2    Berlin Germany   Seattle       8 2022-10-12\n\n\nNote that both data frames contribute a city variable and these are renamed to resolve name collision.\nA similar operation binding rows stacks one data frame on top of another. The result from dplyr::bind_rows contains all columns that appear in any of the inputs, unobserved combinations are set to NA.\n\ndplyr::bind_rows(capitals1,capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3        Amsterdam      NL\n4 Washington, D.C.  U.S.A.\n\ndplyr::bind_rows(capitals1,weather1)\n\n       city country degrees       date\n1 Amsterdam      NL      NA       &lt;NA&gt;\n2    Berlin Germany      NA       &lt;NA&gt;\n3 Amsterdam    &lt;NA&gt;      10 2022-10-14\n4   Seattle    &lt;NA&gt;       8 2022-10-12\n\n\n\n\n\n\nBox, G. E. P., G. M. jenkins, and G. C. Reinsel. 1976. Time Series Analysis, Forecasting and Control. 3rd. Ed. Holden-Day.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "summarization.html",
    "href": "summarization.html",
    "title": "5  Summarization",
    "section": "",
    "text": "5.1 Introduction\nA single statistic such as the sample standard deviation is a summary, as is a cross-tabulation of the levels of two categorical variables, as is a series of box plots. The purposes of data summarization are many:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#introduction",
    "href": "summarization.html#introduction",
    "title": "5  Summarization",
    "section": "",
    "text": "Definition: Data Summarization\n\n\nData summarization is the numerical, tabular, and graphical distillation of the essence of a data set and the relationships between its variables through aggregation.\n\n\n\n\nProfiling. Borne (2021) refers to it as “having that first date with your data.” We want to know what we are dealing with.\nDescription. What are the central tendencies and the dispersion of the variables? For example, what does the comparison of statistics measuring the central tendency tell us about the distribution of the data, the presence of outliers? What distributional assumptions are reasonable for the data. Are transformations in a feature processing step called for?\nAggregation. Suppose you could not store the raw data but you need to retain information for future statistical processing. What kind of information would you compute and squirrel away? A sufficient statistic is a function of the data that contains all information toward estimating a parameter of the data distribution. For example, the sample mean \\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\) is sufficient to estimate the mean of a set of \\(n\\) independent and identically distributed random variables. If \\(Y\\) is uniform on \\([0,\\theta]\\), then \\(\\max\\{Y_i\\}\\) is sufficient for \\(\\theta\\). The quantities we compute during summarization are frequently sufficient statistics; examples are sums, sums of squares, sums of crossproducts.\nRelationships. Summarization is not only about individual variables, but also about their relationship. The correlation matrix of \\(p\\) numerical variables is a frequent summary that describes the pairwise linear relationships in the data.\nDimension Reduction. In high-dimensional statistical problems the number of potential input variables is larger than what we can handle (on computational grounds) or should handle (on statistical grounds). Summarization can reduce a \\(p\\)-dimensional problem to an \\(m\\)-dimensional problem where \\(m \\ll p\\). Principal component analysis (PCA) relies on matrix factorization (eigenvalue or singular value decomposition) of a crossproduct matrix to find a set of \\(m\\) linear combinations of the \\(p\\) input variables that explain a substantial amount of variability in the data. The \\(m\\) linear combinations summarize the essence of the relationships in the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#location-and-dispersion-statistics",
    "href": "summarization.html#location-and-dispersion-statistics",
    "title": "5  Summarization",
    "section": "5.2 Location and Dispersion Statistics",
    "text": "5.2 Location and Dispersion Statistics\nCommon location and dispersion measures for quantitative variables are shown in Table 5.1 and Table 5.2.\n\n\n\nTable 5.1: Important statistics measuring location attributes of a variable in a sample of size \\(n\\). Sample mean, sample median, and sample mode are measures of the central tendency of a variable. \\(Y^*\\) denotes the ordered sequence of observations and \\(Y^*[k]\\) the value at the \\(k\\)th position in the ordered sequence. The min is defined as the smallest non-missing value because NaNs often sort as the smallest values in software packages.\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nMin\n\n\\(Y^*[1]\\)\nThe smallest non-missing value\n\n\nMax\n\n\\(Y^*[n]\\)\nThe largest value\n\n\nMean\n\\(\\overline{Y}\\)\n\\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\)\nMost important location measure, but can be affected by outliers\n\n\nMedian\nMed\n\\[\\left \\{    \\begin{array}{cc}       Y^* \\left[ \\frac{n+1}{2} \\right ] & n \\text{ is even} \\\\        \\frac{1}{2} \\left( Y^* \\left[\\frac{n}{2} \\right] + Y^* \\left[\\frac{n}{2}+1\\right] \\right) & n\\text{ is odd} \\end{array}\\right .\\]\nHalf of the observations are smaller than the median; robust against outliers\n\n\nMode\nMode\n\nThe most frequent value; not useful when real numbers are unique\n\n\n1st Quartile\n\n\\(Y^*\\left[\\frac{1}{4}(n+1) \\right]\\)\n25% of the observations are smaller than \\(Q_1\\)\n\n\n2nd Quartile\n\nSee Median\n50% of the observations are smaller than \\(Q_2\\). This is the median\n\n\n3rd Quartile\n\n\\(Y^*\\left[\\frac{3}{4}(n+1) \\right]\\)\n75% of the observations are smaller than \\(Q_3\\)\n\n\nX% Percentile\n\n\\(Y^*\\left[\\frac{X}{100}(n+1) \\right]\\)\nFor example, 5% of the observations are larger than \\(P_{95}\\), the 95% percentile\n\n\n\n\n\n\n\n\n\nTable 5.2: Important statistics measuring dispersion (variability) of a variable in a sample of size \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nRange\n\\(R\\)\n\\(Y^*[n] - Y^*[1]\\)\nSimply largest minus smallest value\n\n\nInter-quartile Range\nIQR\n\\(Q_3 - Q_1\\)\nUsed in constructing box plots; covers the central 50% of the data\n\n\nStandard Deviation\n\\(S\\)\n\\(\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left( Y_i - \\overline{Y}\\right)^2}\\)\nMost important dispersion measure; in the same units as the sample mean (the units of \\(Y\\))\n\n\nVariance\n\\(S^2\\)\n\\(\\frac{1}{n-1}\\sum_{i=1}^n \\left( Y_i - \\overline{Y} \\right)^2\\)\nImportant statistical measure of dispersion; in squared units of \\(Y\\)\n\n\n\n\n\n\n\nstats Package\nThe following list of functions provides basic location and dispersion statistics in R (stats package). The min and max functions are provided by the base package.\n\n\n\nTable 5.3: R summarization functions in stats.\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nNotes\n\n\n\n\nmean\nSample (arithmetic) mean\ncan be trimmed (trim=)\n\n\nmedian\nSample median\n\n\n\nsd\nStandard deviation\nuses \\(n-1\\) as denominator\n\n\nvar\nSample variance\nuses \\(n-1\\) as denominator. Also can calculate variance-covariance matrix of vectors\n\n\nrange\nMinimum and maximum\nreturns a vector with two values\n\n\nIQR\nInterquartile range\n\n\n\nmad\nMedian absolute deviation\ndefault metric for center is the median\n\n\nquantile\nSample quantiles\ngive list of probabilities in probs= argument; default is minimum, maximum, \\(Q_1\\), \\(Q_2\\), and \\(Q_3\\)\n\n\nfivenum\nTukey’s five number summary\nMin, lower hinge, median, upper hinge, maximum\n\n\nboxplot.stats\nStatistics to construct box plot\nstats value contains extreme of lower whisker, lower hinge, median, upper hinge, extreme of upper whisker\n\n\ncov\nSample covariance\nSingle statistic or covariance matrix\n\n\ncor\nSample correlation\nSingle statistic or correlation matrix\n\n\n\n\n\n\nThe summary function in R is a generic function that produces summaries of an R object. The return value depends on the type of its argument. When summary is called on a data frame, it returns basic location statistics for the numeric variables and the counts-per-level for factors. Interestingly, it does not compute any measures of dispersion.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nTo compute some of the functions in Table 5.3 for multiple columns in a matrix or data frame, the apply function is very helpful. The following function call request the mean for the numeric columns (the first four columns) of the iris data set.\n\napply(iris[,-5],2,mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nThe next statement requests the standard deviations\n\napply(iris[,-5],2,sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\nThe second argument specifies the margin over which the function will be applied: 1 implies calculations for rows, 2 implies calculations for columns. The following statements compute column and row sums for the 50 I. setosa observations in the iris data set\n\ncol.sums &lt;- apply(iris[iris$Species==\"setosa\",1:4], 2, sum)\ncol.sums\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       250.3        171.4         73.1         12.3 \n\nrow.sums &lt;- apply(iris[iris$Species==\"setosa\",1:4], 1, sum)\nrow.sums\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2 12.0 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7  9.7 10.7 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2  9.5 10.7  9.4 \n  49   50 \n10.7  9.9 \n\n\n\n\ndplyr Package\nThe dplyr package is part of the tidyverse, an “opinionated” collection of R packages for data science. Packages in the tidyverse include dplyr, ggplot2, tibble, tidyr, purr, stringr, and readr. dplyr, tidyr and ggplot2 are arguably the most important packages in the collection (that is my “opinionated” view).\nThe tidyverse packages share a common philosophy; that makes it easy to use code across multiple packages and to combine them. An example of this common philosophy is piping.\nSuppose we use dplyr to compute the mean and standard deviation for the petal width of I. virginica observations in the iris data set. You can put this together in a single function call to the dplyr::summarize function.\n\nlibrary(dplyr)\nsummarize(filter(iris,Species==\"virginica\"), \n          count=n(),\n          mean=mean(Petal.Width), \n          stdDev=sd(Petal.Width))\n\n  count  mean    stdDev\n1    50 2.026 0.2746501\n\n\nA more elegant way of processing the data is with a series of steps, where the result of one step is passed automatically to the next step. In tidyverse syntax, such a step is called a pipe and indicated with %&gt;%. Rewriting the previous summarize statement using pipes leads to\n\niris %&gt;% filter(Species==\"virginica\") %&gt;%\n    summarize(count =n(),\n              mean  =mean(Petal.Width),\n              stdDev=sd(Petal.Width)) \n\n  count  mean    stdDev\n1    50 2.026 0.2746501\n\n\nThe pipeline starts with the data frame iris. Since a pipe operation passes its input as the first argument to the next operation, the filter statement is really filter(iris,Species==\"virginica).\nIf you want to save the result of this piping operation, simply assign it to an object:\n\nvirg_summ &lt;- iris %&gt;% filter(Species==\"virginica\") %&gt;%\n                  summarize(count =n(),\n                            mean  =mean(Petal.Width),\n                            stdDev=sd(Petal.Width)) \n\nOr, you can keep going, piping the result into other tidyverse functions, for example\n\niris %&gt;% filter(Species==\"virginica\") %&gt;%\n    summarize(count =n(),\n              mean  =mean(Petal.Width),\n              stdDev=sd(Petal.Width)) %&gt;%\n    glimpse()\n\nRows: 1\nColumns: 3\n$ count  &lt;int&gt; 50\n$ mean   &lt;dbl&gt; 2.026\n$ stdDev &lt;dbl&gt; 0.2746501\n\n\nTable 5.4 lists summarization functions in dplyr.\n\n\n\nTable 5.4: Summarization functions in dplyr.\n\n\n\n\n\nFunction\nDescription\nNotes\n\n\n\n\nmean\nSample (arithmetic) mean\n\n\n\nmedian\nSample median\n\n\n\nsd\nStandard deviation\nuses \\(n-1\\) as denominator\n\n\nvar\nSample variance\nuses \\(n-1\\) as denominator\n\n\nmin\nMinimim\n\n\n\nmax\nMaximum\n\n\n\nIQR\nInterquartile range\n\n\n\nfirst\nFirst value of a vector\n\n\n\nlast\nLast value of a vector\n\n\n\nnth\n\\(n\\)th value of a vector\n\n\n\nn\nNumber of values in a vector\n\n\n\nn_distinct\nNumber of distinct (unique) values in a vector\n\n\n\n\n\n\n\n\nIn the previous examples we filtered and summarized on a single variable. If you want to calculate statistics for multiple variables you can either repeat the statements or indicate that the operation should apply across multiple columns. In the early release of dplyr the summarize_each function applied summary calculations for more than one column. This function has been deprecated in favor of across, which gathers multiple columns and can be applied to other dplyr statements as well (filter, group_by, etc.).\nThe following statement computes the mean for all numeric columns of the iris data set.\n\niris %&gt;% summarize(across(where(is.numeric), mean))\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.843333    3.057333        3.758    1.199333\n\n\nThe next form of summarize(across()) computes sample mean and median for the variables whose name begins with Sepal.\n\niris %&gt;% summarize(across(starts_with(\"Sepal\"), \n                          list(mn=mean, md=median)))\n\n  Sepal.Length_mn Sepal.Length_md Sepal.Width_mn Sepal.Width_md\n1        5.843333             5.8       3.057333              3\n\n\n\nThe order in which the data manipulations occur matter greatly for the result. In the following statement, the observations are filtered for which the Sepal.Length exceeds the average Sepal.Length by 10%.\n\niris %&gt;% filter(Sepal.Length &gt; 1.1*mean(Sepal.Length))\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1           7.0         3.2          4.7         1.4 versicolor\n2           6.9         3.1          4.9         1.5 versicolor\n3           6.5         2.8          4.6         1.5 versicolor\n4           6.6         2.9          4.6         1.3 versicolor\n5           6.7         3.1          4.4         1.4 versicolor\n6           6.6         3.0          4.4         1.4 versicolor\n7           6.8         2.8          4.8         1.4 versicolor\n8           6.7         3.0          5.0         1.7 versicolor\n9           6.7         3.1          4.7         1.5 versicolor\n10          7.1         3.0          5.9         2.1  virginica\n11          6.5         3.0          5.8         2.2  virginica\n12          7.6         3.0          6.6         2.1  virginica\n13          7.3         2.9          6.3         1.8  virginica\n14          6.7         2.5          5.8         1.8  virginica\n15          7.2         3.6          6.1         2.5  virginica\n16          6.5         3.2          5.1         2.0  virginica\n17          6.8         3.0          5.5         2.1  virginica\n18          6.5         3.0          5.5         1.8  virginica\n19          7.7         3.8          6.7         2.2  virginica\n20          7.7         2.6          6.9         2.3  virginica\n21          6.9         3.2          5.7         2.3  virginica\n22          7.7         2.8          6.7         2.0  virginica\n23          6.7         3.3          5.7         2.1  virginica\n24          7.2         3.2          6.0         1.8  virginica\n25          7.2         3.0          5.8         1.6  virginica\n26          7.4         2.8          6.1         1.9  virginica\n27          7.9         3.8          6.4         2.0  virginica\n28          7.7         3.0          6.1         2.3  virginica\n29          6.9         3.1          5.4         2.1  virginica\n30          6.7         3.1          5.6         2.4  virginica\n31          6.9         3.1          5.1         2.3  virginica\n32          6.8         3.2          5.9         2.3  virginica\n33          6.7         3.3          5.7         2.5  virginica\n34          6.7         3.0          5.2         2.3  virginica\n35          6.5         3.0          5.2         2.0  virginica\n\n\nThe average is computed across all 150 observations in the data set (50 observations for each species). By adding a group_by statement prior to the filter, the sepal lengths are being compared to the species-specific means. The presence of the group_by statement conditions the subsequent filter to be applied separately for each of the three species.\n\niris %&gt;% group_by(Species) %&gt;% filter(Sepal.Length &gt; 1.1*mean(Sepal.Length))\n\n# A tibble: 19 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1          5.8         4            1.2         0.2 setosa    \n 2          5.7         4.4          1.5         0.4 setosa    \n 3          5.7         3.8          1.7         0.3 setosa    \n 4          7           3.2          4.7         1.4 versicolor\n 5          6.9         3.1          4.9         1.5 versicolor\n 6          6.6         2.9          4.6         1.3 versicolor\n 7          6.7         3.1          4.4         1.4 versicolor\n 8          6.6         3            4.4         1.4 versicolor\n 9          6.8         2.8          4.8         1.4 versicolor\n10          6.7         3            5           1.7 versicolor\n11          6.7         3.1          4.7         1.5 versicolor\n12          7.6         3            6.6         2.1 virginica \n13          7.3         2.9          6.3         1.8 virginica \n14          7.7         3.8          6.7         2.2 virginica \n15          7.7         2.6          6.9         2.3 virginica \n16          7.7         2.8          6.7         2   virginica \n17          7.4         2.8          6.1         1.9 virginica \n18          7.9         3.8          6.4         2   virginica \n19          7.7         3            6.1         2.3 virginica \n\n\nNone of the I. setosa observations exceeded the overall sepal length by 10%. But three of the I. setosa observations exceeded the petal length of that species by 10%.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#group-by-summarization",
    "href": "summarization.html#group-by-summarization",
    "title": "5  Summarization",
    "section": "5.3 Group-by Summarization",
    "text": "5.3 Group-by Summarization\n\nUsing stats::aggregate\nThe preceding dplyr pipeline is an example of group-by processing: computing summaries separately for the levels of a qualitative variable (Species).\nGroup-by summarization is also possible with functions in the stats package. The aggregate function allows the use of the formula syntax that is common in many stats function. In addition to the “model”, you need to specify the aggregation function you wish to apply. For example, to compute the means for petal length by species and the standard deviations for petal length and petal width by species, use the following syntax:\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"mean\")\n\n     Species Petal.Length\n1     setosa        1.462\n2 versicolor        4.260\n3  virginica        5.552\n\naggregate(cbind(Petal.Length,Petal.Width) ~ Species, data=iris, FUN=\"sd\")\n\n     Species Petal.Length Petal.Width\n1     setosa    0.1736640   0.1053856\n2 versicolor    0.4699110   0.1977527\n3  virginica    0.5518947   0.2746501\n\n\nYou can also use aggregate with functions that have more complex return arguments, quantile and summary, for example.\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"quantile\")\n\n     Species Petal.Length.0% Petal.Length.25% Petal.Length.50% Petal.Length.75%\n1     setosa           1.000            1.400            1.500            1.575\n2 versicolor           3.000            4.000            4.350            4.600\n3  virginica           4.500            5.100            5.550            5.875\n  Petal.Length.100%\n1             1.900\n2             5.100\n3             6.900\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"summary\")\n\n     Species Petal.Length.Min. Petal.Length.1st Qu. Petal.Length.Median\n1     setosa             1.000                1.400               1.500\n2 versicolor             3.000                4.000               4.350\n3  virginica             4.500                5.100               5.550\n  Petal.Length.Mean Petal.Length.3rd Qu. Petal.Length.Max.\n1             1.462                1.575             1.900\n2             4.260                4.600             5.100\n3             5.552                5.875             6.900\n\n\n\n\nUsing dplyr::group_by\nThe group_by statement in dplyr is a simple technique to apply group-specific operations; it is thus one of the first statements you see in summarization after the data frame:\n\nstarwars %&gt;% group_by(gender) %&gt;% \n    filter(mass &gt; median(mass,na.rm=TRUE)) %&gt;%\n    summarize(count=n())\n\n# A tibble: 3 × 2\n  gender    count\n  &lt;chr&gt;     &lt;int&gt;\n1 feminine      3\n2 masculine    19\n3 &lt;NA&gt;          1\n\n\nYou can combine group-by execution with summarization across multiple columns. The following pipeline computes the arithmetic mean of all numeric columns in the iris data set and arranges the result by descending value of the average sepal width:\n\niris %&gt;% \n    group_by(Species) %&gt;% \n    summarize(across(where(is.numeric), mean)) %&gt;%\n    arrange(desc(Sepal.Width))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 virginica          6.59        2.97         5.55       2.03 \n3 versicolor         5.94        2.77         4.26       1.33 \n\n\n\n\nContinuous Grouping Variable\nGroup-by computations can also be used if the grouping variable is continuous. We first create bins of the continuous variable, assign observations to the bins, and then group the summaries by the bins. The following code assigns observations to four bins based on the quartiles of Sepal.Length an then computes the average Sepal.Width for each level of Sepal.Length.\n\nlibrary(dplyr)\n\niris_data &lt;- iris\n1qs &lt;- quantile(iris$Sepal.Length, probs=c(0, 0.25, 0.5, 0.75, 1))\nqs\n2iris_data$sep.len.cut &lt;- cut(x=iris$Sepal.Length, breaks = qs)\n\n# Fix the assignment of the minimum value to the first category\niris_data$sep.len.cut[which.min(iris$Sepal.Length)] &lt;- \n    attr(iris_data$sep.len.cut,\"levels\")[1]\n\niris_data %&gt;% group_by(sep.len.cut) %&gt;%\n         summarize(count=n(), mean=mean(Sepal.Width))\n\n\n1\n\nThe quantile function computes the sample quantiles for the requested vector of probabilities. 0 and 1 are included to capture the minimum and maximum value.\n\n2\n\nThe cut function applies the computed quantiles as break points to bin the values of Sepal.Length.\n\n\n\n\n  0%  25%  50%  75% 100% \n 4.3  5.1  5.8  6.4  7.9 \n# A tibble: 4 × 3\n  sep.len.cut count  mean\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n1 (4.3,5.1]      41  3.18\n2 (5.1,5.8]      39  3.09\n3 (5.8,6.4]      35  2.87\n4 (6.4,7.9]      35  3.07\n\n\nYou can include the creation of the categories directly into the group_by statement if the cutpoints require no further processing.\n\nmtcars %&gt;%\n    group_by(hp_cut = cut(hp, 3)) %&gt;%\n    summarize(count=n(), mn_mpg=mean(mpg), mean_disp=mean(disp))\n\n# A tibble: 3 × 4\n  hp_cut     count mn_mpg mean_disp\n  &lt;fct&gt;      &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 (51.7,146]    17   24.2      135.\n2 (146,241]     11   15.7      339.\n3 (241,335]      4   14.6      340.\n\n\n\n\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Date with Your Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "6  Visualization",
    "section": "",
    "text": "6.1 Introduction\nThe purpose of data visualization is to communicate quantitative and qualitative data in an accurate and easily understandable way. It is used to convey information without distracting from it. It should be useful, appealing and never misleading. Like data summarization, visualization is a means of reducing a large amount of information to the essential elements.\nGelman and Unwin (2013) list these basic goals of data visualization:\nTukey (1993) summarized the purpose of graphical displays in four statements:\nThe graphic must allow us to compare things, it is not about revealing actual values. The mean or median can be represented as a reference line, as in the boxplot, but you do not need to show the actual value of the mean or median. It exists on the graphic through its relation to other graphical elements. By the same token Tukey argues that you should not have to display the scale of the horizontal or vertical axis unless that is needed for the interpretation; for example when plotting on a log scale.\nYou can always add labels that show the values being graphed, but overdoing that turns a graphic into an elaborate tabular display. If that is necessary, then maybe a table is a better way to present the information. The interocular impact Tukey is talking about in 3. is the information that hits the viewer “right between the eyes”.\nAbout Tukey’s fourth point, Gelman and Unwin write\nConsider this example from their paper (Figure 6.1)\nAt the start of the 20th century, ten last letters dominated boys’ names, by 1950 that had reduced to six letters. By 2010, the letter “n” stands out; about 1/3 of boys’ names in the U.S. end in “n”! That is interocular impact, it stands out. Is this a spurious effect? What could cause this? What does it mean? Gelman and Unwin, consulting with the authors of the graphic, provide this explanation. Over time, parents felt less constrained in choosing baby names, whereas a hundred years ago they choose from a small set of common names, often picking names of male relatives. Surprisingly, the greater freedom in choosing names nowadays results in selecting soundalikes (Aiden, Caden, Hayden, etc.) resulting in clustering in the last letter. Gelman and Unwin conclude",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#introduction",
    "href": "visualization.html#introduction",
    "title": "6  Visualization",
    "section": "",
    "text": "Discovery\n\nGive an overview—a qualitative sense of what is in a data set.\nConvey scale and complexity—a network graph does not reveal the underlying structure but gives a sense of the size and connectivity in the network.\n**Exploration*—discover unexpected aspects of the data.\n\nCommunication\n\nTell a story—to self and others by displaying information in an understandable way. “If we communicate well, we call it storytelling”.\nAttract attention and stimulate interest—graphs are grabby.\n\n\n\n\n1. Graphics are for the qualitative/descriptive–conceivably the semiquantitative–never for the carefully quantitative (tables do that better).\n\n\n2. Graphics are for comparison–comparison of one kind or another–not for access to individual amounts.\n\n\n3. Graphics are for impact–interocular impact if possible, swinging-finger impact if that is the best one can do, or impact for the unexpected as a minimum–but almost never for something that has to be worked at hard to be perceived.\n\n\n4. Finally, graphics should report the results of careful data analysis-rather than be an attempt to replace it.\n\n\n\n\n\nTo put it another way, a picture may be worth a thousand words, but a picture plus 1000 words is more valuable than two pictures or 2000 words.\n\n\n\n\n\n\n\n\nFigure 6.1: From Gelman and Unwin (2013)\n\n\n\n\n\nNowadays the distribution of names is more diffuse but the distribution of sounds is more concentrated.\n\n\nExploratory and Presentation Graphics\nGelman and Unwin (2013) discuss the difference between information visualization, the use of graphics to convey information outside of statistics, and statistical graphics, the use of visualization to convey information in data. Infographics, the result of information visualization, grab your attention and get you thinking. Statistical graphics facilitate a deeper understanding of the data. The two should be related—the information visualized in an Infographic is very often data, and statistical graphics are often not as catchy and beautiful as professional visualizations. There is a good reason for that, and it depends on whether you create graphics in exploratory mode or in presentation mode. What makes a good graphic depends on the mode.\n\nExploratory mode\nDuring exploratory work you are trying to get insight into the data. Graphing is about producing a large number of graphics for an audience of one—yourself. Speed, flexibility, and alternative views of the data are of the essence. You do not need to grab your own attention; you already have it. You do not need to provide a lot of context; you already know it. Characteristics of good data visualization in exploratory mode are\n\nSimple to create\nMultiple views of the data\nMakes you think and curious about digging deeper, stimulates self-interest\nLess is more; shows only what it needs to\nEnough detail to give insight\n\nThink of thousands of graphs viewed by one person—yourself.\n\n\nPresentation mode\nIn presentation mode you work on a small number of graphics that are viewed by a much larger audience. The audience is not yourself. You do not have the attention of the audience yet and need to grab it. The audience also does not have full context yet and should be able to construct it from the graphic (and maybe a few other pieces of information). The presentation graphic will have elements such as legends, labels, annotations, and captions that help convey the context.\nCharacteristics of a good data visualization in presentation mode are\n\nGrabs attention and draws the reader in; wow factor\nShows only what it needs to\nA single view; is self-contained\nTell a story\nProvides context through the visualization itself (legend, caption)\n\nThink of thousands of persons viewing one graph—produced by yourself.\nNotice that in both modes graphics should display only what they need to. Avoid excessive annotations, colors, labels, grid lines—the stuff Tufte (1983) and Tufte (2001) refers to as chartjunk. However, what is needed depends on the purpose of the graphic. While less is always more, presentation graphics are more self contained and need to provide information and context that you already have in exploration mode.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#basic-graphics-in-r",
    "href": "visualization.html#basic-graphics-in-r",
    "title": "6  Visualization",
    "section": "6.2 Basic Graphics in R",
    "text": "6.2 Basic Graphics in R\nA wide variety of plotting functions come with the base graphics package. To see all functions in that library, check out the following command:\n\nlibrary(help=\"graphics\")\n\nFor statistical programming, the most important plotting functions are\n\nplot: generic function for plotting of R objects\nsmoothScatter: smoothed color density representation of a scatterplot\npairs: matrix of scatter plots\nhist: histogram of the given data values\nacf: computes (and by default plots) estimates of the autocorrelation function\nboxplot: box-and-whisker plot(s) of the given (grouped) values\nbarplot: bar plot with vertical or horizontal bars\ndotchart: Cleveland’s dot plot\npie: draw a pie chart\nstars: star (segment) plots and spider (radar) plots\ncontour: contour plot, or add contour lines to an existing plot\nimage: grid of colored or gray-scale rectangles with colors corresponding to the values in z\n\nplot is both the generic scatterplot function and a generic plotting function for other R objects. For example, when passed an output object from lm, plot produces a variety of regression diagnostic plots:\nreg &lt;- lm(mpg ~ cyl + disp + hp, data=mtcars)\n\nplot(reg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther graphic functions support the previous\n\nlines: A generic function taking coordinates and joining the corresponding points with line segments\nablines: This function adds one or more straight lines through the current plot\npoints: draws a sequence of points at the specified coordinates\npanel.smooth: simple panel plot\npolygon: polygon draws the polygons whose vertices are given in x and y\nsymbols: this function draws symbols on a plot\narrows: add arrows to a plot\naxis: add an axis to a plot\nlegend: add a legend to a plot\nmtext: writes text into the margin of a plot\ntext: adds text (one or a vector of labels) to a plot\ntitle: add titles (main, sub) and labels (xlab, ylab) to a plot\n\nThe behavior of the graphics functions can be modified with parameters. There are two ways to pass parameters to a plot: specify it as part of the function call, or use the generic par function.\n\nset.seed(543)\ndat &lt;- matrix(rnorm(100),nrow=50,ncol=2)\nplot(dat, las=1, bty=\"l\", xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\npar(las=1, bty=\"l\")\nplot(dat)\n\n\n\n\n\n\n\n\nxlab= and ylab= are not graphical parameters, they are passed in the plot function. las= and bty= are graphical parameter that determine the style of axis labels and the boundary box of the plot. These parameters can also be passed in the par function. Check the documentation for parameters that are set or queried with par.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#grammar-of-graphics",
    "href": "visualization.html#grammar-of-graphics",
    "title": "6  Visualization",
    "section": "6.3 Grammar of Graphics",
    "text": "6.3 Grammar of Graphics\nThe grammar of graphics was described by statistician Leland Wilkinson and conceptualizes data visualization as a series of layers, in an analogy with linguistic grammar (Figure 6.2). Just like a sentence has subject and predicates, a scientific graph has parts.\nThe grammar of graphics is helpful because we associate data visualization not by the name of this plot or that chart type, but by a series of elements, depicted as layers.\nEach graphic consists of at least the following layers:\n\nThe data itself; the first layer in Figure 6.2\nThe mappings from data attributes to perceptible qualities; the aesthetics layer\nThe geometrical objects that represent the data; the geometries layer\n\nIn addition, we might apply statistical transformations of the data, must place all objects in a 2-dimensional space and decide on presentation elements such as fonts and colors. And if the data consist of multiple groups, we need a faceting specification to organize the graphic or the page in multiple groups.\n\n\n\n\n\n\nFigure 6.2: The grammar of graphics; a layered approach to building data visualizations.\n\n\n\nThinking about data visualization in these terms is helpful because we get away from thinking about pie charts and box plots and line charts, and instead about how to organize the basic elements of a visualization.\nGrammar of graphics principles are implemented in the ggplot2 library in R. ggplot2 is part of the tidyverse collection of packages, like dplyr. Layers are added in ggplot2 by adding them (literally, with a + operation) to an existing plot. This can be combined with the piping in tidyverse. The following code produces two scatter plots from the mtcars data. The aesthetic specifies the columns for the x-axis and y-axis (aes), geom_point specifies the geometry consists of the points of the data values. The second ggplot call modifies the point size and shape within the geometry and changes the overall theme of appearance. The facets, statistics, and coordinates layers are not used in these plots.\nlibrary(ggplot2)\nlibrary(dplyr)\n\nmtcars %&gt;% \n    ggplot(aes(y=mpg,x=wt)) +\n    geom_point()\nmtcars %&gt;% \n    ggplot(aes(y=mpg,x=wt)) +\n    geom_point(size=2.5, shape=21) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA common mistake with ggplot is to put the + at the beginning of the line; it has to appear at the end of the line. Forgetting this can drive novice users insane, why would this code fail\nmtcars %&gt;% \n    ggplot(aes(y=mpg,x=wt)) \n+   geom_point()\nbut this code works\nmtcars %&gt;% \n    ggplot(aes(y=mpg,x=wt)) +\n    geom_point()\nCheck the error message from ggplot in the first case. It contains the line\n\nDid you accidentally put + on a new line?\n\n\n\nThe next example uses filter and select functions from dplyr to subset rows and columns of the penguin data set. This data set can be added to your R installation with\n\ninstall.packages(\"palmerpenguins\")\n\ngeom_col is one of two geometries for bar charts. geom_bar makes the height of the bars proportional to the number of cases in each group, geom_bar draws the bars according to the values in the data.\nTwo aesthetic mappings are used. The first in the ggplot command specifies the columns from which the graph is constructed. The aesthetic in the geom_col geometry specifies that the sex variable is mapped to create the fill for the bars. Finally, facet_wrap uses the formula specification to group the bar charts by penguin species—this is an example of a paneled graph (Section 6.4).\n\nlibrary(palmerpenguins)\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;%\n  select(sex,body_mass_g, species) %&gt;%\n  ggplot(aes(x=sex, y=body_mass_g)) +\n  geom_col(aes(fill=sex)) + \n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\nFor many examples on using ggplot2 in R consult the R Graphics Cookbook or the Data Visualization chapter in R for Data Science (Wickham, Cetinkaya-Rundel, and Grolemund 2023). For example, Wickham, Cetinkaya-Rundel, and Grolemund (2023) walk you step by step through the process of creating Figure 6.3 layer by layer.\n\n\n\n\n\n\n\n\nFigure 6.3: Body mass and flipper length for Palmer penguins.\n\n\n\n\n\nSpoiler alert: here is the final code:\n\nlibrary(ggthemes)\n\nggplot(\n  data = penguins,\n  aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title   =\"Body mass and flipper length\",\n    subtitle=\"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x       =\"Flipper length (mm)\", \n    y       =\"Body mass (g)\",\n    color   =\"Species\", \n    shape   =\"Species\"\n  ) +\n  scale_color_colorblind()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-viz-multipanel",
    "href": "visualization.html#sec-viz-multipanel",
    "title": "6  Visualization",
    "section": "6.4 Multipanel Plots",
    "text": "6.4 Multipanel Plots\nMultipanel plots, as the name suggests, arrange multiple plots in an overall visualization, the space allocated to an individual plot is called a panel.\nFigure 6.4 is an example of a two-panel plot with separate panels for male and female penguins that shows separate regressions with confidence intervals for each species and sex combination. The facet_wrap function accomplishes the grouping of the data by sex and the paneling.\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;%\n  select(sex,body_mass_g, species, flipper_length_mm) %&gt;%\n  ggplot(aes(x=flipper_length_mm, y=body_mass_g, color=species)) +\n  geom_point() + \n  geom_smooth(method=\"lm\", se=TRUE) +\n  facet_wrap(~ sex) +\n  theme(axis.text.x = element_text(angle=45,hjust=1))\n\n\n\n\n\n\n\nFigure 6.4: Body mass versus flipper length by species and paneled by gender.\n\n\n\n\n\n\nIn general, we distinguish two types of multipanel plots. In trellis (also called lattice) plots, the panels are defined by grouping variables. The panels created with facet_wrap in the previous plots are ggplot examples of how to do that. These plots are also called conditional graphics as each panel is conditioned on certain values of the data. Furthermore, the basic plot elements in the panels are the same; scatterplots with smoothing lines, for example.\nCompound graphics on the other hand, display different plot types in the panels and the data are typically not conditioned in the sense that different panels show different subsets of the data. Rather, the multiple panels display different aspects of the same data.\nA big difference between conditional plots and compound graphics is that the software manages the panels for you in the first case and that you have to manage and populate the panels in the second case.\n\nConditional Plots\nYou can produce conditional plots in R in two ways: using conditioning capabilities built into a graphics package or using a graphics package built specifically for conditioning. ggplot follows the first model. You can construct any basic visualization following graphics of grammar rules. In the Facets layer you decide how to divide the data into groups (the conditioning) and assign groups to panels. The lattice package follows the second approach. It has specific functions for conditioning and paneling specific plot types. For example, lattice::xyplot produces paneled scatterplots, lattice::barchart produces conditional bar charts, lattice::bwplot produces conditional boxplots, and so on.\n\nggplot facets\nWith ggplot, the facet_wrap and facet_grid functions determine the layout of the panels. facet_wrap creates a sequence of panels while facet_grid uses a row-column layout.\nThe following code, modified from Wilke (2019), see here, uses the facet_grid paneling capabilities of ggplot to display the survival or death for passengers on the titanic, based on the passengers in the training data, conditioned by gender and passenger class.\n\nlibrary(titanic)\n\ntitanic_train %&gt;% \n    mutate(surv = ifelse(Survived == 0, \"Died\", \"Survived\")) %&gt;%\n    mutate(class = ifelse(Pclass==1,\"1st\",ifelse(Pclass==2,\"2nd\",\"3rd\"))) %&gt;%\n    mutate(sex = ifelse(Sex=='male',\"Male\",\"Female\")) %&gt;%\n    \n    ggplot(aes(sex, fill = sex)) + \n    geom_bar() +\n    facet_grid(class ~ surv) +\n    scale_x_discrete(name = NULL) + \n    scale_y_continuous(limits = c(0, 195), expand = c(0, 0)) +\n    scale_fill_manual(values = c(\"#D55E00D0\", \"#0072B2D0\"), guide = \"none\") +\n    theme(\n      axis.line         = element_blank(),\n      axis.ticks.length = grid::unit(0, \"pt\"),\n      axis.ticks        = element_blank(),\n      axis.text.x       = element_text(margin = margin(7, 0, 0, 0)),\n      strip.text        = element_text(margin = margin(3.5, 3.5, 3.5, 3.5)),\n      strip.background  = element_rect(\n        fill     = \"grey85\", \n        color    = \"grey85\",\n        linetype = 1, \n        size     = 0.25\n      ),\n      panel.border = element_rect(\n        color    = \"grey85\", \n        fill     = NA, \n        linetype = 1,\n        size     = 1.)\n      )\n\n\n\n\n\n\n\nFigure 6.5: Survial of passengers on the Titanic by gender and passenger class.\n\n\n\n\n\n\nThe data for the following example were collected at the Winchester Agricultural Experiment Station of Virginia Tech and are analyzed in (Schabenberger and Pierce 2001, 466–74). Ten apple trees were randomly selected at the experiment station and 25 apples were randomly chosen on each tree. The data analyzed here comprise the eighty apples in the largest size class, those apples with an initial diameter equal or greater than 2.75 inches. Over a period of 12 weeks diameter measurements of the apples were taken at 2-week intervals. The variables in the data set are\n\nTree: the tree number\nappleid: the number of the apple within the tree. Note that the same appleid can appear on multiple trees and only apples in the largest diameter size class appear in the data set.\nmeasurement: the index of the measurements. Measurements are taken in two-week intervals, so that measurement=1 refers to the state of the apple after 2 weeks and measurement=6 refers to the state of the apple at the end of the 12-week period\ndiameter the diameter of the apple at the time of measurement\n\n\napples &lt;- duckload(\"apples\")\n\nhead(apples)\n\n  Tree appleid measurement diameter\n1    1       1           1     2.90\n2    1       1           2     2.90\n3    1       1           3     2.90\n4    1       1           4     2.93\n5    1       1           5     2.94\n6    1       1           6     2.94\n\n\nSuppose we want to create a large panel plot where each of the eighty apples is in its own panel. This side-by-side comparison allows us to better see the apple-specific growth trends which would get lost if we were to overlay data from all apples in a single plot. But apple IDs are not unique within tree numbers in this data set. To uniquely identify an apple, we have to somehow indicate that nesting. This can be done by combining tree and apple id into a single identifier.\n\napples %&gt;% \n    mutate(TreeAppleID = paste(as.character(Tree),\"|\",as.character(appleid))) %&gt;%\n\n    ggplot(aes(y = diameter, x = measurement)) + \n    geom_point(color = \"blue\", size = 0.5) + \n    geom_smooth(formula   = y ~ x,\n                method    = 'lm', \n                se        = FALSE, \n                size      = 0.25, \n                color     = '#D55E00',\n                fullrange = TRUE) + \n   scale_x_continuous(name=\"Measurement occasion\") + \n   scale_y_continuous(name=\"Diameter\") + \n   facet_wrap(~TreeAppleID, ncol = 10) +\n   theme(axis.title = element_text(size = 14),\n         axis.ticks = element_blank(),\n         axis.ticks.length = unit(0, \"pt\"),\n         strip.text = element_text(margin = margin(3.5, 3.5, 3.5, 3.5)),\n         panel.border = element_rect(color     = \"grey80\", \n                                     fill      = NA, \n                                     linetype  = 1, \n                                     linewidth = 1.\n    ),\n    plot.margin = margin(3, 5, 3, 1.5)\n  )\n\n\n\n\n\n\n\n\nFor comparison purposes it is important that the scales of the axes, in particular the y-axis, is the same across all panels. Otherwise it would not be possible to compare growth trajectories between apples.\n\n\nTrellis plots with lattice\nThe lattice package has special plotting functions for conditional plot types. For example, xyplot produces paneled scatterplots. The following code displays the apple diameters over time, grouped (conditioned) by tree. The number of apples differs from tree to tree. While the overall growth trajectory over time, the slope of the trends, is similar across trees and apples, there appear to be tree-specific differences in the growth level (the intercept). One of the apples on trees #1, #7, and #10 has a much larger diameter compared to the other apples on those trees; evidence of greater within-tree variability.\nThe strip= option determines the labeling of the header strip of the panels. The as.table=TRUE option arranges the tree numbers from smallest to largest (try it out without that option) and the layout= option requests a single page of up to 12 panels in 4 rows and 3 columns.\n\nlibrary(lattice)\n\nxyplot(diameter ~ measurement | Tree, \n       data=apples,\n       strip = function(...) {\n           strip.default(..., \n                         strip.names =c(T,T), \n                         strip.levels=c(T,T),\n                         sep=\" \")\n       },\n       xlab=\"Measurement index\",\n       ylab=\"Diameter (inches)\",\n       type=c(\"p\"),\n       as.table=TRUE,\n       layout=c(4,3,1))\n\n\n\n\n\n\n\nFigure 6.6: Trellis plot of the apple diameters for the ten trees.\n\n\n\n\n\n\n\n\nCompound Graphics\nThere are two basic ways to arrange the plotting area in R for compound graphics: with par(mfrow=) and with layout.\nThe two important options to create multipanel plots with par are par(mfrow=) (or par(mfcol=)) and par(fig=). mfrow and mfcol specify the number of panels as a 2-dimensional vector of the form c(num_rows,num_cols). mfrow fills the panels by rows, mfcol fills them by columns.\nThe following code arranges two scatterplots, a histogram, and a box plot in a 2 x 2 array.\n\nattach(mtcars)\npar(mfrow=c(2,2))\nplot(wt, mpg)\nplot(wt, disp)\nhist(wt)\nboxplot(wt)\n\n\n\n\n\n\n\n\nIf you want to exercise more control over the size of the panels you can use the par(fig=) option to specify coordinates for the figure region. If you use par(fig=) you do not need to specify mfrow or mfcol. But you need to tell R to create a new plot with new=TRUE so it does not add a visualization to an existing plot.\nThe following code creates a scatterplot in the center of the graphics area with boxplots for the two variables in the margins. The par(fig=c( )) options specify the coordinates for the individual plots on the graphics area as a vector of the form c(x1, x2, y1, y2).\n\nset.seed(645)\ndat &lt;- matrix(rnorm(100),nrow=50,ncol=2)\n\npar(fig=c(0,0.8,0,0.8), new=TRUE)\nplot(dat, xlab=\"X\", ylab=\"Y\", las =1)\n\npar(fig=c(0,0.8,0.55,1), new=TRUE)\nboxplot(dat[,1], horizontal=TRUE, axes=FALSE)\n\npar(fig=c(0.65,1,0,0.8),new=TRUE)\nboxplot(dat[,2], axes=FALSE)\n\n\n\n\n\n\n\n\nThe second approach of controlling the layout of a compound graph in R is to use the layout function. Its first argiment is a matrix that specifies the location of the figures. For example, the matrix \\[\n\\left [\\begin{array}{cc} 1 & 1 \\\\ 2 & 3 \\end{array} \\right]\n\\] specifies that there are three plots, with the first occupying the first row of the multipanel plot, and plots 2 and 3 sharing the panels in the second row. The matrix \\[\n\\left [\\begin{array}{cc} 2 & 0 \\\\ 1 & 3 \\end{array} \\right]\n\\] specifies that the compound graphic is made up of three graphics. The first plot that is produced will go into the lower left corner of the graph, the second plot goes in the upper left corner, and the third plot goes into the lower right corner. That is the pattern in the following code.\n\nset.seed(6)\nx &lt;- rexp(50)\n\nlayout(matrix(c(2, 0, 1, 3), nrow=2, ncol=2, byrow=TRUE),\n       widths  =c(3, 1),\n       heights =c(1, 3), \n       respect =TRUE)\n# Margin specification in terms of lines\n# The default is c(5.1, 4.1, 4.1, 2.1)\npar(mar=c(5.1, 4.1, 0, 0)) # (bottom, left, top, right)\nplot(x, cex=2, pch=20)\n\npar(mar = c(0, 4.1, 0, 0))\nhist(x, main=\"\", bty=\"n\", axes=FALSE, ylab=\"\")\n\npar(mar = c(5.1, 0, 0, 0))\nboxplot(x, axes=FALSE, bty=\"n\")\n\n\n\n\n\n\n\n\nThe widths and heights options of layout give the relative widths of the columns and rows of the graphics area, respectively. widths=c(3, 1) specifies that the first column is three times as wide as the second column. The par(mar=) calls change the margins of the graph area to allow for the placement of the graphics elements. mar= specifies margins in terms of number of lines.\n\n\n\nFigure 6.1: From Gelman and Unwin (2013)\nFigure 6.2: The grammar of graphics; a layered approach to building data visualizations.\nFigure 6.6: Trellis plot of the apple diameters for the ten trees.\n\n\n\nGelman, A., and A. Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22: 2–28. https://www.tandfonline.com/doi/full/10.1080/10618600.2012.761137.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary Statistical Models for the Plant and Soil Sciences. CRC Press, Boca Raton.\n\n\nTufte, E. 1983. The Visual Display of Quantitative Information. Graphics Press.\n\n\n———. 2001. The Visual Display of Quantitative Information, 2nd Ed. Graphics Press.\n\n\nTukey, John W. 1993. “Graphic Comparisons of Several Linked Aspects: Alternatives and Suggested Principles.” Journal of Computational and Graphical Statistics 2 (1): 1–33. http://www.jstor.org/stable/1390951.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd Ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization. O’Reilly Media. https://clauswilke.com/dataviz/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "7  Vectors and Matrices",
    "section": "",
    "text": "7.1 Introduction\nWorking with vectors and matrices is essential for statisticians. We express the mathematics of statistics in terms of scalars, vectors, and matrices. As you move into machine learning, in particular deep learning, the horizon expands from matrices to tensors (multi-dimensional arrays). For now we stick with one-dimensional (=vectors) and two-dimensional (=matrices) arrays of real numbers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#introduction",
    "href": "matrices.html#introduction",
    "title": "7  Vectors and Matrices",
    "section": "",
    "text": "Creating Vectors and Matrices\nVectors and matrices in R are special cases of the array data type. An array can have one, two, or more dimensions as indicated by its dim() attribute. Arrays with one dimensions are vectors, two-dimensional arrays are matrices.\nIf you create a one-dimensional array, you are automatically creating a vector\n\na &lt;- c(1:5)\na\n## [1] 1 2 3 4 5\nis.vector(a)\n## [1] TRUE\n\nFrequently we create vectors of constant values, the rep function helps with that:\n\nrep(1:4)\n\n[1] 1 2 3 4\n\nrep(1:4,times=2)\n\n[1] 1 2 3 4 1 2 3 4\n\n\nTo create a regular sequence of values, use seq\n\nseq(1:5)\n## [1] 1 2 3 4 5\nseq(from=1,to=10,by=0.5)\n##  [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n## [16]  8.5  9.0  9.5 10.0\nseq(length.out=10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nTo create a matrix you can use different approaches:\n\nUse the matrix function\nConvert a vector by changing its dimensions\nCoerce a numeric object into a matrix with as.matrix\n\n\nmatrix function\n\nB &lt;- matrix(1:10,ncol=2) # default is ncol=1, byrow=FALSE\nB\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\nE &lt;- matrix(1:10,ncol=2,byrow=TRUE)\nE\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n[4,]    7    8\n[5,]    9   10\n\n\n\n\nConverting a vector into a matrix\nSince a matrix is a two-dimensional array, and a vector is one-dimensional, a simple technique for creating a matrix from a vector is to make an assignment to the dimension attribute. In the following code, a one-dimensional vector of length 20 is shaped into a \\((10 \\times 2)\\) matrix. Note that the elements of the matrix are filled in column order: the first 10 elements of the vector are assigned to the rows of column 1, the next 10 elements are assigned to the rows of column 2.\n\nset.seed(876)\nx &lt;- round(rnorm(20),3)\nx\n##  [1]  0.171  0.782 -1.064 -0.264  0.114  1.940  0.461  1.226 -0.415  0.013\n## [11]  1.154  0.837  0.247 -0.916  0.378 -1.406 -0.367  0.127 -0.848 -0.146\ndim(x) &lt;- c(10,2)\nx\n##         [,1]   [,2]\n##  [1,]  0.171  1.154\n##  [2,]  0.782  0.837\n##  [3,] -1.064  0.247\n##  [4,] -0.264 -0.916\n##  [5,]  0.114  0.378\n##  [6,]  1.940 -1.406\n##  [7,]  0.461 -0.367\n##  [8,]  1.226  0.127\n##  [9,] -0.415 -0.848\n## [10,]  0.013 -0.146\nis.matrix(x)\n## [1] TRUE\n\nWhat if you want to create a \\((10 \\times 2)\\) matrix but fill the matrix in row-order: the first two elements in row 1, the next two elements in row 2, and so forth? The solution is to assign the dimensions in reverse order and transpose the result.\n\nx &lt;- round(rnorm(20),3)\nx\n##  [1]  0.742  1.415  1.603 -0.124 -0.828 -0.138  0.152  0.425 -0.159 -0.837\n## [11]  0.364 -0.373 -0.375  0.194  0.238 -0.740  2.435  1.573  1.117 -1.773\ndim(x) &lt;- c(2,10)\nt(x)\n##         [,1]   [,2]\n##  [1,]  0.742  1.415\n##  [2,]  1.603 -0.124\n##  [3,] -0.828 -0.138\n##  [4,]  0.152  0.425\n##  [5,] -0.159 -0.837\n##  [6,]  0.364 -0.373\n##  [7,] -0.375  0.194\n##  [8,]  0.238 -0.740\n##  [9,]  2.435  1.573\n## [10,]  1.117 -1.773\n\nConverting a vector into a matrix by changing its dimensions has the advantage that the object is not copied, saving memory.\n\n\nCoercion\nR is good at coercion, the implicit conversion from one data type to another. Coercion can happens implicitly when you pass an object of a different type to a function. Coercion can also be done explicitly using as.*-style functions.\n\n\n\n\n\n\nNote\n\n\n\n\n\nI really meant “as.*-style functions” as in as.matrix, as.data.frame, as.Date, as.dendrogram, etc. Not as*-style functions.\n\n\n\nFor example, to coerce an R object into a matrix, use the as.matrix function. A common usage is to convert a dataframe of numerical data:\n\ndf &lt;- data.frame(int=rep(1,4), x1=c(1,2,3,4), x2=rnorm(4))\nis.matrix(df)\n\n[1] FALSE\n\nis.matrix(as.matrix(df))\n\n[1] TRUE\n\n\n\n\n\nBasic Operations\nWhen operating on matrices and vectors, we need to distinguish elementwise operations from true matrix operations. For example, take the \\((5 \\times 2)\\) matrices B and E created earlier. Their elementwise product is the matrix with typical element \\([b_{ij}*e_{ij}]\\). In other words, elements of the matrices are matched up and the multiplication is performed separately in each cell.\nThe matrix product \\(\\textbf{B}* \\textbf{E}\\) is not possible because the matrices do not conform to matrix multiplication. However, the product \\(\\textbf{B}* \\textbf{E}^\\prime\\) is possible, the result is a (5 )$ matrix.\n\nElementwise operations\n\nB\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\n\nE\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\n## [3,]    5    6\n## [4,]    7    8\n## [5,]    9   10\n\n# Elementwise addition\n5 + B\n##      [,1] [,2]\n## [1,]    6   11\n## [2,]    7   12\n## [3,]    8   13\n## [4,]    9   14\n## [5,]   10   15\na + B\n##      [,1] [,2]\n## [1,]    2    7\n## [2,]    4    9\n## [3,]    6   11\n## [4,]    8   13\n## [5,]   10   15\nB + E\n##      [,1] [,2]\n## [1,]    2    8\n## [2,]    5   11\n## [3,]    8   14\n## [4,]   11   17\n## [5,]   14   20\n\n# Elementwise multiplication\n5 * B\n##      [,1] [,2]\n## [1,]    5   30\n## [2,]   10   35\n## [3,]   15   40\n## [4,]   20   45\n## [5,]   25   50\na * B\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    4   14\n## [3,]    9   24\n## [4,]   16   36\n## [5,]   25   50\nB * E\n##      [,1] [,2]\n## [1,]    1   12\n## [2,]    6   28\n## [3,]   15   48\n## [4,]   28   72\n## [5,]   45  100\n\nNote that when the dimensions of the two arrays do not match up, values are repeated as necessary. For example, in 5+B, the scalar 5 is applied to each cell of B. The operation is essentially the same as\n\nmatrix(rep(5,10),5,2) + B\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    7   12\n[3,]    8   13\n[4,]    9   14\n[5,]   10   15\n\n\nSimilarly, in a+B, where a is a vector of length 5, the vector is added elementwise to the first column of B, then to the second column of B.\nYou can also perform elementwise logical operations, creating vectors and matrices of TRUE/FALSE values.\n\n(B==3) | (B==2)\n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n[3,]  TRUE FALSE\n[4,] FALSE FALSE\n[5,] FALSE FALSE\n\n\n\n\nMatrix operations\n\nTransposition\nA matrix is transposed, its rows and columns exchanged, with the t() operator. Transposition exchanges the dimensions of the underlying array.\n\nB\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\ndim(B)\n## [1] 5 2\nt(B)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\ndim(t(B))\n## [1] 2 5\n\n\n\nMultiplication\nMatrix multiplication is performed with the %*% operator. For this operation to succeed, the matrices have to conform to multiplication, that is, the number of columns of the matrix on the left side of the multiplication must equal the number of rows on the right side.\nThis will fail:\n\nB %*% E\n\nError in B %*% E: non-conformable arguments\n\n\nAnd this product succeeds, since \\(\\textbf{B}\\) and \\(\\textbf{E}^\\prime\\) conform to multiplication.\n\nB %*% t(E)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   13   27   41   55   69\n[2,]   16   34   52   70   88\n[3,]   19   41   63   85  107\n[4,]   22   48   74  100  126\n[5,]   25   55   85  115  145\n\n\n\n\nDiagonal matrices\nIn statistics we frequently encounter diagonal matrices, square matrices with zeros in off-diagonal cells. Programmatically, two important situations arise:\n\nExtracting the diagonal elements of a matrix\nForming a diagonal matrix from a vector\n\nThe diag function handles both cases\n\n# The (3 x 3) identity matrix\ndiag(1,3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n# A (3 x 3) diagonal matrix with 1, 2, 3 on the diagonal\ndiag(1:3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n\n\n diag(B %*% t(E))\n## [1]  13  34  63 100 145\n\nC_ &lt;- matrix(1:9 + rnorm(9,0,1e-3),ncol=3)\nC_\n##          [,1]     [,2]     [,3]\n## [1,] 0.999239 4.000457 7.000228\n## [2,] 1.999412 4.999930 7.999253\n## [3,] 3.000382 6.000139 9.000183\ndiag(C_)\n## [1] 0.999239 4.999930 9.000183\n\nThe trace of a matrix is the sum of its diagonal elements. You can compute the trace by combining sum and diag functions:\n\n# The trace of matrix C_\nsum(diag(C_))\n\n[1] 14.99935\n\n\n\n\nCrossproduct matrix\nThe crossproduct of matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) is \\(\\textbf{A}^\\prime \\textbf{B}\\) provided that \\(\\textbf{A}^\\prime\\) and \\(\\textbf{B}\\) are conformable for multiplication. The most important crossproduct matrices in statistics are crossproducts of a matrix with itself: \\(\\textbf{A}^\\prime\\textbf{A}\\). These crossproducts are square, symmetric matrices.\nYou can calculate a crossproduct matrix directly using matrix multiplication, or a dedicated function (base::crossprod or Matrix::crossprod). The dedicated functions are slightly smaller than computing the product directly, but I have found the difference to be pretty negligible, even for large matrices.\n\nX &lt;- matrix(rnorm(300),nrow=100,ncol=3)\n\n# Computing X`X by direct multiplication\nXpX &lt;- t(X) %*% X\nXpX\n\n          [,1]      [,2]       [,3]\n[1,]  81.91635 12.696119 -13.566906\n[2,]  12.69612 83.278958  -1.086481\n[3,] -13.56691 -1.086481 102.548803\n\n# Computing X`X using the crossprod() function\ncrossprod(X)\n\n          [,1]      [,2]       [,3]\n[1,]  81.91635 12.696119 -13.566906\n[2,]  12.69612 83.278958  -1.086481\n[3,] -13.56691 -1.086481 102.548803\n\n\n\n\nInverse matrix\nThe inverse of square matrix \\(\\textbf{A}\\), denoted \\(\\textbf{A}^{-1}\\), if it exists, is the multiplicative identity: \\[\n\\textbf{A}^{-1}\\textbf{A}= \\textbf{A}\\textbf{A}^{-1} = \\textbf{I}\n\\] The inverse exists if \\(\\textbf{A}\\) is of full rank–we say that than the matrix is non-singular.\n\nlibrary(Matrix)\nrankMatrix(XpX)[1]\n\n[1] 3\n\n\nThe matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) in our example has rank 3, which equals its number of rows (columns). The matrix is thus of full rank and can be inverted. Computing the inverse matrix is a special case of using the solve function. solve(a,b,...) solves a linear system of equation of the form \\[\n\\textbf{A}\\textbf{X}= \\textbf{B}\n\\] When the function is called without the b argument, it returns the inverse of a:\n\nXpX_inverse &lt;- solve(XpX)\n\nXpX_inverse\n##             [,1]          [,2]          [,3]\n## [1,]  0.01278294 -0.0019270000  0.0016707297\n## [2,] -0.00192700  0.0122999860 -0.0001246209\n## [3,]  0.00167073 -0.0001246209  0.0099711670\n\nround(XpX %*% XpX_inverse,4)\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#least-squares-from-scratch",
    "href": "matrices.html#least-squares-from-scratch",
    "title": "7  Vectors and Matrices",
    "section": "7.2 Least Squares from Scratch",
    "text": "7.2 Least Squares from Scratch\nCoding algorithms in statistical programming often starts with reproducing the formulas on paper in a programming language.\nSuppose we wish to fit a multiple linear regression model with target variable (output) \\(Y\\) and predictor variables (inputs) \\(X_1, \\cdots, X_p\\). The \\(n\\) data points for this analysis are arranged in an \\((n \\times 1)\\) vector \\[\n\\textbf{Y} = [Y_1, \\cdots, Y_n]^\\prime\n\\] an \\((n \\times (p+1))\\) matrix \\[\n\\textbf{X} = \\left [ \\begin{array}{ccc}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{1n} & \\cdots & x_{pn}\n\\end{array}\\right]\n\\] and an \\((n \\times 1)\\) vector of error terms \\[\n\\boldsymbol{\\epsilon} = [\\epsilon_1, \\cdots, \\epsilon_n]^\\prime\n\\]\nThe complete regression model can be written as \\[\n\\textbf{Y} = \\textbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\qquad \\boldsymbol{\\epsilon} \\sim (\\textbf{0},\\sigma^2 \\textbf{I})\n\\] The statement on the right says that the model errors have mean zero, equal variance \\(\\sigma^2\\) and are uncorrelated—this is also sometimes called the iid assumption (identically and independently distributed), although lack of correlation does not strictly imply independence.\nThe parameter vector \\(\\boldsymbol{\\beta}\\) in this model is typically estimated by ordinary least squares (OLS), the solution is \\[\n\\widehat{\\boldsymbol{\\beta}} = \\left (\\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] provided that \\(\\textbf{X}\\) is of full column rank (which implies \\(\\textbf{X}^\\prime\\textbf{X}\\) is non-singular and (\\(\\textbf{X}^\\prime\\textbf{X})^{-1}\\) exists) and the predicted values are \\[\n\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\n\\]\nLet’s use a data set and compute the OLS estimates and the predicted values using matrix–vector operations, then compare the results to the standard output of the linear modeling function lm() in R.\nThe data set for this exercise is the fitness data set. The data comprise measurements of aerobic capacity and other attributes on 31 men involved in a physical fitness course at N.C. State University.\nAerobic capacity is the ability of the heart and lungs to provide the body with oxygen. It is a measure of fitness and expressed as the oxygen intake in ml per kg body weight per minute. Measuring aerobic capacity is expensive and time consuming compared to attributes such as age, weight, and pulse. The question is whether aerobic capacity can be predicted from the easily measurable attributes. If so, a predictive equation can reduce time and effort to assess aerobic capacity.\nThe variables are\n\nAge: age in years\nWeight: weight in kg\nOxygen: oxygen intake rate (ml per kg body weight per minute)\nRunTime: time to run 1.5 miles (minutes)\nRestPulse: heart rate while resting\nRunPulse: heart rate while running (same time Oxygen rate measured)\nMaxPulse: maximum heart rate recorded while running\n\nThe linear model we have in mind is \\[\n\\text{Oxygen}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\beta_2\\text{Weight}_i + \\beta_3\\text{RunTime}_i + \\beta_4\\text{RestPulse}_i + \\beta_5\\text{RunPulse}_i + \\beta_6\\text{MaxPulse}_i + \\epsilon_i\n\\] \\(i=1,\\cdots,31\\).\nThe following code makes a connection to the ads DuckDB database, loads the fitness table into an R dataframe, displays the first 6 observations, and closes the connection to the database again.\n\nlibrary(\"duckdb\")\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nfit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\ndbDisconnect(con)\n\nThe target variable for the linear model is Oxygen, the remaining variables are inputs to the regression. The next statements create the \\(\\textbf{y}\\) vector and the \\(\\textbf{X}\\) matrix for the model. Note that the first column of \\(\\textbf{X}\\) is a vector of ones, representing the intercept \\(\\beta_0\\).\n\ny &lt;- as.matrix(fit[,which(names(fit)==\"Oxygen\")])\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\nhead(X)\n\n     Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\n[1,]      1  44  89.47   11.37        62      178      182\n[2,]      1  40  75.07   10.07        62      185      185\n[3,]      1  44  85.84    8.65        45      156      168\n[4,]      1  42  68.15    8.17        40      166      172\n[5,]      1  38  89.02    9.22        55      178      180\n[6,]      1  47  77.45   11.63        58      176      176\n\n\nNext we are building the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix and compute its inverse, \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\), with the solve() function. t() transposes a matrix and %*% indicates that we are performing matrix multiplication rather than elementwise multiplication.\n\nXpX &lt;- t(X) %*% X\nXpXInv &lt;- solve(XpX)\n\nWe can verify that XpxInv is indeed the inverse of XpX by multiplying the two. This should yield the identity matrix\n\nround(XpX %*% XpXInv,3)\n\n          Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\nIntcpt         1   0      0       0         0        0        0\nAge            0   1      0       0         0        0        0\nWeight         0   0      1       0         0        0        0\nRunTime        0   0      0       1         0        0        0\nRestPulse      0   0      0       0         1        0        0\nRunPulse       0   0      0       0         0        1        0\nMaxPulse       0   0      0       0         0        0        1\n\n\nNext we compute the OLS estimate of \\(\\boldsymbol{\\beta}\\) and the predicted values \\(\\widehat{\\textbf{y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\).\n\nbeta_hat &lt;- XpXInv %*% t(X) %*% y\nbeta_hat\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\ny_hat &lt;- X %*% beta_hat\n\nThe estimate of the intercept is \\(\\widehat{\\beta}_0\\) = 102.9345, the estimate of the coefficient for Age is \\(\\widehat{\\beta}_1\\) = -0.227 and so on.\nWe could have also used the solve function in its intended application, to solve a system of linear equations–we abused the behavior of solve a bit by using it with only one argument; it will then return the inverse matrix of the argument.\nThe linear system to solve in the ordinary least squares problem is \\[\n\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}= \\textbf{X}^\\prime\\textbf{Y}\n\\] This system of equations is called the normal equations. The solution can be computed with the solve function:\n\nsolve(XpX,crossprod(X,y))\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\n\nThe results match beta_hat computed earlier.\nThe residuals \\(\\widehat{\\boldsymbol{\\epsilon}} = \\textbf{y}- \\widehat{\\textbf{y}}\\) and the error sum of squares\n\\[\n\\text{SSE} = (\\textbf{y}- \\widehat{\\textbf{y}} )^\\prime (\\textbf{y}- \\widehat{\\textbf{y}}) = \\sum_{i=1}^n \\left(y_i - \\widehat{y}_i\\right)^2\\] and the estimate of the residual variance \\[\\widehat{\\sigma}^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\n\\]\nare computed as\n\nresiduals &lt;- y - y_hat\nSSE &lt;- sum(residuals^2)\nn &lt;- nrow(fit)\nrankX &lt;- rankMatrix(XpX)[1]\nsigma2_hat &lt;- SSE/(n - rankX)\n\nSSE\n\n[1] 128.8379\n\nsigma2_hat\n\n[1] 5.368247\n\n\nWe used the rankMatrix function in the Matrix package to compute the rank of \\(\\textbf{X}\\), which is identical to the rank of \\(\\textbf{X}^\\prime\\textbf{X}\\). With these quantities available, the variance-covariance matrix of \\(\\widehat{\\boldsymbol{\\beta}}\\), \\[\\\n\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\textbf{X}^\\prime\\textbf{X})^{-1}\n\\] can be estimated by substituting \\(\\widehat{\\sigma}^2\\) for \\(\\sigma^2\\). The standard errors of the regression coefficient estimates are the square roots of the diagonal values of this matrix.\n\nVar_beta_hat &lt;- sigma2_hat * XpXInv\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nse_beta_hat\n\n     Intcpt         Age      Weight     RunTime   RestPulse    RunPulse \n12.40325810  0.09983747  0.05459316  0.38456220  0.06605428  0.11985294 \n   MaxPulse \n 0.13649519 \n\n\nNow let’s compare our results to the output from the lm() function in R.\n\nlinmod &lt;- lm(Oxygen ~ ., data=fit)\nsummary(linmod)\n\n\nCall:\nlm(formula = Oxygen ~ ., data = fit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4026 -0.8991  0.0706  1.0496  5.3847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 102.93448   12.40326   8.299 1.64e-08 ***\nAge          -0.22697    0.09984  -2.273  0.03224 *  \nWeight       -0.07418    0.05459  -1.359  0.18687    \nRunTime      -2.62865    0.38456  -6.835 4.54e-07 ***\nRestPulse    -0.02153    0.06605  -0.326  0.74725    \nRunPulse     -0.36963    0.11985  -3.084  0.00508 ** \nMaxPulse      0.30322    0.13650   2.221  0.03601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.317 on 24 degrees of freedom\nMultiple R-squared:  0.8487,    Adjusted R-squared:  0.8108 \nF-statistic: 22.43 on 6 and 24 DF,  p-value: 9.715e-09\n\n\nBased on the quantities calculated earlier, the following code reproduces the lm summary.\n\ntvals &lt;- beta_hat/se_beta_hat\npvals &lt;- 2*(1-pt(abs(tvals),n-rankX))\nresult &lt;- cbind(beta_hat, se_beta_hat, tvals, pvals)\ncolnames(result) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"Pr(&gt;|t|)\")\nround(result,5)\n\n           Estimate Std. Error  t value Pr(&gt;|t|)\nIntcpt    102.93448   12.40326  8.29899  0.00000\nAge        -0.22697    0.09984 -2.27343  0.03224\nWeight     -0.07418    0.05459 -1.35873  0.18687\nRunTime    -2.62865    0.38456 -6.83544  0.00000\nRestPulse  -0.02153    0.06605 -0.32600  0.74725\nRunPulse   -0.36963    0.11985 -3.08401  0.00508\nMaxPulse    0.30322    0.13650  2.22145  0.03601\n\ncat(\"\\nResidual standard error: \", sqrt(sigma2_hat),\" on \", n-rankX, \"degrees of freedom\\n\")\n\n\nResidual standard error:  2.316948  on  24 degrees of freedom\n\nSST &lt;- sum( (y -mean(y))^2 )\ncat(\"Multiple R-squared: \", 1-SSE/SST, \n    \"Adjusted R-squared: \", 1 - (SSE/SST)*(n-1)/(n-rankX), \"\\n\")\n\nMultiple R-squared:  0.8486719 Adjusted R-squared:  0.8108399 \n\nFstat &lt;- ((SST-SSE)/(rankX-1)) / (SSE/(n-rankX))\ncat(\"F-statistic: \", Fstat, \"on \", \n    rankX-1, \"and\", n-rankX, \"DF, p-value:\", 1-pf(Fstat,rankX-1,n-rankX))\n\nF-statistic:  22.43263 on  6 and 24 DF, p-value: 9.715305e-09",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#building-a-model-matrix",
    "href": "matrices.html#building-a-model-matrix",
    "title": "7  Vectors and Matrices",
    "section": "7.3 Building a Model Matrix",
    "text": "7.3 Building a Model Matrix\nIn the previous example, the model matrix \\(\\textbf{X}\\) was formed in code by appending a \\((31 \\times 6)\\) matrix of input variables to a \\(31 \\times 1\\) vector of ones:\n\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\n\nThe lm function used a special syntax to specify the model, called a model formula: Oxygen ~ .. The formula specifies the target variable (the dependent variable) on the left side of the tilde and the input (predictor, independent) variables on the right hand side of the tilde. The special dot syntax implies to include all variables in the data frame (except for Oxygen) as input variables for the right hand side of the model. Also, the intercept is automatically included in model formulas, because not having an intercept is a special case in statistical modeling.\nYou can generate a model matrix easily by using the model.matrix function with a model formula. In the fitness example,\n\nX_ &lt;- model.matrix(Oxygen ~ ., data=fit)\n\nThe two matrices are identical, as you can see with\n\nsum(X - X_)\n\n[1] 0\n\n\nIf we wanted to repeat the regression calculations for a model that contains only Age and MaxPulse as predictor variables, it would be easy to construct the model matrix with\n\nX_small_model &lt;- model.matrix(Oxygen ~ Age + MaxPulse, data=fit)\n\nUsing model.matrix is very convenient when you work with factors, classification input variables that are not represented in the model by their actual values (which could be strings) but by converting a variable with \\(k\\) unique values into \\(k\\) columns in \\(\\textbf{X}\\). These columns use 0/1 values to encode which level of the classification input variable matches the value for an observation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "8  Random Numbers",
    "section": "",
    "text": "8.1 Pseudo Random Number Generation\nMost random number generators (RNGs) are not generating true random numbers in the sense that the sequence of numbers is impossible to foresee. They are pseudo random number generators (PRNGs); they produce predetermined streams of numbers that appear random.\nAn example of a true random number generator, and probably a surprising one, is the wall of lava lamps in the lobby of the San Francisco office of internet infrastructure company Cloudflare.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "random.html#pseudo-random-number-generation",
    "href": "random.html#pseudo-random-number-generation",
    "title": "8  Random Numbers",
    "section": "",
    "text": "Example: Cloudflare Lava Lamps: LavaRand\n\n\nIn the lobby of the San Francisco office of internet service provider Cloudflare is a wall of eighty lava lamps (Figure 8.1). This is not some retro thing, but part of the mission-critical operations of the internet infrastructure company. The lava lamps are used to generate cryptographic keys to secure the internet communications and services. Cloudflare is so convinced that the keys generated are unbreakable, because they are truly unpredictable, that they blog about how the lava-lamp key generator works; see here.\nTrue cryptography requires that keys are unpredictable, something that computers are not good at. By definition, the operations of a computer are predictable, they follow a precise program. The same input will produce the same output—every time. The reliability of computers is bad for cryptography. You can make things better by making the input to a RNG random itself, known as a cryptographically-secure pseudorandom number generators (CSPRNGs). That would make the sequence less predictable but where does the random input come from? Another “predictable” random number generator?\n\n\n\n\n\n\nFigure 8.1: Lava lamps in the lobby of the headquarters of Cloudflare. Source.\n\n\n\nCloudflare uses the wall of 80 lava lamps to generate unpredictable input using a camera that continuously looks at the wall and takes video of the state of the lamps. From the Cloudflare blog:\n\nLavaRand is a system that uses lava lamps as a secondary source of randomness for our production servers. A wall of lava lamps in the lobby of our San Francisco office provides an unpredictable input to a camera aimed at the wall. A video feed from the camera is fed into a CSPRNG, and that CSPRNG provides a stream of random values that can be used as an extra source of randomness by our production servers. Since the flow of the “lava” in a lava lamp is very unpredictable, “measuring” the lamps by taking footage of them is a good way to obtain unpredictable randomness.\n\nComputers are predictable, the physical real world is not!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "random.html#how-to-generate-random-numbers",
    "href": "random.html#how-to-generate-random-numbers",
    "title": "8  Random Numbers",
    "section": "8.2 How to Generate Random Numbers",
    "text": "8.2 How to Generate Random Numbers\n\nQuantile Functions\nCan you generate random numbers for one distribution from random numbers for another distribution? If the answer is “Yes” this could make it easier to write random number generators, a basic generator could be used to transform random numbers from one distribution into those of another. Fortunately, the answer is indeed “Yes” and the results leading there are very cool properties of cumulative distribution (c.d.f.) and quantile functions.\n\n\nDefinition: Quantile Function\n\n\nIf \\(Y\\) is a random variable with c.d.f. \\(F(y) = \\Pr(Y \\le y)\\), then the quantile function \\(Q(p)\\) is the inverse of the cumulative distribution function; for a given probability \\(p\\), it returns the value \\(y\\) for which \\(F(y) = p\\): \\[\nQ(p) = F^{-1}(p)\n\\]\n\n\nSo, if \\(F\\) maps \\(F:\\mathbb{R} \\rightarrow [0,1]\\), the quantile function maps \\(Q:[0,1] \\rightarrow \\mathbb{R}\\).\nHere is a possibly surprising result: if \\(Y\\) has c.d.f. \\(F(y)\\), then we can think of the c.d.f as a transformation of \\(Y\\). What would its distribution look like? The following R code draws 1,000 samples from a G(\\(2,1.5^2\\)) distribution and plots the histogram of the c.d.f. values \\(F(y)\\).\n\n# See section below on setting random number seed values\nset.seed(455675)\nyn &lt;- rnorm(1000,mean=2,sd=1.5)\nF_yn &lt;- pnorm(yn,mean=2,sd=1.5)    \nhist(F_yn,main=\"\")\n\n\n\n\n\n\n\n\nThe distribution of \\(F(y)\\) is uniform on (0,1)—this is true for any distribution, not just the Gaussian.\nWe can combine this result with the following, possibly also surprising, result: If \\(U\\) is a uniform random variable, and \\(Q(p)\\) is the quantile function of \\(Y\\), then \\(Q(u)\\) has the same distribution as \\(Y\\). This suggests a method to generate random numbers from any distribution if you have a generator of uniform random numbers: plug the uniform random numbers into the quantile function. Figure 8.2 shows this for G(2,1.5^2) random numbers and Figure 8.3 for Beta(1.5,3) random numbers.\n\nnorm_rv &lt;- qnorm(runif(1000),mean=2,sd=1.5)\nhist(norm_rv,main=\"\",xlab=\"Y\")\n\n\n\n\n\n\n\nFigure 8.2: G(2,1.5^2) random variables via quantile transform of U(0,1) random numbers.\n\n\n\n\n\n\nbeta_rv &lt;- qbeta(runif(1000),shape1=1.5,shape2=3)\nhist(beta_rv,main=\"\",xlab=\"Y\")\n\n\n\n\n\n\n\nFigure 8.3: Beta(1.5,3) random variables via quantile transform of U(0,1) random numbers.\n\n\n\n\n\nBecause the quantile function is the inverse c.d.f., this method of generating random numbers is also known as the inversion method.\n\n\nTransformations\nAnother approach to generate random numbers based on uniform numbers applies known transformations. The Box-Muller method (Box and Muller 1958), for example, generates pairs of independent standard Gaussian random variables from pairs of independent uniform random variables. If \\(U_1\\) and \\(U_2\\) are independent draws from a uniform distribution on [0,1], then \\[\n\\begin{align*}\nZ_1 &= \\sqrt{-2\\log U_1} \\cos(2\\pi U_2) \\\\\nZ_2 &= \\sqrt{-2\\log U_1} \\sin(2\\pi U_2)\n\\end{align*}\n\\] are independent random variables with G(0,1) distribution. The Box-Muller random number generator is an example of transforming uniform variables into normal variables without going through the quantile function.\nA general, and very intuitive, method to create random numbers from any distribution is based on acceptance-rejection sampling.\n\n\nAcceptance-Rejection Method\nWhat if we do not know the quantile function \\(Q(p)\\), and all we have is the density or mass function \\(p(y)\\) of the variable whose random numbers we wish to generate? Such a situation might arise when we observe an empirical distribution function and now want to generate a sequence of random numbers from the particular distribution. Again, we can use uniform random numbers and “transform” them in such a way that the long-run frequency distribution of the transform matches the target distribution. Figure 8.4 depicts the idea graphically.\nThe target distribution we wish to draw random numbers from is \\(p(y)\\). We know the density to the extent that we can evaluate whether a pair of points \\((y,U(y))\\) falls above or below \\(p(y)\\). This allows us to transform a sequence of uniform random numbers drawn between \\([\\min(y),\\max(y)]\\) into a sequence of random numbers from \\(p(y)\\).\n\n\n\n\n\n\nFigure 8.4: Random numbers via acceptance-rejection sampling.\n\n\n\nThe acceptance-rejection algorithm to sample from \\(p(y)\\) based on \\(U(\\min(y),\\max(y))\\) random variables is very simple (Figure 8.5):\n\nDraw a random number \\(X \\sim U(\\min\\{y\\}, \\max\\{y\\})\\)\nDraw a random number \\(Y \\sim U(\\min\\{p(y)\\}, \\max\\{p(y)\\})\\)\nIf \\(Y \\le p(X)\\), accept \\(X\\) as a draw from \\(p(y)\\). Otherwise, reject \\(X\\) and return to 1.\n\n\n\n\n\n\n\nFigure 8.5: Random numbers via acceptance-rejection sampling.\n\n\n\nYou continue this acceptance-rejection decision until you have accepted enough random numbers.\n\n\nExample: Rejection Method for Beta(\\(\\alpha,\\beta\\))\n\n\nSuppose we wish to generate 1,000 random numbers from a Beta distribution and we do not have access to a quantile function. However, we can evaluate the density of the Beta random variable \\(Y\\), \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\]\nIn R we can calculate this with the dbeta(y,alpha,beta) function. The following code uses a simple while loop to fill the pre-allocated vector rbet with Beta(1.5,3) random variables. The maximum density of a Beta(1.5,3) is about \\(\\max\\{p(y)\\} = 1.87\\); this value is used in generating the random number for the vertical axis.\n\nrbet &lt;- rep(NA,1000)\nnumacc &lt;- 0\nntries &lt;- 0\nwhile (numacc &lt; length(rbet)) {\n    # Draw independent random numbers for abscissa and ordinate\n    x &lt;- runif(1,0,1   )\n    y &lt;- runif(1,0,1.87)\n    # Check whether to accept or reject X depending on \n    # whether Y is below or above the target density\n    if (y &lt;= dbeta(x,1.5,3)) {\n        numacc &lt;- numacc+1\n        rbet[numacc] &lt;- x\n    }\n    ntries &lt;- ntries + 1\n}\nntries\n\n[1] 1848\n\nhist(rbet,main=\"\")\n\n\n\n\n\n\n\nFigure 8.6: Beta(1.5,3) random variables by acceptance-rejection sampling.\n\n\n\n\n\nThe distribution of the random numbers generated via quantile transform (Figure 8.3) and via acceptance-rejection (Figure 8.6) are very similar. Note that the acceptance method performs 1848 attempts, each requires two uniform random numbers. In total, the method uses 3696 uniform random numbers and 1848 evaluations of the Beta density function to create the random sequence of 1,000 Beta(1.5,3) draws. The U(0,1) is a special case of the Beta distributions with \\(\\alpha = \\beta = 1\\). The target distribution in this example is not that different from the sampled distribution. When the two distributions are more distinct, you will reject more samples along the way.\n\n\nIf you were to use acceptance-rejection sampling to generate data from a Beta(1,1) distribution, what would be the proportion of accepted samples?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "random.html#random-numbers-in-r",
    "href": "random.html#random-numbers-in-r",
    "title": "8  Random Numbers",
    "section": "8.3 Random Numbers in R",
    "text": "8.3 Random Numbers in R\nOne of the most commonly used random number generators for uniform random variables is the Mersenne Twister (MT) algorithm of Matsumoto and Nishimura (1998). The MT algorithm is the default RNG in many commercial and open source packages, for example, in R, SAS, SPSS, Stata.\nThe period of the Mersenne Twister algorithm is based on a Mersenne prime, from which it derives its name. The period of a PRNG is the sequence length after which the generated numbers repeat themselves. The period of the MT algorithm is very long, \\(2^{19937}-1\\). Obviously, you want the period to be very large, but that in itself does not guarantee a high quality random number generator. The algorithms also need to pass statistical tests that analyze the distribution of the generated sequences, for example the battery of “Diehard Tests”.\nOne such test examines whether successive pairs of observations drawn from U(0,1) evenly fill the unit square (Figure 8.7).\n\nm = 40000\npar(mfrow=c(1,2))\nfor (i in 1:2) {\n    u = runif(m);  \n    plot(u[1:(m-1)], u[2:m], pch=\".\",\n         xlab=\"Value\",\n         ylab=\"Next value\")\n}\n\n\n\n\n\n\n\nFigure 8.7: Two sets of succesive pairs of 40,000 U(0,1) random numbers plotted on the unit square.\n\n\n\n\n\nThe MT algorithm performs well on these tests, but it is not perfect—no pseudo-random number generator is. For example, if you wish to generate independent sequences of random numbers, choosing different seed values for each sequence is not the way to go. You should instead use different segments of the same sequence.\nThe RNGkind() function in R returns a list of three strings that identify\n\nthe default random number generator\nthe method to create G(0,1) random numbers\nthe algorithm used to create random numbers from a discrete uniform distribution\n\n\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"       \n\n\nIn this setting, uniform random numbers are generated with the Mersenne Twister algorithm, Gaussian variates are generated by quantile inversion, and discrete uniform random numbers are produced with an acceptance-rejection algorithm.\nThe RNGkind() function is also used to change the RNGs. For example, to use the Box-Muller algorithm instead of inversion for Gaussian random variables, specify\n\nRNGkind(normal.kind=\"Box-Muller\")\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Box-Muller\"       \"Rejection\"       \n\n\nTo reset the assignment to the default, use\n\nRNGkind(normal.kind=\"default\")\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"       \n\n\nThe state of a pseudo RNG is represented by a series of numbers, for the MT algorithm this is a vector of 624 integers. Once the state has been initialized, random numbers are generated according to the algorithm. Each time a random number is drawn, the state of the generator advances.\nThe state of the RNG can be seen with the .Random.seed variable, a vector of integers. The first value is an encoding of the three elements of RNGkind(). For the MT, .Random.seed[3:626] is the 624-dimensional integer vector of the RNG’s state. It is not recommended to make direct assignments to .Random.seed. Instead, you should initialize the RNG to a particular state through the seed value.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "random.html#seed-values",
    "href": "random.html#seed-values",
    "title": "8  Random Numbers",
    "section": "8.4 Seed Values",
    "text": "8.4 Seed Values\nThe random number seed is an integer that initializes the random number generator. What that initialization looks like depends on the RNG. The goal is the same, however: to place the RNG in a known state that generates a known sequence of random numbers. This makes statistical programs that depend on random numbers reproducible in the sense that the exact same results are generated each time the program runs. This might be necessary for auditing, to verify what the program does, and to lock the results for a production environment.\nThe set.seed function in R accomplishes that. Once the seed is set to a given value, the same random number sequence results.\n\nset.seed(123)\nrunif(4)\n\n[1] 0.2875775 0.7883051 0.4089769 0.8830174\n\nrunif(2)\n\n[1] 0.9404673 0.0455565\n\nset.seed(123)\nrunif(6)\n\n[1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565\n\n\nWhether you draw 4 and then 2 random numbers or 6 numbers at once, the same sequence results.\nIf you do not set the seed, the system starts with an initial seed, usually chosen based on the system clock and/or the process ID, and the state of the RNG will differ from session to session. Calling set.seed(NULL) re-initializes the state of the RNG as if no seed had been set.\nWhen it comes to seed values, here are some best practices\n\nDo not go “seed hunting”. Changing the seed values until the results support a particular narrative is bad practice and borders on unethical behavior. If the simulation does not show what you expect, after setting a seed value, the problem might be that you are not drawing enough samples or your program might be incorrect. Do not try to “fix” things by experimenting with seed values.\nTo generate independent sequences of random numbers, do not use separate seeds for the sequences. Use a single seed and extract different segments from a single stream. Choosing an RNG with a long period is important in this case.\nSome functions allow that seeds for their internal random number generation are passed as function arguments. It becomes confusing to use multiple methods throughout a program to affect random number streams. RNGs should be initialized once using set.seed(), preferably at the beginning of the program, and this should be clearly documented.\nDo not use the same seed over and over. A best practice is to choose the seed itself based on a (true) random number generator. An acceptable method is to use the default initialization of the RNG and use one of the state variables as the seed; for example:\n\n\nset.seed(NULL)\n.Random.seed[10]\n\n[1] -242599482\n\nset.seed(.Random.seed[10])\n\nYou need to keep track of the chosen seed value outside of the program, otherwise the results are not reproducible.\n\nBe aware that random numbers can be involved in subtle and unexpected ways in many parts of the code. Jittering observations on plots to avoid overlaying data points involves adding small random amounts—this will advance the internal state of the RNG. Subsampling, cross-validation, bootstrapping, train:test splits, measuring feature importance by permutation, are other examples of RNG-dependent techniques.\nWhen you work with a program interactively, and execute the code in chunks, remember that the internal state of the RNG advances with each call to a random number function. Setting the seed once, then executing a code chunk with RNG three times in a row while debugging, then moving on to a second chunk that depends on random numbers will yield different results compared to running the code top-to-bottom or in batch mode.\n\n\nIt is not always possible to take complete control of random number streams in your statistical program. Stochastic elements can be involved in ways that you cannot fix with a set.seed() command. For example, when you fit artificial neural networks (ANNs) in R using the keras package, the code executes the underlying routines written in Python. Setting a seed in the R session does not affect the random number generator in the NumPy Python library. Even worse, Keras supports multiple machine learning frameworks. If it runs on TensorFlow, for example, TensorFlow has its own random number generator that is used in addition to the NumPy generator.\nWhen code runs on multi-core CPUs or on GPUs (graphical processing units) it can (should) take advantage of threaded execution. This introduces a non-deterministic element to the computations that can result in slightly different numerical results on each run.\nOne suggestion to deal with this “inherent” lack of reproducibility is to run the analysis 30 times and take the average. This does not eliminate the lack of reproducibility, it only lessens it. In most cases the computational demand of this approach is prohibitive.\n\n\n\nFigure 8.1: Lava lamps in the lobby of the headquarters of Cloudflare. Source.\nFigure 8.4: Random numbers via acceptance-rejection sampling.\nFigure 8.5: Random numbers via acceptance-rejection sampling.\n\n\n\nBox, G. E. P., and M. E. Muller. 1958. “A Note on the Generation of Normal Random Deviates.” Annals of Mathematical Statistics 29: 610–11. doi:10.1214/aoms/1177706645.\n\n\nMatsumoto, M., and T. Nishimura. 1998. “Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator.” ACM Transactions on Modeling and Computer Simulation 8: 3–30.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Numbers</span>"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "9  Debugging in R",
    "section": "",
    "text": "9.1 Profiling\nProfiling code gives you an idea of how much time is spent executing the lines of code. This sounds simple, but is actually quite tricky. First, you need to have a way of reliably measure time, and you need to distinguish different definitions of “time” on a computer. In most situations we are interested in wall-clock time, referring to the amount of time elapsed when the code execution is measured by taking the difference between the time of day just prior and after the run.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#profiling",
    "href": "debugging.html#profiling",
    "title": "9  Debugging in R",
    "section": "",
    "text": "Measuring Execution Time\nTo time the code\n\nfor (i in 1:200) { mad(rnorm(1000)) }\n\nwe could wrap it in calls to Sys.time and take the difference:\n\nst &lt;- Sys.time()\nfor (i in 1:200) { mad(rnorm(1000)) }\ndiff &lt;- Sys.time() - st\ndiff\n\nTime difference of 0.01907611 secs\n\n\nSys.time returns the absolute date and time of day, the precision is on the order of milliseconds but depends on the operating system. In order to measure execution time of code, it is recommended to use proc.time() instead, which measures the time the R process has spend on execution. proc.time() returns three numbers, the CPU time charged for the execution of the user instructions, the system time charged for execution by the system on behalf of the calling process, and the elapsed (wall-clock time).\n\nproc.time()\n\n   user  system elapsed \n  0.340   0.033   0.390 \n\n\nWrapping the loop timed previously with Sys.time with calls to proc.time results in the following:\n\nst &lt;- proc.time()\nfor (i in 1:200) { mad(rnorm(1000)) }\ndiff &lt;- proc.time() - st\ndiff\n\n   user  system elapsed \n  0.016   0.000   0.017 \n\n\nYou can simplify this operation by using system.time instead of proc.time. system.time makes calls to proc.time at the beginning and the end of the code execution and reports the difference:\n\nsystem.time(for(i in 1:200) mad(rnorm(1000)))\n\n   user  system elapsed \n  0.016   0.000   0.017 \n\n\n\n\nMicro-benchmarking\nBenchmarking is the comparison of code alternatives. Sometimes, one of the alternatives is an established set of tests or conditions against which a product is evaluated. TPC, for example, is a non-profit organization that establishes benchmark tests for databases. Right or wrong, if you develop a new database, eventually you will have to evaluate the database against the various TPC benchmarks.\nMore often benchmarking comes into play in evaluating alternative ways of accomplishing the same result. Which one is faster? Which one requires less memory? Does it matter? This form of benchmarking uses packages designed to evaluate code execution with sub-millisecond precision. One such package in R is microbenchmark. It gives a more accurate comparison than the frequently seen system.time(replicate(1000,expr)) expression. However, the package is not available for all versions of R. For example, it failed to install on my M2 Mac R 4.3.0. Another package is bench, which we will use here.\nThe idea of micro-benchmarking is to run a piece of code multiple times, either a fixed number of times or for a certain length of time and to describe the statistical properties of the run times (min, max, mean, std. deviation, median) as well as other characteristics (memory consumption, garbage collection, etc.).\nConsider the following three approaches to compute the square root of the elements of a vector:\n\nx &lt;- rep(1,1000)\ns1 &lt;- sqrt(x)\ns2 &lt;- x^0.5\ns3 &lt;- exp(log(x) * 0.5)\n\nMathematically, they are identical. Numerically, they are equivalent in the sense that they lead to the same answer within the limits of finite precision:\n\nsum(s1-s2)\n\n[1] 0\n\nsum(s1-s3)\n\n[1] 0\n\n\nTo see which of the three methods is fastest, we can run a micro-benchmark with bench:mark():\n\nlibrary(bench)\nbm &lt;- mark(\n        sqrt(x),\n        x^0.5,\n        exp(log(x)*0.5),\n        time_unit='us',\n)\nbm\n\n# A tibble: 3 × 6\n  expression          min median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 sqrt(x)           0.902   1.27   619299.    7.86KB     61.9\n2 x^0.5             3.53    3.65   245137.    7.86KB     24.5\n3 exp(log(x) * 0.5) 5.45    5.78   166540.    7.86KB     33.3\n\nplot(bm, type=\"violin\")\n\n\n\n\n\n\n\nFigure 9.1: Results of benchmarking three ways of taking the square root of a vector.\n\n\n\n\n\nThe time_unit='us' option requests that all times are reported in microseconds. If you do not specify this parameter, the times can be reported in different units for the code alternatives. By default, bench::mark runs each expression for at least (min_time=) 0.5 seconds and for up to (max_iterations=) 10,000 iterations.\nThe built-in sqrt function is faster than the power computation and much faster than exponentiation the logs.\nThe violin plot of the results shows that the distribution of the run times is heavily skewed to the right (Figure 9.1). Some individual function calls can exceed the majority of the calls by orders of magnitude—note that the horizontal axis of the plot is on the log scale! You should avoid benchmarking based on the mean run time and instead use the median run time.\n\n\nVisual Profiler in RStudio\nWrapping code in system.time is convenient if you want to know the (user, system, elapsed) time for a chunk of code. Micro-benchmarking is helpful to evaluate alternative ways of writing expressions. To measure the performance of larger pieces of code, down to the level of function calls and individual lines of code we use a profiler.\nTo get a detailed analysis of how much time is spent on lines of user-written code, and how much memory is allocated/deallocated for those lines of code, you can use the profiler tool that is built into RStudio. You first need to load the profvis library, then wrap the code you wish to profile in a call to profvis({code-goes-here}). After the profiling run RStudio opens a window with two tabs. The flame graph provides a visual representation of the time spent in code execution, stacking functions that call each other. The Data tab provides a tabular breakdown—many users find that easier to comprehend than the flame graph.\n\nThe following example calls the profiler for code that computes the histogram, mean, and standard deviation of 1000 bootstrap samples of the trimmed mean in a sample of \\(n=20,000\\) observations from a \\(G(0,2)\\).\n\nlibrary(profvis)\n\nprofvis({\n\n  set.seed(542)\n  n &lt;- 20000\n  x &lt;- rnorm(n, mean=0, sd=sqrt(2))\n\n  bagger &lt;- function(x,b=1000) {\n      n &lt;- length(x)\n      estimates &lt;- rep(0,b)\n      for (i in 1:b) {\n          # Draw a bootstrap sample\n          bsample &lt;- sample(n,n,replace=TRUE)\n          # Compute the trimmed mean of the bootstrap sample\n          estimates[i] &lt;- mean(x[bsample],trim=0.15)\n      }\n      # Compute mean and standard deviation of the estimator\n      mn = mean(estimates)\n      sdev = sd(estimates)\n      hist(estimates)\n      return (list(B=b, mean=mn, sd=sdev))\n  }\n\n  bagger(x,b=1000)\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach block in the flame graph represents a call to a function, or possibly multiple calls to the same function. The width of the block is proportional to the amount of time spent in that function. When a function calls another function, a block is added on top of it in the flame graph. Memory reported in the Data view shows allocated memory as positive values and deallocated memory as negative values (in megabytes).\nThe profiler is a great tool for long-running code, and can point you at opportunities for optimizing execution time. However, you need to be aware of how it works and the resulting issues:\n\nThe profiler samples the code. Every few milliseconds the profiler stops the interpreters and records which function call it is in and traces back through the call stack. You will get different results from run to run, depending on which lines of code and which functions are being sampled. The variability in profiling results affects the functions the most that execute quickly—they might get sampled once or a few times in one profiling run and might get skipped in another run. Fortunately, those are the functions of least interest in code optimization—you focus on the code where the program spends most of the time.\nIf your code runs fast, the profiler might not hit any lines of code. On the other hand, you should be able to reliably hit the long-running parts of the code with the profiler.\nThe profiler does not record some built-in R functions or code written in other languages. Many routines are calling into C, C++, or even Fortran code. The profiler records an overall time for that code, but does not provide a breakdown.\nThe call stack might appear to be in a reverse order from the code. This is because Rs lazy execution model might call a routine only when it is needed, rather then where it is specified in the code.\nYou might see a GC entry in the profiler. This represents the garbage collector which frees resources no longer needed. Garbage collection is unpredictable, depends on the overall state of the system, and is also time consuming—it can throw off the profiling results if it occurs during a run. However, if you consistently find garbage collection in the profiling run, it can be a sign that the code is spending a lot of time freeing and reallocating memory. There will be corresponding memory allocations and this represents an opportunity to optimize the code to be more memory efficient. For example, instead of adding elements to lists inside a loop, pre-allocating the required memory and filling in the elements will trigger fewer reallocations of memory.\n\nThe following code adds elements to a list in a loop. R has to increase the size of the vector several times, leading to memory allocation and garbage collection (freeing) of previously allocated memory.\n\nprofvis({\n    x &lt;- integer()\n    for (i in 1:10000) {\n        x &lt;- c(x, i)\n    }\n}\n)\n\n\n\n\n\n\nYou could achieve the same result with a single memory allocation and without garbage collection with this code\n\nx &lt;- rep(1,10000)\n\nIf you fill values in a loop, pre-allocate the result vector to avoid garbage collection:\n\nx &lt;- integer(10000)\nfor (i in 1:length(x)) {x[i] = i}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#tracebacks",
    "href": "debugging.html#tracebacks",
    "title": "9  Debugging in R",
    "section": "9.2 Tracebacks",
    "text": "9.2 Tracebacks\n\nA traceback is a listing of the call stack of the program. Tracebacks are helpful to see which functions were called when an error occurs. Consider the following example from “Advanced R” by Wickham (2019). Function f calls function g which calls function h and so forth.\n\nf &lt;- function(a) g(a)\ng &lt;- function(b) h(b)\nh &lt;- function(c) i(c)\ni &lt;- function(d) {\n  if (!is.numeric(d)) {\n    stop(\"`d` must be numeric\", call. = FALSE)\n  }\n  d + 10\n}\n\nWhen we call with a non-numeric argument, an error occurs.\n\nf(\"a\")\n\nError: `d` must be numeric\n\n\nIf you are in RStudio, you can now click the “Show Traceback” button next to the error message (Figure 9.2).\n\n\n\n\n\n\nFigure 9.2: The Show Traceback icon in RStudio after an error occured\n\n\n\nAlternatively, you can see the traceback with the traceback() function.\n\ntraceback()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#the-interactive-debugger",
    "href": "debugging.html#the-interactive-debugger",
    "title": "9  Debugging in R",
    "section": "9.3 The Interactive Debugger",
    "text": "9.3 The Interactive Debugger\nThe RStudio IDE has an interactive debugging tool. Interactive debugging allows you to stop code execution at breakpoints, step through the code, interact with function evaluations, see and change the value of variables, and so forth. It is a valuable tool to find problems in programs and to learn how code works. It is highly recommended to step into the functions you frequently use and examine the code line by line—you learn a lot about how your favorite packages work.\nSuppose you want to run a classification model using adaptive boosting on the banana quality data. This data set contains ratings of the fruit quality (Good, Bad) and fruit attributes such as size, weight, sweetness, etc., for 4,000 bananas.\n\nlibrary(\"duckdb\")  \ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nban_train &lt;- dbGetQuery(con, \"SELECT * FROM banana_train\") \ndbDisconnect(con)\n\nhead(ban_train)\n\n       Size      Weight   Sweetness    Softness HarvestTime    Ripeness\n1  1.706644 -0.03692624 -4.46344950 -1.51004720   4.5640225 -0.04171263\n2  3.703947  1.11884890 -3.04337740  0.02899801  -0.9705806 -1.46996620\n3 -3.888493  1.32609500  0.04608246  2.25944950   0.5068415  0.73710510\n4 -3.052952 -0.58796227 -1.63466790  1.04902060  -0.2098602 -1.81881750\n5  1.965954 -1.37867620 -3.14279270 -3.24607060  -0.6116899  1.81526600\n6  2.306789 -3.69348800 -0.04255614  0.70101670   2.7060213  3.96723440\n     Acidity Quality\n1  4.3266883    Good\n2 -0.5881153    Good\n3  2.5832198     Bad\n4  3.9573660     Bad\n5 -0.5000507     Bad\n6 -0.9244213    Good\n\n\nSuppose we want to debug the code for adaptive boosting in the ada package. The following statements load the library and invoke the ada::ada function on the banana training data. Prior to executing the code, execute debugonce(ada::ada) in the Console. This will set a breakpoint at the entry point of the function.\n\nlibrary(ada)\nadab &lt;- ada(Quality ~ ., \n            data=ban_train,\n            control=rpart.control(maxdepth=1, cp=-1, minsplit=0, xval=0)\n            )\n\nWhen the code above is executed, a browser window pops up when the breakpoint at ada::ada() is hit. The browser shows the R source code of the function and highlights the current line of the code (Figure 9.3). The frame of the Console window now shows five icons through which you can navigate the code (Figure 9.4).\n\n\n\n\n\n\nFigure 9.3: Hitting the breakpoint at the ada:ada entry point.\n\n\n\n\nNext (n): Execute the line of code and move to the next line\nStep Into (s): Step into the function called on the line you are on.\nStep Out (f): Run to the end of the current function (or loop) and step back out to the calling function\nContinue (c) : Stop interactive debugging and continue execution of the code\nStop (Q): Stop interactive debugging and terminate execution of the code\n\n\n\n\n\n\n\nFigure 9.4: Controls for moving through the code in the browser window.\n\n\n\nFigure 9.5 shows the result of stepping into the ada function. This brings you to the ada.formula function where the model expression is parsed. Advancing with several Next steps will land the cursor on the call to ada.default. If you now click on Step Into the source file for ada.default is loaded and you can advance through the function. If you click instead on Next, the call to ada.default function is completed and debugging resumes on line 22.\n\n\n\n\n\n\nFigure 9.5: About to execute the call to ada.default.\n\n\n\nDuring interactive debugging the Environment pane shows the values of the variables and objects in the code and the call stack (Figure 9.6).\n\n\n\n\n\n\nFigure 9.6: A view of the environment during interactive debugging.\n\n\n\nYou can invoke the interactive debugger through other methods as well:\n\nSetting a break point in the code by either clicking on the line number or with Shift-F9. I have found this method to not work reliably in .Rmd and .qmd documents. It does work in R scripts (.R files).\nInvoking the browser() command anywhere in the code.\nUsing debug or debugonce. The former sets a breakpoint that invokes the interactive debugger at every invocation of the function, until the undebug function removes the breakpoint. debugonce sets a breakpoint for single execution.\n\nDebugging works on the interpreted R statements. Many functions call complied code in other languages, C and C++, for example. The interactive RStudio debugger cannot step through this compiled code.\n\n\n\nFigure 9.2: The Show Traceback icon in RStudio after an error occured\nFigure 9.3: Hitting the breakpoint at the ada:ada entry point.\nFigure 9.4: Controls for moving through the code in the browser window.\nFigure 9.5: About to execute the call to ada.default.\nFigure 9.6: A view of the environment during interactive debugging.\n\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "10  Reproducible Research and Data Analysis",
    "section": "",
    "text": "10.1 What, me Worry?\nReproducibility in research means to give others the ability to use the same materials as were used by an original investigator in an attempt to arrive at the same results. This is not the same as replicability, the ability to repeat a study, collect new data, and duplicate the results of an original investigator. To reproduce results, we need to have access to the same data, analytics, and code. To replicate results, we need to know exactly how a study or experiment was conducted, which methods and instruments of measurements were used, and so forth, in order to set up a new study or experiment.\nAs a first-year graduate student in statistics or data Science, why would you worry about reproducibility in research? You are unlikely to have your own research project that involves data collection. You are working on data analytics problems by yourself, why share your code with the world? You are planning to work as a data scientist in industry, why worry about concepts of reproducible research?\nFirst, reproducibility in research and data analysis is not about research; it is about the need to perform complex tasks in an organized workflow that builds trust, transparency, and accountability. Here are some reasons why you need to worry about reproducibility:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#what-me-worry",
    "href": "reproducibility.html#what-me-worry",
    "title": "10  Reproducible Research and Data Analysis",
    "section": "",
    "text": "You hardly ever work alone. Team members and supervisors need to be able to understand and judge your work.\nYou need to plan for someone else working on your stuff. Taking over someone else’s code can be a miserable experience. If that task falls on you, what information would you have liked to have to pick up the pieces?\nSoftware changes. Even if you are the only one working on the code—for now—do you ever made a mistake and wished you could revert back to a previous working version? Have you ever lost code or had to deal with incompatible versions on different machines? How do you prove (to yourself) that a change fixed or broke the code? How do you maintain the analyses for different versions of the same data?\nData and analytics are key to reproducibility. If you do not have the data or the program, you cannot validate much in today’s data-driven world. As a statistical programmer, you are at the center of someone else’s reproducibility story.\nUnless you work in purely methodological, theoretical work, as a statistician or data scientist you will be working on data problems of others. That might be an employer or a consulting client. Your work feeds into processes and systems they are accountable for. You need to ensure that there is accountability for your part of the work.\nMany analytic methods are inherently non-reproducible because they depend on stochastic elements. Making a statistical program reproducible means to understand all instances in which random numbers play a role (see Chapter 8). You might not be able to control all of them and need to decide when a sufficient level of reproducibility has been reached. It might be necessary to fix random number seeds, but sacrificing performance by turning off multi-threading is a bridge too far.\nReproducibility is not guaranteed by just reading the research paper. A common misconception is that methods sections in papers are sufficiently detailed to reproduce a result. Data analyses are almost never described in sufficient detail to be reproducible. Put yourself in the shoes of the reader and imagine to provide them with a data set and the pdf of the published paper. Would they be able to analyze the data and derive the effect sizes and \\(p\\)-values in Table 3 of the paper? Even a nod at the software used is not enough. “Analyses were performed with PROC MIXED in SAS.” So what?! There are a million ways to analyze an experiment incorrectly with PROC MIXED by fitting the wrong model, messing up comparisons, misinterpreting parameter estimates, etc.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#improving-reproducibility-of-statistical-programs",
    "href": "reproducibility.html#improving-reproducibility-of-statistical-programs",
    "title": "10  Reproducible Research and Data Analysis",
    "section": "10.2 Improving Reproducibility of Statistical Programs",
    "text": "10.2 Improving Reproducibility of Statistical Programs\nHere are some points to increase the reproducibility of your programs and data analysis\n\nHave a Plan & Document\nBegin by writing a project plan or protocol that includes methods, materials, data sets, software, packages, etc., you intend to use. Make a note of versions, operating systems, cloud instances and other aspects of computingn architecture that can have bearing on your approach and results.\nConsider this a living document, because things will change. You might find out that the package you plan for a particular analysis turns out to be insufficient in capabilities and performance. Document why you are making the change.\nThe project plan should be under version control (Section 10.2.4).\nPlan how to organize the directory structure of your project. Suppose your project involves some data, code, analysis output, and manuscripts you work on. A project-based organization treats each project as a separate directory with sub-directories for data, code, results, and manuscripts. An activity-based organization uses data, code, results, and manuscripts as the highest-level folders and organizes projects within those (Figure 10.1).\n\n\n\n\n\n\nFigure 10.1: Project-based and activity-based folder organization.\n\n\n\nThe best organization is up to you, I generally prefer the project-based over activity-based approach. Some things to keep in mind:\n\nIf projects do not overlap in data and code, it makes sense to deal with them separately. Projects that share large amounts of data are best managed together.\nAvoid duplication, especially when it comes to data. Keeping multiple copies of the same data is bad practice and a recipe for disaster when data changes. The same goes for code.\nModularity is your friend. Writing modular code that assembles programs from functions and components is a great way to organize the work. You can keep the modules separate from the individual projects. The code for the project differs in which modules are used and how they are being called.\nRaw data should never be modified—it should be in a read-only state. The processed data that serves as the input for analytics is kept separate from the raw data.\nProject directories that are self contained can be easily compressed, backed-up, and shared. Achieving self-containedness can be at odds with avoiding duplication; you have to think through and manage the tradeoffs.\nAnything generated from code (output, tables, images) goes into a separate folder. Depending on the volume of results, separate directories for tables, images, text output is a good idea.\nRecord receipt and distribution of assets. Keep a journal (or README file) where you track which data was received when from whom, and where you stored it.\n\n\n\n\n\n\n\nNo Spreadsheets!\n\n\n\nAvoid spreadsheets at all cost. If raw data comes to you in the form of spreadsheets, make them immutable (read-only) and extract the “real” data immediately. Spreadsheets are very common and a horrible format for analytic data. We prefer tabular data in rows and columns where each row is an observation and each column is a variable. The primary data source for statistical analysis should not contain graphs or other calculations.\nSpreadsheets give the appearance of tabular data but they are usually not cleanly organized this way. Some cells contain images, text and other forms of information that does not fit the tabular layout. Cells make references to other (sheets and) cells that make it difficult to know what data you are actually dealing with.\nIf your primary data is in spreadsheets, the data is of constant danger of being corrupted by changes to the spreadsheet. It is too easy to modify raw data by accidentally typing in a cell.\n\n\n\n\nMetadata\nMetadata is data about your data. It provides information about the data but not the data itself.\nSuppose someone sends you a CSV file, the only metadata are names of the variables in the file. Some CSV files won’t even have those. What do the variable names mean? Are those the best names to place on graphs? What are the units of measurements? What else do you need to know about the variables? Are there missing values, if so, how are they coded? When was the data created and by whom? What were the purpose of data collection and the means of data collection? Has the data been processed?\nThe metadata we are most in need of in statistical programming is called structural metadata, it describes the structure of data sets and databases (tables, columns, indices, etc.). This information is tracked in data dictionaries (also called codebooks). A data dictionary might contain for each column in a data set the following:\n\nThe exact variable name in the file or table.\nA long variable name that serves as a label on results (graphs, tables).\nA variable-sized string column that explains what the variable means, how it was measured, its units, etc.\nInformation on how values are formatted, e.g, date, datetime formats, reference values for dates/times.\nStatistical summaries such as min, max, mean, and checksums.\n\nYour data might not come with dictionary information, so you will have to create the dictionary and track down the needed information.\n\n\nLiterate Programming\n\nWhat it is\nThe concept of literate programming was introduced by Knuth (1984) and refers to programs that are not written with a focus on the compiler but are organized in the order demanded by the logic of the programmer’s thoughts. In literate programming the program logic is written out in natural language, supplemented by code snippets or chunks. While literate programming was a concept at the time when Knuth proposed it, it is commonplace today, in particular in data science. Programming environments that combine markdown with code are a manifestation of literate programming: Jupyter Notebook, R Markdown, Quarto, and many more.\nLiterate programs are different from well-documented code or code that includes documentation as part of the file. The flow of these programs still follow the logic of the computer, not the logic of human thought. In literate programming the code follows the structure of the documentation. Chapter 11 is a literate program that implements from scratch an important iterative statistical algorithm: iteratively reweighted least squares (IRLS).\nIn general, we should weave code and text into a complete reference of our work. The document should:\n\ndownload or otherwise load/generate data\nreproduce steps to munge data\nrecreate analysis steps\ncreate figures and tables\nexplain any and all steps along the way\n\n\n\nComments in code\nIf you do not write a literate program you should add comments to your code. The purpose of comments is to express the intent of the code, not to explain what the code does. Explanatory comments are OK if the code is not obvious in some way. If you have a chance to refactor or rewrite non-obvious code, do not hesitate.\nComments should clarify why code is written a certain way and what the code is supposed to accomplish. If you feel that many lines of comments are needed to clarify some code, it can be an indication that the code should be simplified. If you are struggling to explain the intent of a function at the time you write it, imagine how difficult it is to divine that intent from the comment or code in six months or for a programmer not familiar with the code.\n\n\n\nVersion Control\nVersion control refers to the management and tracking of changes in digital content; mostly files and mostly code. Any digital asset can be placed under version control. Even if you are working (mostly) by yourself, using a version control system is important. Employers consider it a non-negotiable skill and you do not want to stand out as the applicant who does not know how to use git. The benefits of version control systems are so big, even the solo programmer would be remiss not using it.\nWhat does a version control system like git do for you:\n\nIt keeps track of files and their changes over time.\nIt saves changes to files without duplicating the contents, saving space in the process.\nIt groups content in logical units (branches) that are managed together. For example, all files associated with a particular build of a software release are kept in a branch.\nIt is a time machine, allowing you to reconstruct a previous state of the project and to see the complete history of the files.\nIt is a backup machine, making sure you have access to older versions of files and that changes do not get lost.\nIt allows you to perform comparisons between versions of files and to reconcile their differences.\nIt allows you to safely experiment with code without affecting code others depend on.\nIt allows you to see which parts of a project are worked on most/least frequently.\nIt is a collaborative tool, that reconciles changes to files made by multiple developers. Version control systems allow you to submit changes to someone else’s code.\nBy supporting modern continuous integration/continuous deployment (CI/CD) principles, a version control system can automate the process of testing and deploying software.\n\nThe list goes on and on. The main point is that these capabilities and benefits are for everyone, whether you work on a project alone or as a team member.\n\n\n\n\n\n\nTip\n\n\n\nOh how I wish there were easily accessible version control systems when I did my Ph.D. work. It involved a lot of programming algorithms and the analysis of real data sets. Developing the code took months to years and went through many iterations. I made frequent backups of the relevant files using really cool storage technology using special 1GB-size cartridges and a special reader. There were disks labeled “January 1993”, “March 1993”, “December 1993”, “Final”, “Final-V2”, and so forth. The storage technology was discontinued by the manufacturer and the cartridges are useless today. I am not able to access the contents even if the bits have not rotted on the media by now.\nTo study how the algorithm I needed to write for the dissertation evolved over time, I would have to go through all the backups and compare files one by one. A version control system will show me the entire history of changes in one fell swoop.\nUsing a cloud-based version control system would have avoided that headache. Alas, that did not exist back then.\n\n\nThere are many version control systems, Git, Perforce, Beanstalk, Mercurial, Bitbucket, Apache Subversion, AWS CodeCommit, CVS (Concurrent Versions System, not the drugstore chain), and others.\nThe most important system today is Git. GitHub and GitLab are built on top of git. What is the relationship? Git is a local version control system, it runs entirely on the machine where it is installed and manages file changes there. GitHub and GitLab are a cloud-based systems that allow you to work with remote repositories. In addition to supporting Git remotely, GitHub adds many cool features to increase developer productivity. The files for the pages you are reading are managed with Git and stored in a remote repository on GitHub (the URL is https://github.com/oschabenberger/oschabenberger-github.io-sp). GitHub also hosts the web site for the text through GitHub Pages. GitHub Actions can be set up so that the web site (the book) automatically rebuilds if any source files changes.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#git-crash-course",
    "href": "reproducibility.html#git-crash-course",
    "title": "10  Reproducible Research and Data Analysis",
    "section": "10.3 Git Crash Course",
    "text": "10.3 Git Crash Course\nGit is installed on your machine, it is a local tool for versioning files. You can perform all major Git operations (clone, init, add, mv, restore, rm, diff, grep, log, branch, commit merge, rebase, etc.) without an internet connection. The collaborative aspect of version control comes into play when you use a Git service provider such as GitHub or GitLab. Besides making Git a tool for multi-user applications, using GitHub or GitLab also gives you the ability to work with remote repositories; you can push your local changes to a server in the cloud, making it accessible to others and making it independent of the local workstation. Just because you push a repository to GitHub does not necessarily give everyone on the internet access to it—you manage whether a repository is private or public.\n\nInstalling Git\nThere are several ways to get Git on your machine, see here. On MacOS, installing the XCode Command Line tools will drop git on the machine. To see if you already have Git, open a terminal and check:\n➜  Data Science which git\n/usr/bin/git\nThe executable is installed in /usr/bin/git on my MacBook.\n\n\nBasic Configuration\nThere are a million of configuration options for Git and its commands. You can see the configuration with\n➜ git config --list\nTo connect to GitHub later, add your username and email address to the configuration:\n➜ git config --global user.name \"First Last\"\n➜ git config --global user.email \"first.last@example.com\"\nYou can have project-specific configurations, simply remove the --global option and issue the git config command from the project (repository) directory.\n\n\nRepositories\nA repository is a collection of folders and files. Repositories are either cloned from an existing repository or initialized from scratch. To initialize a repository, change into the root directory of the project and issue the git init command:\nData Science cd \"STAT 5014\"\n➜ STAT 5014 pwd\n/Users/olivers/Documents/Teaching/Data Science/STAT 5014\n➜ STAT 5014 git init\nInitialized empty Git repository in /Users/olivers/Documents/Teaching/Data Science/STAT 5014/.git/\n➜ STAT 5014 git:(main)\nTo get help on git or any of the git commands, simply add --help:\n➜ git --help\n➜ git status --help\n➜ git add --help\n\nStages of a file\nA file in a Git repository goes through multiple stages (Figure 10.2). At first, the file is unmodified and untracked. A file that was changed in any way is in a modified state. That does not automatically update the repository. In order to commit the change, the file first needs to be staged with the git add command.\nWhen you issue a git add on a new file or directory, it is being tracked. When you clone a repository, all files in your working directory will be tracked and unmodified.\n\n\n\n\n\n\nFigure 10.2: The lifecycle of a file in Git. Source\n\n\n\nA file that is staged will appear under the “Changes to be committed” heading in the git status output.\nOnce you commit the file it goes back into an unmodified and tracked state.\n\n\nTracking files\nTo track files in a repository, you need to explicitly add them to the file tree with git add. This does not push the file into a branch or a remote repository, it simply informs Git which files you care about.\n➜ git add LeastSquares.R\n➜ git add *.Rmd\n➜ git add docs/\nThe previous commands added LeastSquares.R, all .Rmd files in the current directory, and all files in the docs subfolder to the Git tree. You can see the state of this tree any time with\n➜ git status\ngit status shows you all files that have changed as well as files that are not tracked by Git and are not ignored. For example, after making some changes to the quarto.yml and to reproducibility.qmd files since the last commit, the status of the repository for this material looks as follows:\n➜  StatProgramming git:(main) ✗ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   _quarto.yml\n    modified:   docs/reproducibility.html\n    modified:   docs/search.json\n    modified:   reproducibility.qmd\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .DS_Store\n    .gitignore\n    .nojekyll\n    .python-version\n    StatProgramming.Rproj\n    _book/\n    ads.ddb\n    customstyle.scss\n    data/\n    debug_ada.R\n    debug_ada.Rmd\n    images/\n    latexmacros.tex\n    sp_references.bib\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nTwo more files have been noted by Git as modified, docs/reproducibility.html and docs/search.json. These files are generated by Quarto when the content of the modified files is being rendered. They will be added to the next commit to make sure the website is up to date and not just the source (.qmd) files.\ngit add can be a bit confusing because it appears to perform multiple functions: to track a new file and to stage a file for commit. If you think of git add as adding precisely the content to the next commit, then the multiple functions roll into a single one.\n\nAn ignored file is one for which you explicitly tell Git not to worry about. You list those files in a .gitignore file. (You can have multiple .gitignore files in the directory hierarchy, refer to the Git documentation on how they interact. The typical scenario is a .gitignore file in the root of the repository.)\nThe contents of the following .gitignore file state that all .html files should be ignored, except for foo.html. Also, StatLearning.Rproj will be ignored.\n➜ cat .gitignore\n*.html\n!foo.html\nStatLearning.Rproj\nFiles that are listed in .gitignore are not added to the repository and persist when a repository is cloned. However, if a file is already being tracked, then adding it to .gitignore does not untrack the file. To stop tracking a file that is currently tracked, use\ngit rm --cached filename \nto remove the file from the tree. The file name can then be added to the .gitignore file to stop the file from being reintroduced in later commits.\nFiles that you want to exclude from tracking are often binary files that are the result of a build or compile, and large files. Also, if you are pushing to a public remote repository, make sure that no files containing sensitive information are added.\n\n\nCommitting changes\nOnce you track a file, Git keeps track of the changes to the file. Those changes are not reflected in the repository until you commit them with the commit command. A file change will not be committed to the repository unless it has been staged. git add will do that for you.\nIt is a good practice to add a descriptive message to the commit command that explains what changes are committed to the repository:\n➜ git commit -m \"Early stopping criterion for GLMM algorithm\"\nIf you do not specify a commit message, Git will open an editor in which you must enter a message.\nSince only files that have been added with git add are committed, you can ask Git to notice the changes to the files whose contents are tracked in your working tree and do corresponding git adds for you by adding the -a option to the commit:\n➜ git commit -a -m \"Early stopping criterion for GLMM algorithm\"\nWhat happens when you modify a file after you ran git add but before the net commit? The file will appear in git status as both staged and ready to be committed and as unstaged. The reason is because Git is tracking two versions of the file now: the state it was in when you first ran git add and the state it is in now, which includes the modifications since the last git add. In order to stage the most recent changes to the file, simply run git add on the file again.\n\n\nRemote repositories\nThe full power of Git comes to light when you combine the local work in Git repositories with a cloud-based version control service such as GitHub or GitLab. To use remote repositories with Git, first set up an account, say with GitHub.\nThe Git commands to interact with a remote repository are\n\ngit pull: Incorporates changes from a remote repository into the current branch. If the current branch is behind the remote, then by default it will fast-forward the current branch to match the remote. The result is a copy of changes into your working directory.\ngit fetch: Copies changes from a remote repository into the local Git repository. The difference between fetch and pull is that the latter also copies the changes into your working directory, not just into the local repo.\ngit push: Updates remote references using local references, while sending necessary objects.\ngit remote: Manage the set of remote repositories whose branches you track.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have used other version control systems, you might have come across the terms pushing and pulling files. In CVS, for example, to pull a file means adding it to your local checkout of a branch, to push a file means adding it back to the central repository.\nWith Git, push and pull command only come into play when you work with remote repositories. As long as everything remains on your machine, you do not need those commands. However, most repositories these days are remote, so the initial interaction with a repository is often a clone, pull, or fetch.\n\n\nStart by creating a new repository on GitHub by clicking on the New button. You have to decide on a name for the repository and whether it is public or private. Once you created a remote repository, GitHub gives you alternative ways of addressing it, using https, ssh, etc.\n\n\n\n\n\n\nTip\n\n\n\nDepending on which type of reference you use on the command line, you also need different ways of authenticating the transaction. GitHub removed passwords as an authentication method for command-line operations some time ago. If you use SSH-style references you authenticate using the passphrase of an SSH key registered with GitHub. If you use https-style references you authenticate with an access token you set up in GitHub.\n\n\nBack on your local machine you manage the association between the local repository and the remote repository with the git remote commands. For example,\n➜ git remote add origin git@github.com:oschabenberger/oschabenberger-github.io-bn.git\nassociates the remote repository described by the ssh syntax git@github.com:oschabenberger/oschabenberger-github.io-bn.git with the local repository. Using html syntax, the same command looks like this:\n➜ git remote add origin https://github.com/oschabenberger/oschabenberger-github.io-bn\nGitHub provides these strings to you when you create a repository.\nTo update the remote repository with the contents of the local repository, issue the git push command:\n➜ git push",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#coding-best-practices",
    "href": "reproducibility.html#coding-best-practices",
    "title": "10  Reproducible Research and Data Analysis",
    "section": "10.4 Coding Best Practices",
    "text": "10.4 Coding Best Practices\nWriting code, maintaining code, reviewing code, and fixing code are essential elements of what data professionals do. Understanding and following best practices is important. 80% of jobs in data science are outside of academic environments. Those employers have standard operating procedures for software developers that will apply also to statistical programming and data science projects. For example, many companies use code reviews by fellow developers to evaluate new or modified code before it can be committed to a repository. The feedback from code reviews flows into performance evaluations and has impact on your career. Not following the coding guidelines and best practices of an organization is a great way of shortening tenure. Software projects are collaborative projects and someone else will need to work with your code. How easy is it to understand? Is it well documented? Is it properly structured and modular? How easy is it to debug?\nHere are some coding practices for you to consider.\n\nStructure and Organization\n\nNaming\n\nChoose names for variables and functions that are easy to understand. Variable and function names should be self explanatory. Most modern programming languages and tools no longer limit the length of function or variable names, there is no excuse for using a1, a2, b3 as variable names. Use nouns for names of variables and objects that describe what the item holds; for example, originalData and randomForestResult instead of d and out.\nStick with a naming convention such as snake_case, PascalCase, or camelCase. In snake_case, spaces between words are replaced with an underscore. In camelCase, words are concatenated and the first letter of the word is capitalized. PascalCase is a special case where the first letter of the entire name is also capitalized; camelCase is ambivalent about capitalizing the first letter of the name. The following are examples of names in camelCase.\n\naccountBalance\nthisVariableIsWrittenInCamelCase\nitemNumber\nsocialSN\nMasterCard\nAn issue with camelCase is that it is not entirely clear how to write names in that style that contain other names or abbreviations, for example, is it NASAAdminFiles or NasaAdminFiles? I am not sure it really matters.\nsnake_case is popular because it separates words with underscores—mimicking white space—while producing valid names for computer processing. The following are examples of names in snake_case:\naccount_balance\nACCOUNT_BALANCE\nhome_page\nitem_Number\nUsing upper-case letters in snake_case is called “screaming snake case”, situations where I have seen it used are the definition of global constants or macro names in C. kebab case is similar to snake case but uses a hyphen instead of an underscore. Here are examples of names in kebab case:\naccount-balance\nhome-page\nitem-Number\nAlthough it might look nice, it is a good idea to avoid kebab case in programs. Imagine the mess that ensues if the hyphen were to be interpreted as a minus sign! While the compiler might read the hyphen correctly, the code reviewer in the cubicle down the hall might think it is a minus sign.\nDo not assign objects to existing names, unless you really want to override them. This goes in particular for internal symbols and built-in functions. Unfortunately, R does not blink and allows you to do things like this:\nT &lt;- runif(20)\nC &lt;- summary(lm(y ~ x))\nThese assignments override the global variable whose value is set to TRUE for logical comparison and the function C() that defines contrasts for factors. If in doubt whether it is safe to assign to a name, check in the console whether the name exists or request help for it\n?T\n?C()\n\n\nComments\nUnless you write a literal program use comments throughout to clarify why code is written a certain way and what the code is supposed to accomplish. Even with literal programs, comments associated with code are a good practice because the code-portion of the literal program can get separated from the text material at some later point.\nComments frequently are intended by programmers to leave themselves some notes, for example, about functions yet to be written or to be refactored later. Make it clear with a “TODO” at the beginning of the comment where those sections of the program are and make the TODO comment stand out visually from other comments.\nIt is a good practice to have a standardized form for writing comments. For example, you can have a standard comment block at the beginning of functions. Some organizations will require you to write very detailed comment blocks that explain all inputs and outputs down to length of vectors and data types.\n# -------------------------------------------------------------------\n# TODO: Add check whether it is safe to perform the division before\n#       returning from the function. Variances can be zero or near zero.\n# -------------------------------------------------------------------\n\n# ###################################################################\n# Function: getIRLSWeights\n# Purpose: retrieve the vector of weights for iteratively \n#          reweighted least squares\n# Arguments:\n#       eta: numeric vector of linear predictors\n#       link: character string describing the link function\n#       dist: character string describing the distribution of the response \n#\n# Returns: the vector of weights, same length as the eta vector (input)\n#\n# Notes: \n#   For efficiency, eta is not checked for NULL or missing values. \n#   These checks are in the deta_dmu() and get_var() functions.\n# ###################################################################\n\ngetIRLSWeights &lt;- function(eta, link=\"log\", dist=\"poisson\") {\n    var_z &lt;- deta_dmu(eta,link)^2 * get_var(get_mu(eta,link),dist)\n    return (1/var_z)    \n}\nIf you program in Python, you would add docstrings to the function. This also serves as the help information for the user.\n\n\nWhitespace\nJudicious use of whitespace makes code more readable. It helps to differentiate visually and to see patterns. Examples are indentation (use spaces, not tabs), alignment within code blocks, placement of parentheses, and so forth.\nWhich of the following two functions is easier to read? It does not matter for the R interpreter but it matters for the programmer.\nget_z &lt;- function(y, eta, link) {\nif (is.null(y) || is.null(eta)) {\nstop(\"null values not allowed\") }\nif (anyNA(y) || anyNA(eta)) {\nstop(\"cannot handle missing values\") }\nz &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\nreturn(z)\n}\n\nget_z &lt;- function(y, eta, link) {\n    if (is.null(y) || is.null(eta)) {\n        stop(\"null values not allowed\")\n    }\n    if (anyNA(y) || anyNA(eta)) {\n        stop(\"cannot handle missing values\")\n    }\n    z &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\n    return(z)\n}\nThe following code uses indentation to separate options from values and to isolate the function definition for handling the reference strip. The closing parenthesis is separated with whitespace to visually align with the opening parenthesis of xyplot.\nxyplot(diameter ~ measurement | Tree, \n       data    = apples,\n       strip   = function(...) {\n           # alter the text in the reference strip   \n           strip.default(..., \n                         strip.names  = c(T,T), \n                         strip.levels = c(T,T),\n                         sep          = \" \")\n       },\n       xlab    = \"Measurement index\",\n       ylab    = \"Diameter (inches)\",\n       type    = c(\"p\"),\n       as.table= TRUE,\n       layout  = c(4,3,1)\n      )\nWith languages such as Python, where whitespace is functionally relevant, you have to use spacing within the limits of what the language allows.\n\n\nFunctions\nIn R almost everything is a function. When should you write functions instead of one-off lines of code? As always, it depends; a partial answer hides in the question. When you do something only once, then writing a bunch of lines of code instead of packaging the code in a function makes sense. When you write a function you have to think about function arguments (is the string being passed a single string or a vector?), default values, return values, and so on.\nHowever, many programming tasks are not one-offs. Check your own code, you probably write the same two or three “one-off” lines of code over and over again. If you do it more than once, consider writing a function for it. If you do a substantial task over and over, consider writing a package.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in writing an R package, check out this chapter in the book by Peng, Kross, and Anderson (2020).\n\n\nFunction names should be verbs associated with the function purpose, e.g., joinTables(), updateWeights(). For functions that retrieve or set values, using get and set is common: getWeights(), setOptimizationInput().\nThe comment block for function should document the function purpose, required arguments, and returns.\nSome argue that it is good coding practice to have default values on function arguments. For example,\naddNumbers &lt;- function(a=1, b=2) {return(a+b)}\ninstead of\naddNumbers &lt;- function(a, b) {return(a+b)}\nAdding defaults ensures that all variables are initialized with valid values and simplifies calling the function. On the other hand, it can mask important ways to control the behavior of the function. Users will call a function as they see it being used by others and not necessarily look at the function signature. Take the duckload() function:\n\nduckload &lt;- function(tableName, whereClause=NULL, dbName=\"ads.ddb\") {\n    if (!is.null(tableName)) {\n        if (!(\"duckdb\" %in% (.packages()))) {\n            suppressWarnings(library(\"duckdb\"))\n            message(\"duckdb library was loaded to execute duckload().\")\n        }\n        con &lt;- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)\n        query_string &lt;- paste(\"SELECT * from \", tableName)\n        if (!is.null(whereClause)) {\n            query_string &lt;- paste(query_string, \" WHERE \", whereClause)\n        }\n        df_ &lt;- dbGetQuery(con, query_string)\n        dbDisconnect(con)\n        return (df_)\n    } else {\n        return (NULL)\n    }\n}\n\nWould you know from the following usage pattern that you can pass a WHERE clause to the SQL string?\n\nduckload(\"apples\")\n\nIf the function arguments had no defaults, the function call would reveal its capabilities:\n\nduckload(\"apples\", whereClause=NULL, dbName=\"ads.ddb\")\n# or\nduckload(\"apples\",NULL,\"ads.ddb\")\n\nOther good practices to observe when writing functions:\n\nAlways have an explicit return argument. It makes it much easier to figure out where you return from the function and what exactly is being returned.\nCheck for NULL inputs\nCheck for missing values unless your code can handle them.\nHandle errors (see below)\nPass through variable arguments (...)\nIf you return multiple values, organize them in a list or a data frame. Lists are convenient to collect objects that are of different types and sizes into a single object. The following function returns a list with three elements,\n\n\niterationWeight &lt;- function(Gm,wts,method=\"tree\") {\n    pclass &lt;- predict(Gm,type=\"vector\")\n    misclass &lt;- pclass != Gm$y\n    Em &lt;- sum(wts*misclass)/sum(wts)\n    alpha_m &lt;- log((1-Em)/Em)\n    return (list(\"misclass\"=misclass,\"Em\"=Em,\"alpha_m\"=alpha_m))\n}\n\n\n\nError Handling\nThink of a function as a contract between you and the user. If the user provides specified arguments, the function produces predictable results. What should happen when the user specifies invalid arguments or when the function encounters situations that would create unpredictable results or situations that keep it from continuing?\nYour opportunities to handle these situations include issue warning messages with warning(), informational messages with message(), stopping the execution with stop() and stopifnot() and try-catch-finally execution blocks. In general, stopping the execution of a function with stop or stopifnot is a last resort if the function cannot possibly continue. If the data passed are of the wrong type and cannot be coerced into the correct data type, or if coercion would result in something nonsensical, then stop.\nIn the event that inputs are invalid and you cannot perform the required calculations, could you still return NULL as a result? If so, do not stop the execution of the function. You can issue a warning message and then return NULL. Warning messages are also appropriate when the function behavior is changing in an unexpected way. For example, the input data contains missing values (NAs) and your algorithm cannot handle them. If you process the data after omitting missing values, then issue a warning message if that affects the dimensions of the returned objects.\nKeep in mind that R is used in scripts and as an interactive language. Messages from your code are intended for human consumption so they should be explicit and easy to understand. But avoid making your code too chatty.\nTo check whether input values have the expected types you can use functions such as\n\nis.numeric()\nis.character()\nis.factor()\nis.ordered()\nis.vector()\nis.matrix()\n\nand to coerce between data types you can use the as. versions\n\nas.numeric()\nas.character()\nas.factor()\nas.ordered()\nas.vector()\nas.matrix()\n\ntryCatch() is the R implementation of the try-catch-finally logic you might have seen in other languages. It is part of the condition system that provides a mechanism for signaling and handling unusual conditions in programs. tryCatch attempts to evaluate expression expr, and if it succeeds, executes the code in the finally block. You can add erorr and warning handlers with the error= and warning= options.\n\ntryCatch(expr,\n         error = function(e){\n           message(\"An error occurred:\\n\", e)\n         },\n         warning = function(w){\n           message(\"A warning occured:\\n\", w)\n         },\n         finally = {\n           message(\"Finally done!\")\n         }\n        )\n\ntryCatch is an elegant way to handle conditions, but you should not overdo it. It can be a drag on performance. For example, if you require input to be of numeric type, then it is easier and faster to check with is.numeric than to wrap the execution in tryCatch.\n\n\nDependencies\nIt is a good idea to check dependencies in functions. Are the required packages loaded? It is kind of you to load required packages on behave of the caller rather than stopping execution. If you do, issue a message to that effect. See duckload() above for an example.\nInstalling packages on behalf of the caller is a step too far in my opinion, since you are now changing the R environment. You can check whether a package is installed with require. The following code stops executing if the dplyr package is not installed.\n\ncheck_pkg_deps &lt;- function() {\n    if (!require(dplyr))\n        stop(\"the 'dplyr' package needs to be installed first\")\n}\n\nrequire() is similar to library(), but while the latter fails with an error if the package cannot be loaded, require returns TRUE or FALSE depending on whether the library was loaded and does not throw an error if the package cannot be found. Think of require as the version of library you should use inside of functions.\n\n\nDocumentation\nComments in code are not documentation. Documentation is a detailed explanation of the purpose of the code, how it works, how its functions work, their arguments, etc. It also includes all information someone would want to need to take over the project. In literal programs you have the opportunity to write code and documentation at the same time. Many software authoring frameworks include steps in programming that generate the documentation. For example, to add documentation to an R package, you need to create a “man” subdirectory that contains one file per function in the special R Documentation format (.Rd). You can see what the files look like by browsing R packages on GitHub. For example, here is the repository for the ada package.\nAt a minimum a README file in Markdown should accompany the program. The file has setup instructions and use instructions someone would have to follow to execute the code. It identifies author, version, major revision history, and details on the functions in the public API—those functions called by the user of the program.\nThere are great automated documentation systems such as doxygen which annotate the source code in such a way that documentation can be extracted automatically. An R package for generating inline documentation that was inspired by doxygen is roxygen2.\n\n\n\nFigure 10.2: The lifecycle of a file in Git. Source\n\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "irls_literate.html",
    "href": "irls_literate.html",
    "title": "11  Iteratively Reweighted Least Squares",
    "section": "",
    "text": "11.1 Generalized Linear Models\nThe generalization of GLMs is found in a comparison to the classical linear model with Gaussian errors: \\[\nY = \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\epsilon, \\quad \\epsilon \\sim \\textit{ iid } G(0, \\sigma^2)\n\\tag{11.1}\\]\nHere \\(Y\\) is the target variable of interest (dependent variable), \\(\\textbf{x}= [1, x_1,\\cdots,x_p]^\\prime\\) is a \\((p \\times 1)\\) vector of input variables, \\(\\boldsymbol{\\beta}\\) is the vector of parameters of the mean function, and \\(\\epsilon\\) is an error term. These error terms are independently and identically distributed as Gaussian random variables with mean zero and common variance \\(\\sigma^2\\).\nGLMs relax several elements of this model:\nHowever, the relaxation of the model conditions is not without limits. Rather than allowing \\(Y\\) to have any distribution, its distribution has to be a member of a special family of probability distributions known as the exponential family of distributions. Rather than allowing any arbitrary nonlinear relationship between inputs and target, only certain (invertible) transformations are permitted; the effect of the inputs remains linear on some scale, although it is usually not the scale of the mean. Rather than allowing any arbitrary variance, the variance of the targets can be unequal but it is determined through the distribution itself.\nThe specification of a GLM includes the following components:\nThe parameters \\(\\boldsymbol{\\beta}\\) in Equation 11.1 can be estimated by least squares in closed form. That means we can compute the parameter estimates directly. If \\(\\textbf{X}\\) is of full column rank, the ordinary least squares estimates is \\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] and this is also the maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#generalized-linear-models",
    "href": "irls_literate.html#generalized-linear-models",
    "title": "11  Iteratively Reweighted Least Squares",
    "section": "",
    "text": "The distribution of \\(Y\\) does not have to be Gaussian.\nThe relationship between the inputs and the mean of \\(Y\\) does not have to be linear in the coefficients.\nThe model does not have an additive error structure.\nThe target variables do not have to have the same variance.\n\n\n\n\n\\(Y \\sim P_{expo}\\): \\(Y\\) follows a distribution in the exponential family, this family includes important distributions such as the Bernoulli, Binomial, Negative Binomial, Poisson, Geometric, Exponential, Gamma, Beta, Gaussian.\n\\(\\text{E}[Y] = \\mu\\)\n\\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\): a predictor \\(\\eta\\) that is linear in the parameters\n\\(g(\\mu) = \\eta\\): A transformation of the mean is linearly related to the parameters. The function \\(g()\\) is called the link function of the GLM and is invertible, that is, \\(\\mu = g^{-1}(\\eta)\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#iteratively-reweighted-least-squares",
    "href": "irls_literate.html#iteratively-reweighted-least-squares",
    "title": "11  Iteratively Reweighted Least Squares",
    "section": "11.2 Iteratively Reweighted Least Squares",
    "text": "11.2 Iteratively Reweighted Least Squares\nFor other GLMs, the maximum likelihood estimates have to be found numerically. McCullagh and Nelder Frs (1989) show that the following procedure, known as iteratively reweighted least squares, converges to the maximum likelihood estimates. To motivate the algorithm we start by a linear approximation (a first-order Taylor series) of the linked observations about an estimate of the mean: \\[\n\\begin{align*}\ng(y) &= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[ \\frac{\\partial g(y)}{\\partial y}\\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= \\widehat{\\eta} + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}}  \\\\\n&= z\n\\end{align*}\n\\tag{11.2}\\]\n\\(z\\) is called an adjusted dependent variable or a working response variable or a pseudo-response. The final expression in Equation 11.2 can be viewed as a linear model with response variable \\(z\\), systematic part \\(\\widehat{\\eta} = \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}}\\) and error term \\((y-\\mu)[\\partial \\eta/\\partial \\mu]\\). The variance of this error term is \\[\n\\text{Var}[z] = \\left[\\frac{\\partial \\eta}{\\partial \\mu}\\right]^2 \\text{Var}[Y]\n\\]\nThe iterative procedure is as follows: given an initial value of \\(z\\), which requires an initial estimate of \\(\\mu\\), fit a weighted linear model with inputs \\(\\textbf{x}= [x_1,\\cdots,x_p]^\\prime\\) and weights given by the inverse of \\(\\text{Var}[z]\\). The solution to the weighted linear model is an updated parameter vector \\(\\boldsymbol{\\beta}\\). Re-calculate \\(z\\) and the weights and repeat the weighted linear regression fit. Continue until the relative change in the parameter estimates, the log likelihood function, the deviance, or some other criterion is negligible.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#literate-programming",
    "href": "irls_literate.html#literate-programming",
    "title": "11  Iteratively Reweighted Least Squares",
    "section": "11.3 Literate Programming",
    "text": "11.3 Literate Programming\nIteratively reweighted least squares (IRLS) can be implemented without explicit specification of the probability distribution of the target variable \\(Y\\). All we need for the algorithm is the following:\n\nThe input variables \\(\\textbf{x}= [1, x_1, \\cdots, x_p]^\\prime\\) that form the linear predictor \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\)\nThe link function \\(g(\\mu) = \\eta\\) and its inverse \\(\\mu = g^{-1}(\\eta)\\)\nThe variance of an observation, \\(\\text{Var}[Y]\\). Note that in the exponential family the variance is related to the mean and the variance function is found as the derivative of the inverse canonical link function \\(b^\\prime(\\theta) = \\mu\\). The details are not important here, we can always find the variance as a function of the mean \\(\\mu\\) if we know which member of the exponential family we are dealing with.\nThe derivative of the linear predictor \\(\\eta\\) with respect to the mean \\(\\mu\\).\n\nTo compute the log likelihood of the data for a particular value of the parameter estimates, we also need to know the distribution and need to evaluate the log likelihood function.\n\nThe Data\nThe data for this application is the Bikeshare data that comes with the ISLR2 library and contains 8,645 records of the number of bike rentals per hour in Washington, DC along with time/date and weather information.\n\nlibrary(ISLR2)\nstr(Bikeshare)\n\n'data.frame':   8645 obs. of  15 variables:\n $ season    : num  1 1 1 1 1 1 1 1 1 1 ...\n $ mnth      : Factor w/ 12 levels \"Jan\",\"Feb\",\"March\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ day       : num  1 1 1 1 1 1 1 1 1 1 ...\n $ hr        : Factor w/ 24 levels \"0\",\"1\",\"2\",\"3\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ holiday   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ weekday   : num  6 6 6 6 6 6 6 6 6 6 ...\n $ workingday: num  0 0 0 0 0 0 0 0 0 0 ...\n $ weathersit: Factor w/ 4 levels \"clear\",\"cloudy/misty\",..: 1 1 1 1 1 2 1 1 1 1 ...\n $ temp      : num  0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.2 0.24 0.32 ...\n $ atemp     : num  0.288 0.273 0.273 0.288 0.288 ...\n $ hum       : num  0.81 0.8 0.8 0.75 0.75 0.75 0.8 0.86 0.75 0.76 ...\n $ windspeed : num  0 0 0 0 0 0.0896 0 0 0 0 ...\n $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...\n $ registered: num  13 32 27 10 1 1 0 2 7 6 ...\n $ bikers    : num  16 40 32 13 1 1 2 3 8 14 ...\n\n\nThe target variable is bikers, the total number of daily bikers. Because this is a count variable on a per unit (daily) basis, we implement a Poisson GLM with a log link. The input variables of interest are the factors month (mnth), weather situation (weathersit) and the temperature (temp).\n\n\nStarting Values\nSince the algorithm is iterative, we need to get it off the ground with starting values. From Equation 11.2 we can spy two different approaches for starting the iterations: with an initial value \\(\\boldsymbol{\\beta}^{(0)}\\) from which we calculate \\(\\mu^{(0)} =  g^{-1})(\\textbf{x}^\\prime\\boldsymbol{\\beta}^{(0)})\\) or with an initial value \\(\\mu^{(0)}\\) determined independently from \\(\\boldsymbol{\\beta}\\).\nThe second approach works if the data can be evaluated at the link function, possibly after a small adjustment. With a log link we need to worry about whether any of the counts are zero.\n\ninitial &lt;- function(values, link=\"log\", c=0.5) {\n    if (!is.null(values) && !anyNA(values)) {\n        if (toupper(link)==\"LOG\") {\n            if (sum(values == 0)) {\n                eta0 &lt;- log(ifelse(values==0,c,values))\n            } else {\n                eta0 &lt;- log(values)\n            }\n        }\n        return(eta0)\n    } else {\n        return (NULL)\n    }\n}\n\n\n\nFunctions\nSome additional functions will help evaluate the necessary pieces of the working variable \\(z\\) and the weights in the linear model step. The function dmu_deta returns the derivative of the mean with respect to the linear predictor, evaluated at the current estimates: \\[\n\\frac{\\partial \\mu}{\\partial \\eta} \\biggr\\vert_{\\widehat{\\mu}}\n\\] It is easy to extend that function to handle other link functions. The function get_mu returns the mean of the response based on the link function and the linear predictor. The function get_var computes the variance of the target variable based on the distribution in the exponential family, the function get_z constructs the working response variable, and get_w computes the weight for the reweighted least squares step.\n\nget_var &lt;- function(mu, dist=\"Poisson\") {\n    if (toupper(dist)==\"POISSON\") {\n        return (mu)\n    }\n    return (NULL)\n}\n\nget_mu &lt;- function(eta, link=\"log\") {\n    if (toupper(link)==\"LOG\") {\n        return (exp(eta))\n    }\n    return (NULL)\n}\n\ndeta_dmu &lt;- function(eta, link=\"log\") {\n    if (toupper(link)==\"LOG\") {\n        return(detadmu = 1/exp(eta))\n    }\n    return (NULL)\n}\n\nget_z &lt;- function(y, eta, link) {\n    if (is.null(y) || is.null(eta)) {\n        stop(\"null values not allowed\")\n    }\n    if (anyNA(y) || anyNA(eta)) {\n        stop(\"cannot handle missing values\")\n    }\n    z &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\n    return(z)\n}\n\n# The weight for the linear model is the inverse of the variance of\n# the working variable z.\nget_w &lt;- function(eta, link=\"log\", dist=\"poisson\") {\n    var_z &lt;- deta_dmu(eta,link)^2 * get_var(get_mu(eta,link),dist)\n    return (1/var_z)    \n}\n\n\n\nIterations\nThe iterations can be carried out in a loop. The algorithm stops when the max number of iterations is exceeded or when the largest relative absolute change in a parameter estimates between iterations \\(t+1\\) and \\(t\\) is smaller than a tolerance: \\[\n\\frac{\\max\\left\\{|\\widehat{\\beta}_j^{(t)} - \\widehat{\\beta}_j^{(t-1)}|\\right\\}}\n{\\max\\left\\{|\\widehat{\\beta}_j^{(t)}|,|\\widehat{\\beta}_j^{(t-1)}|\\right\\}} \\le \\text{tol}\n\\] The function converged checks whether the relative parameter convergence citerion is met:\n\nconverged &lt;- function(newbeta,oldbeta,tol=1e-6) {\n    diff &lt;- abs(newbeta - oldbeta)\n    denom &lt;- pmax(abs(newbeta),abs(oldbeta))\n    return (max(diff/denom) &lt; tol)\n}\n\nIn that case we say that the iterations have converged to a solution and report the parameter estimates.\nThe iterations call lm.wfit to fit a weighted linear model with model matrix \\(\\textbf{X}\\), target vector \\(\\textbf{z}= [z_1,\\cdots,z_n]^\\prime\\) and weight vector \\(\\textbf{w}= [w_1,\\cdots,w_n]^\\prime\\).\n\nmaxiter &lt;- 50\ntol &lt;- 1e-6\nY &lt;- Bikeshare$bikers\nX &lt;- model.matrix( ~ mnth + weathersit + temp, data = Bikeshare)\neta &lt;- initial(Y,\"log\")\n\nfor (iter in 1:maxiter) {\n    z &lt;- get_z(Y,eta,\"log\")\n    w &lt;- get_w(eta,\"log\",\"Poisson\")\n    # Fit a weighted linear model with response z, model X, and weight w\n    linreg &lt;- lm.wfit(X,z,w)\n    # Check whether the model has converged\n    if ((iter &gt; 1) && converged(linreg$coefficients,beta,tol)) {\n        beta &lt;- linreg$coefficients\n        cat(\"Converged after \", iter, \"iterations\")\n        print(beta)\n        break\n    } else {\n        beta &lt;- linreg$coefficients\n    }\n    # update eta for the next go-around\n    eta &lt;- linreg$fitted.values\n}\n\nConverged after  6 iterations              (Intercept)                   mnthFeb                 mnthMarch \n              3.367063899              -0.046719502              -0.006319815 \n                mnthApril                   mnthMay                  mnthJune \n             -0.109689766              -0.139946963              -0.428625482 \n                 mnthJuly                   mnthAug                  mnthSept \n             -0.714615564              -0.523849543              -0.213759334 \n                  mnthOct                   mnthNov                   mnthDec \n              0.163239847               0.242305655               0.321518995 \n   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow \n             -0.077249678              -0.474060776              -0.529583958 \n                     temp \n              3.391086355 \n\n\nThe algorithm converges after six iterations, not counting the initial setup. The parameter estiamtes can be validated with the glm function.\n\nmod.pois &lt;- glm(bikers ~ mnth + weathersit + temp,\n                data = Bikeshare, \n                family = \"poisson\")\nmod.pois$coefficients\n\n              (Intercept)                   mnthFeb                 mnthMarch \n              3.367063899              -0.046719502              -0.006319815 \n                mnthApril                   mnthMay                  mnthJune \n             -0.109689766              -0.139946963              -0.428625482 \n                 mnthJuly                   mnthAug                  mnthSept \n             -0.714615564              -0.523849543              -0.213759334 \n                  mnthOct                   mnthNov                   mnthDec \n              0.163239847               0.242305655               0.321518995 \n   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow \n             -0.077249678              -0.474060776              -0.529583958 \n                     temp \n              3.391086355",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#to-do",
    "href": "irls_literate.html#to-do",
    "title": "11  Iteratively Reweighted Least Squares",
    "section": "11.4 To Do",
    "text": "11.4 To Do\nYou can improve on the above implementation in a number of ways and you should give it a try:\n\nMonitor the log likelihood or deviance instead or in addition to the parameter estimates.\nReport the log likelihood, null deviance, and residual deviance at convergence.\nCompute approximate standard errors and approximate significance tests for the coefficients.\nStart the iterations from a vector of initial parameters \\(\\boldsymbol{\\beta}^{(0)}\\).\nExtend the program to other link functions and distributions.\nHow would you accommodate a Binomial(\\(n,\\pi\\)) distribution?\nRewrite the code in terms of family() objects, see ?family() for details. As an example, poisson()$mu.eta reports the function that computes \\(\\partial \\mu /\\partial \\eta\\) for the default link function of the poisson family.\nAdd an offset variable to the generalized linear model.\nAccommodate a scale parameter (two-parameter exponential family).\n\n\n\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear Models, 2nd Ed. Chapman & Hall, New York.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021.\nModern Data Science with r, 2nd Ed. Chapman & Hall/CRC\nPress. https://mdsr-book.github.io/mdsr3e/.\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Date with\nYour Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.\n\n\nBox, G. E. P., G. M. jenkins, and G. C. Reinsel. 1976. Time Series\nAnalysis, Forecasting and Control. 3rd. Ed. Holden-Day.\n\n\nBox, G. E. P., and M. E. Muller. 1958. “A Note on the Generation\nof Normal Random Deviates.” Annals of Mathematical\nStatistics 29: 610–11. doi:10.1214/aoms/1177706645.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for\nVisualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nGelman, A., and A. Unwin. 2013. “Infovis and Statistical Graphics:\nDifferent Goals, Different Looks.” Journal of Computational\nand Graphical Statistics 22: 2–28. https://www.tandfonline.com/doi/full/10.1080/10618600.2012.761137.\n\n\nKnuth, D. E. 1984. “Literate Programming.”\nThe Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMatsumoto, M., and T. Nishimura. 1998. “Mersenne Twister: A\n623-Dimensionally Equidistributed Uniform Pseudo-Random Number\nGenerator.” ACM Transactions on Modeling and Computer\nSimulation 8: 3–30.\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear\nModels, 2nd Ed. Chapman & Hall, New York.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with\nPandas, NumPy and Jupyter, 3rd Ed. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering\nSoftware Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nSchabenberger, O., and Francis J. Pierce. 2001. Contemporary\nStatistical Models for the Plant and Soil Sciences. CRC Press, Boca\nRaton.\n\n\nTufte, E. 1983. The Visual Display of Quantitative Information.\nGraphics Press.\n\n\n———. 2001. The Visual Display of Quantitative Information, 2nd\nEd. Graphics Press.\n\n\nTukey, John W. 1993. “Graphic Comparisons of Several Linked\nAspects: Alternatives and Suggested Principles.” Journal of\nComputational and Graphical Statistics 2 (1): 1–33. http://www.jstor.org/stable/1390951.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook. O’Reilly\nMedia. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC\nPress. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data\nScience: Import, Tidy, Transform, Visualize, and Model Data, 2nd\nEd. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization.\nO’Reilly Media. https://clauswilke.com/dataviz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown:\nThe Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R\nMarkdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "References"
    ]
  }
]