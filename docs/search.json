[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Programming",
    "section": "",
    "text": "Preface\nWelcome to this short treatise about statistical programming and statistical program packages. This is online material in a series of collections used in teaching data science and statistics at the graduate level. This material on statistical programming accompanies\n\nFoundations of Data Science–Beyond the Numbers (Foundations) and\nStatistical Learning (StatLearning).\n\nFoundations covers fundamental concepts in data science and the data science project life cycle. StatLearning is a comprehensive collection of modules on supervised and unsupervised learning from a statistical perspective.\nAll three “books” were written in Quarto because it combines prose with math and supports multiple programming languages and kernels within the same framework. To learn more about Quarto books visit https://quarto.org/docs/books.\n\nIn statistical programming it is not possible to completely separate concept—validating an algorithm with matrix/vector algebra, for example—from implementation—in R or Python or SAS or SPSS or … . Tools used in statistical programming have changed dramatically over the last decades. When I was in graduate school and when I first taught statistics, SAS was the undisputed king of the hill—with the exception of one, every graduate course that touched on computing with data was using SAS.\nI joined SAS in 2001 and contributed to the development of statistical algorithms for many years, maintaining and authoring statistical tools such as the MIXED, NLMIXED, GLIMMIX, NLIN, FMM, PLM procedures and working on subsystems used across many procedures.\nSine then, the world of statistical programming has changed—a lot! Open source statistical languages like R and open source programming languages such as Python are leading the way. There is still some room for proprietary languages such as SAS, STATA, SPSS, and tools like JMP (a SAS business unit), Minitab, and others. But that room is continuing to shrink. Data scientists today predominantly work in R or Python and this is what the modules that follow is focused on.\nIs R better than Python or is Python better than R? As always, it depends. R was developed as a statistical programming language, not unlike SAS. This is evident in the way it manipulates data, invokes algorithms, formulates statistical models, handles factors, etc. As someone once joked\n\nR is what happens when statisticians design a programming language.\n\nIt should be added that the author of this quote is a committed Pythonista.\nThe same could be said about SAS. And that is not all bad. R is designed for statistical programming and is excellent for that purpose.\nPython is a general purpose scripting language that has been fortified to perform domain-specific tasks through libraries. My experience has been that those who come to data science from a statistical background are versed in R, those that come from a non-statistical background are more familiar with Python. You can stick with what you are comfortable with. The concepts in statistical programming are the same, their implementations might differ depending on the tool.\nOne of the more vexing issues I have with Python for statistical programming is that the origin of many popular libraries lies in machine learning. Not that there is anything wrong with it, but the approach to data analytics in statistical modeling and in machine learning is different. Statistical modeling is based on the notion of a data-generating mechanism, the data at hand is a realization of a stochastic process. Statistical properties of the estimated quantities derive from the underlying random process and the particulars of the estimation principle. Machine learning does not appeal to a data-generating mechanism, although it has the concept of “errors”, deviations between prediction and ground truth, which are treated as random variables. The standard output from Python libraries such as scikit-learn often does not produce the quantities statisticians are looking for, sometimes they are not produced by any combination of methods or options. Not having access to standard errors for estimated quantities is a head-scratcher for statisticians.\nOn the other hand, statistics relies heavily on asymptotic properties of estimators, what happens when sample sizes grow to infinity. Machine learning is more concerned with inferences for the data set at hand, not for some imaginary size. That is a healthy approach in my opinion.\n\nOne of the most instructive and insightful things to do in statistics is recomputing the quantities you see on software output from scratch. This often uses matrix-vector operations, random number generation, simulation. Your implementation is probably not as efficient as the code in the software library and that is OK. Well-written software rarely takes formulas straight from the paper. The best way of communicating mathematics for human interpretation is not the best way of communicating the finite precision operations to a machine. For example, the eigenvalue decomposition of a symmetric matrix is a straightforward mathematical concept. The matrices of eigenvectors and eigenvalues are extremely important to distill properties of the data, for example in a Principal Component Analysis (PCA). Performing the eigendecomposition in a computer is fraught with precision issues and a more stable approach numerically is to go through a singular value decomposition instead.\nNevertheless, working through the eigendecomposition for a well-behaved data set and comparing the results to say, the princomp function in R is a great exercise and deepens your understanding of PCA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Statistical Computing and Computational Statistics\nThe terms statistical computing and computational statistics are often used interchangeably. Trying to separate them seems like semantic hair splitting. But we shall try.\nThe field of statistics can broadly be categorized into mathematical statistics and computational statistics. The former derives results based on the mathematical-statistical properties of quantities derived from data. The central limit theorem, for example, tells us that if \\(Y_1,\\cdots,Y_n\\) are a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the distribution of \\[\nZ = \\frac{\\overline{Y}-\\mu}{\\sigma/\\sqrt{n}}\n\\] approaches that of a standard Gaussian (\\(G(0,1)\\)) as \\(n\\rightarrow \\infty\\). That is a result in mathematical statistics. We also know that if the \\(Y_i\\) are \\(\\textit{iid } G(\\mu,\\sigma^2)\\), then \\(Z\\) is exactly \\(G(0,1)\\) distributed (for any \\(n\\)) and in that case \\[\nT = \\frac{\\overline{Y}-\\mu}{s/\\sqrt{n}}\n\\tag{1.1}\\]\nfollows a \\(t_{n-1}\\) distribution with \\(n-1\\) degrees of freedom. The quantity \\(s\\) in Equation 1.1 is the standard deviation of the sample, \\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n \\left(Y_i - \\overline{Y}\\right)^2}\n\\]\nTo evaluate the hypothesis \\(H:\\mu = 4\\) based on a random sample from a Gaussian distribution, you apply statistical computing to the mathematical statistical result and evaluate the quantities with the help of a computer and software. The software implementation of the \\(t\\) test needs to compute \\(\\overline{y}\\) and \\(s\\) based on the sample. There are multiple ways of doing that. For example, on a single pass through the data the code might compute: \\[\nS_1 = \\sum_{i=1}^{n^*} y_i \\qquad S_2 = \\sum_{i=1}^{n^*} y_i^2\n\\] for the \\(n^*\\) observations without missing values for \\(y\\). \\(n^*\\), the number of valid samples, is a by-product of this pass. The sample mean and standard deviation are then computed as \\[\n\\overline{y} = \\frac{S_1}{n^*} \\qquad s = \\sqrt{\\frac{1}{n^*-1} \\left(S_2 - S_1^2/n^*\\right)}\n\\] A second approach would be to perform two passes through the data, computing \\(\\overline{y}\\) on the first pass and the sum of the squared differences from the sample mean on the second pass. The third method uses the knowledge that the least-squares estimate in an intercept-only model is the sample mean and uses the lm function in R to fit a model. The three approaches, including the result from the built-in method, are shown in the following code snippet.\nset.seed(1335)\ny &lt;- rnorm(100,mean=3,sd=2)\nn &lt;- length(y)\n\n# Method 1\nS1 &lt;- sum(y)\nS2 &lt;- sum(y^2)\nybar &lt;- S1/n\nsd &lt;- sqrt((S2-S1^2/n)/(n-1))\ncat(\"Sample mean: \", ybar, \" Sample std. dev: \",sd,\"\\n\")\n\nSample mean:  2.913484  Sample std. dev:  1.809064 \n\n# Method 2\nybar &lt;- sum(y)/n\nsd &lt;- sqrt(sum((y-ybar)^2)/(n-1))\ncat(\"Sample mean: \", ybar, \" Sample std. dev: \",sd,\"\\n\")\n\nSample mean:  2.913484  Sample std. dev:  1.809064 \n\n# Method 3\nsummary(lm(y ~ 1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5917 -1.4713  0.0674  0.9708  5.0478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9135     0.1809    16.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.809 on 99 degrees of freedom\nThe Std. Error shown in the summary from lm is the standard deviation of the Estimate, which in turn is \\(\\overline{y}\\). The standard error reported there is thus an estimate of \\[\n\\sqrt{\\text{Var}[\\overline{Y}]} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n# Built-in methods\nmean(y)\n\n[1] 2.913484\n\nsd(y)\n\n[1] 1.809064\nComputational statistics, compared to mathematical statistics, uses computational methods to solve statistical problems. Rather than relying on asymptotic distributional properties of estimators, computational statistics uses the power of computers to derive the actual distribution of estimators. A famous example of a computational statistical method is the bootstrap, a resampling procedure that draws \\(B\\) random samples of size \\(n\\) with replacement from the sample data. You calculate the value of the statistic of interest in each bootstrap sample and build up the distribution of the statistic. At the end of the bootstrap procedure you can summarize this distribution any way you wish, using statistical computing. The next code snippet calculates the bootstrap estimate of the sample mean and reports the center and standard deviation of its distribution after 10, 100, and 1000 bootstrap samples. We know from basic statistics that the theoretical values are \\[\n\\text{E}[\\overline{Y}] = \\mu = 3 \\qquad \\sqrt{\\text{Var}[\\overline{Y}]} = \\frac{\\sigma}{\\sqrt{n}}=0.2\n\\]\nbootstrap &lt;- function(y,B=500) {\n    n &lt;- length(y)\n    S1 &lt;- 0\n    S2 &lt;- 0\n    for (i in 1:B) {\n        bs &lt;- sample(n,n,replace=TRUE)\n        stat &lt;- mean(y[bs])\n        S1 &lt;- S1 + stat\n        S2 &lt;- S2 + stat^2\n    }\n    ybar &lt;- S1/B\n    sd &lt;- sqrt((S2-S1^2/B)/(B-1))\n    return(list(ybar=ybar,sd=sd))\n}\n\nset.seed(6543)\nbootstrap(y,10)\n\n$ybar\n[1] 2.836697\n\n$sd\n[1] 0.1959311\n\nbootstrap(y,100)\n\n$ybar\n[1] 2.950894\n\n$sd\n[1] 0.2039954\n\nbootstrap(y,1000)\n\n$ybar\n[1] 2.910958\n\n$sd\n[1] 0.1813154\nStatistical methods that rely on random number generators to derive statistical distributions belong to computational statistics. Cross-validation, bootstrapping, bagging, and Markov chain Monte Carlo methods, are examples. Computational statistics also includes computer-intensive methods that cannot be solved without the assistance of computers. Examples are numerical optimization for nonlinear problems, artificial neural networks, support vector machines, gradient boosting, and so on.\nBy applying computer science to data, machine learning has dramatically increased the number of techniques that rely on computational methods over theory. One of the most influential papers in the last decade, entitled “Attention is all you need”, by Vaswani et al. (2017), introduced us to the multi-head attention mechanism in encoder-decoder models and laid the foundation for the large-language model revolution that led to GPT, Gemini and others. The paper has been cited over 127,000 times (as of June 2024) and does not contain a single theorem or proof—it is a clinic in applying computational concepts to data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#languages-and-tools",
    "href": "intro.html#languages-and-tools",
    "title": "1  Introduction",
    "section": "1.2 Languages and Tools",
    "text": "1.2 Languages and Tools\nJust like general software and application development, statistical data processing can be done with no-code, low-code, or high-code tools/environments. A high-code environment is a traditional programming environment where developers sling code. A low-code environment allows the assembly of software from pre-built, reusable components using visual drag-and-drop interfaces. A no-code environment allows users with no programming skills to build applications. While low-code platforms require some programming skills to stitch together components, no-code environments do not require any coding skills. This comes at the expense of rigid templates and built-ins that often do not offer enough opportunities for customization.\nThe traditional way to perform statistical analysis is through statistical programming using an integrated development environment (IDE). Some products offer programmatic (high-code) and visual (no/low-code) interfaces (SAS, JMP, Stata, IBM SPSS); they have their own IDEs.\nOver the last decades open-source tools have pushed proprietary tools and solutions toward the edges of the data analytics market. You cannot measure this effect in terms of market share, as this metric is based on revenue numbers, but it is clearly seen in user engagement, community activity, and the anemic growth numbers of some of the commercial alternatives for statistical and machine learning software. If the market grows by 10% year-over-year and your growth is flat, you are losing share of the market.\nThe big winners in this transition are R and Python. The former is a statistical programming language based originally on the S language. The otherwise capable commercial S-Plus product was doomed once open-source R took hold. You can still see references to original S implementations in R documentations today. Popular packages such as MASS (Modern Applied Statistics with S) started as S modules.\nWhile R was designed as a domain-specific programming language for statistical applications, Python is a general-purpose language. Through libraries such as pandas, numpy, scipy, polars, statsmodels, scikit-learn, keras, matplotlib, seaborn and many others, Python has evolved into a highly capable language for data processing. In the area of deep learning and artificial intelligence, Python has become the standard. As you progress on the analytic journey you will find R to be a great language for many statistical needs. As you move into modern machine learning and artificial intelligence you will transition toward Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#statistical-programming-and-software-engineering",
    "href": "intro.html#statistical-programming-and-software-engineering",
    "title": "1  Introduction",
    "section": "1.3 Statistical Programming and Software Engineering",
    "text": "1.3 Statistical Programming and Software Engineering\nA statistical program is a piece of software, but statistical programming is not bona fide software engineering. Data scientists and statistical programmers are sometimes compared to software developers. Yes, they do share certain traits; both are using tools, languages, and frameworks to build complex systems with software. And because the result is software, statistical programmers need to know the principles of good software engineering and how to apply these in the context of a statistical program:\n\nModularity: Separate software into components according to functionality and responsibility (functions, modules, public and private interfaces)\nSeparation of Concerns: Human beings have a limited capacity to manage contexts. Breaking down a larger task into units and abstractions that you can deal with one at a time is helpful. Interface and implementation are separate concerns. Data quality and data modeling are separate concerns. Not in the sense that they are unrelated, low quality data leads to low quality models—garbage in, garbage out. But in the sense that you can deal with data quality prior to the modeling task. Code efficiency (runtime performance) is sometimes listed as an example for separating concerns: write the code first to meet criteria such as correctness and robustness, then optimize the code for efficiency, focusing on the parts of the code the run spends most time in.\nAbstraction: Separate the behavior of software components from their implementation. Look at each component from two points of views: what it does and how it does it. A client-facing API (Application Programming Interface) specifies what a module does. It does not convey the implementation details. By looking at the function interface of prcomp and princomp in R, you cannot tell that one function is based on singular-value decomposition and the other is based on eigenvalue decomposition.\nGenerality: Software should be free from restrictions that limit its use as an automated solution for the problem at hand. Limiting supported data types to doubles and fixed-size strings is convenient, but not sufficiently general to deal with today’s varied data formats (unstructured text, audio, video, etc.). The “Year 2000” issue is a good example of lack of generality that threatened the digital economy: to save memory, years were represented in software products as two-digit numbers, causing havoc when 1999 (“99”) rolled over to “00” on January 1, 2000.\nAnticipation of Change: Software is an automated solution. It is rarely finished on the first go-around; the process is iterative. Starting from client requirements the product evolves in a back and forth between client and developer, each side refining their understanding of the process and the product at each step. Writing software that can easily change is important and often difficult. When software components are tightly coupled and depend on each other, it is unlikely that you can swap out for another without affecting both.\nConsistency: It is easier to do things within a familiar context. Consistent layout of code and user interfaces helps the programmer as well as the user as well as the next programmer. Consistency in code formatting, comments, naming conventions, variable assignments, etc. makes it easier to read and modify code and helps to prevent errors. When you are consistent in initializing all local variables in C functions, you will never have uninitialized variable bugs.\n\nBut there are important differences between statistical programming and general software engineering. These stem to a large part from the inherent uncertainty and unpredictability of the data, the raw material of a statistical program.\n\nInput inherently unpredictable and uncertain. Statistical code is different from non-analytic code in that it is processing an uncertain input. A JSON parser also processes variability, each JSON document is different from the next. Does it not also deal with uncertain input? If the parser is free of bugs, the result of parsing is known with certainty. For example, we are convinced that the sentence “this book is certainly concerned with uncertainty” has been correctly extracted from the JSON file. Assessing the sentiment of the sentence, however, is a data science task: a sentiment model is applied to the text and returns a set of probabilities indicating how likely the model believes the sentiment of the text is negative, neutral, or positive. Subsequent steps taken in the software are based on interpreting what is probable.\nUncertainty about methods. Whether a software developer uses a quicksort or merge sort algorithm to order an array has impact on the performance of the code but not on the result. Whether you choose a decision tree or a support vector machine to classify the data in the array impacts the performance and the result of the code. A chosen value for a tuning parameter, e.g., the learning rate, can produce stable results with one data set and highly volatile results with another.\nRandom elements in code. Further uncertainty is introduced through analytic steps that are themselves random. Splitting data into training and test data sets, creating random folds for cross-validation, drawing bootstrap samples in random forests, random starting values in clustering or neural networks, selecting the predictors in random forests, Monte Carlo estimation, are some examples where data analysis involves drawing random numbers. The statistical programmer needs to ensure that random number sequences that create different numerical results do not affect the quality of the answers. The results are frequently made repeatable by fixing the seed or starting value of the random number generator. While this makes the program flow repeatable, it is yet another quantity that affects the numerical results. It is also a potential source for misuse: “let me see if another seed value produces a smaller prediction error.”\nData are messy. Data contains missing values and can be full of errors. There is uncertainty about how disparate data sources represent a feature (a customer, a region, a temperature) that affects how you integrate the data sources. These sources of uncertainty can be managed through proper data quality and data integration. As a data scientist you need to be aware and respectful of these issues; they can doom a project if not properly addressed. In an organization without a dedicated data engineering team resolving data quality issues might fall on your shoulders. If you are lucky to work with a data engineering team you still need to be mindful of these challenges and able to confirm that they have been addressed or deal with some of them (missing values).\n\nOther differences between statistical programming and software engineering are\n\nThe use of high-level languages. statistical programming uses languages like R and Python. The software is written at a high level of abstraction, calling into existing packages and libraries. Rather than writing your own implementation of a random forest, you use someone else’s implementation. Instead, your concern shifts to how to use the hyperparameters of the random forest to the greatest effect for the particular data set. You can perform statistical programming in C, C++, or Rust. These system-level languages are best for implementing efficient algorithms, that are then called from a higher-level interface in R or Python.\nThe length of the programs. Statistical programs are typically short, a few hundred to a few thousands lines long. While a thousand lines of Python code may sound like much, it is not much compared to the size of large software engineering projects.\nThe programs are often standalone. A single file or module can contain all the code you need for a statistics project. That is good and bad. Good because it is easy to maintain. Bad because we often skip steps of good software hygiene such as documentation and source control.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-in-this-book",
    "href": "intro.html#what-is-in-this-book",
    "title": "1  Introduction",
    "section": "1.4 What is in this Book?",
    "text": "1.4 What is in this Book?\nThis book is a crash course in statistical programming based primarily on R and Python. It is not a treatise on computational statistics or statistical computing. It is also not an introductory text on R or Python. There are many excellent resources available for free on both languages. Below is a list of resources I have consulted.\nThe approach will be to solve practical problems in statistical programming and introduce concepts as we go along. Rather than dwelling on data types, operators, flow control, basic functions, and so on, the examples in the text will cover some of these concepts implicitly.\n\nR Resources\n\nR for Data Science, 2nd ed. (Wickham, Cetinkaya-Rundel, and Grolemund 2023).\nAdvanced R (Wickham 2019).\nModern Data Science with R, 2nd ed. (Baumer, Kaplan, and Horton 2021).\nR Graphics Cookbook: Practical Recipes for Visualizaing Data, 2nd ed. (Chang 2018)\nR Markdown: The Definite Guide (Xie, Allaire, and Grolemund 2019)\nR Markdown Cookbook (Xie, Dervieux, and Riederer 2021)\n\n\n\nBasic Packages\n\nFind an R Package\ntidyverse\n\ndplyr\nggplot2\nstringr\n\n\n\n\nPython Resources\n\nPython for Data Analysis: Data Wrangling with pandas, NumPy and Jupyter, 3rd ed. (McKinney 2022)\nPython Data Science Handbook (VanderPlas 2016)\n\n\nBasic Libraries\n\nNumPy User’s Guide\npandas documentation\npolars documentation\n\n\n\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021. Modern Data Science with r, 2nd Ed. Chapman & Hall/CRC Press. https://mdsr-book.github.io/mdsr3e/.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy and Jupyter, 3rd Ed. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook. O’Reilly Media. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd Ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown: The Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R Markdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "2.1 Getting Started with R\nTo get started with R as a statistical programming language you need access to R itself and a development environment from which to submit R code.\nDownload R for your operating system from the CRAN site. CRAN is the “Comprehensive R Archive Network” and also serves as the package management system to add new packages to your installation.\nIf you use VS Code as a development environment, add the “R Extension for Visual Studio” to your environment. We are focusing on RStudio as a development environment here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-getting-started-R",
    "href": "getting_started.html#sec-getting-started-R",
    "title": "2  Getting Started",
    "section": "",
    "text": "TL;DR What you Need\n\n\n\n\n\nTo work with R in this course, you need to be able to run R code, mix it with prose and formulas in a notebook-style environment, and turn program and output into pdf and html files. To accomplish this you will need\n\nR. Download from CRAN\nRStudio. Download RStudio Desktop from Posit\nLaTex. TinyTeX is a small distribution based on Tex Live that works well with R and can be manipulated through the tinytex R package.\n\nYou can skip R and RStudio installs if you do the work in a Posit Cloud account. These are available for free here.\n\n\n\n\n\n\n\nPosit Cloud\nIn today’s cloud world, you can get both through Posit Cloud. Posit is the company behind RStudio, Quarto, and other cool tools. Their cloud offering gives you access to an RStudio instance in the cloud. You can sign up for a free account here. The only drawback of the free account is its limitations in terms of RAM, CPU, execution time, etc. For the work you will be doing in this course, and probably many other courses, you will not exceed the limitations of the free account.\nOnce you have created an account, the workspace is organized the same way as a RStudio session on your desktop.\n\n\nR and RStudio\nRStudio is an integrated development environment (IDE) for R, but supports other languages as well. For example, using Quarto in RStudio, you can mix R, Python, and other code within the same document. Download Rstudio Desktop here.\nThe RStudio IDE is organized in panes, each pane can have multiple tabs (Figure 2.1). The important panes are\n\nSource. The files you edit. These can be R files (.R), Rmarkdown (.Rmd), Quarto (.qmd), or any other text files.\nConsole. Here you can enter R commands directly at the command prompt “&gt;”. This pane also has a Terminal tab for an OS terminal and a Background Jobs tab. The latter is important when you knit documents into pdf or html format.\nEnvitonment. Displays information about the objects created in the R session. You can click on an object for a more detailed look at it in the Viewer.\nHelp. This pane contains many useful tabs, such as a File browse, package information, access to the documentation and help system. Plots generated from the Console or from an R script are displayed in the Plots tab of this pane.\n\n\n\n\n\n\n\nFigure 2.1: RStudio IDE\n\n\n\n\n\nPackage Management\nThe R installation comes with attached base packages, you do not need to install or load those. Any other packages are enabled in a two-step process:\n\nInstall the package\nLoad the package in your R session with the library() command.\n\nInstalling the package is done once, this step adds the package to your system. Loading the library associated with the package needs to be done in every R session. Without loading the library, R cannot find the functions exported by the library.\n\nInstalling standard packages\nA standard R package is made available through the CRAN (Comprehensive R Archive Network) repositories. To install package “foo” from CRAN use\n\ninstall.packages(\"foo\")\n\nTo install multiple packages, specify them as a character vector:\n\ninstall.packages(c(\"foo\",\"bar\",\"foobar\"))\n\nTo uninstall (remove) one or more packages from a system, use the\n\nremove.packages(c(\"foo\",\"bar\"))\n\ncommand.\nPackages are installed by default into the directory given as the first element of the .libPaths() function. On my Mac this is\n\n.libPaths()[1]\n\n[1] \"/Users/olivers/Library/R/arm64/4.3/library\"\n\n\nIf you wish to install a package in a different location, provide the location in the lib=\"\" argument of install.packages(). Note that if you use a non-default location for the package install you need to specify that location when you load the library with the library command.\nTo make the functionality in a package available to your R session, use the library command. For example, the following statements make the dplyr and Rfast functions available.\n\nlibrary(\"dplyr\")\nlibrary(\"Rfast\")\n\nLibraries export functions into the R name space and sometimes these can collide. For example, the Rfast package exports functions knn and knn.cv for \\(k\\)-nearest neighbor and cross-validated \\(k\\)-nearest neighbor analysis. Functions by the same name also exist in the class package. To make it explicit which function to use, prepend the function name with the package name:\n\nRfast::knn()\n\nclass::knn.cv()\n\nTo load a library from a non-standard location, for example, when you installed the package in a special directory by using lib= on install.packages(), you need to specify the lib.loc=\"\" option in the library command.\n\ninstall.packages(\"some_package_name\", lib=\"/custom_path/to/packages/\")\n\nlibrary(\"some_package_name\", lib.loc=\"/custom_path/to/packages/\")\n\nAll available packages in your R environment can be seen with the\n\nlibrary() \n\ncommand.\nLibraries have dependencies and if you want to install all libraries that a given one depends on, choose dependencies=TRUE in the install.packages() call:\n\ninstall.packages(\"randomForest\", dependencies=TRUE)\n\n\n\nInstalling non-standard packages\nA package that is not served by the CRAN repository cannot be installed with install.packages(). The need for this might arise when you want to install a developer-modified version of a package before it lands on CRAN. This can be accomplished with the devtools package. The following statements install “some_package” from GitHub.\n\nlibrary(\"devtools\")\ndevtools::install_github(\"some_package\")\n\nOnce a non-standard package is installed you load it into a session in the same way as a standard package, with the library command.\nYou can see all packages installed on your system with\n\nas.vector(installed.packages()[,\"Package\"])\n\nand the packages loaded into your workspace with\n\n(.packages())\n\n [1] \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"       \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n\nA more detailed breakdown of the packages in groups, along with other information about the session, is available from sessionInfo().\nAs you write more R code and add packages to your system, you will ask yourself “Did I not install that previously?” The following code snippet helps to install only those packages from a list that are not already installed.\n\nlibs_to_load &lt;- c(\"dplyr\", \"readr\", \"magrittr\",\"reshape2\",\"ggplot2\")\nlibs_to_install &lt;- libs_to_load[!libs_to_load %in% installed.packages()]\nfor (lib in libs_to_install) install.packages(lib, dependencies=TRUE)\nsapply(libs_to_load, library, character=TRUE)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\n$dplyr\n [1] \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"       \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n$readr\n [1] \"readr\"        \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"        \n [6] \"dplyr\"        \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"       \n[11] \"datasets\"     \"methods\"      \"base\"        \n\n$magrittr\n [1] \"magrittr\"     \"readr\"        \"Rfast\"        \"RcppParallel\" \"RcppZiggurat\"\n [6] \"Rcpp\"         \"dplyr\"        \"stats\"        \"graphics\"     \"grDevices\"   \n[11] \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n$reshape2\n [1] \"reshape2\"     \"magrittr\"     \"readr\"        \"Rfast\"        \"RcppParallel\"\n [6] \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"        \"stats\"        \"graphics\"    \n[11] \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n$ggplot2\n [1] \"ggplot2\"      \"reshape2\"     \"magrittr\"     \"readr\"        \"Rfast\"       \n [6] \"RcppParallel\" \"RcppZiggurat\" \"Rcpp\"         \"dplyr\"        \"stats\"       \n[11] \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"     \n[16] \"base\"        \n\n\n\n\nUnloading a library\nThe easiest way to unload the libraries you loaded in an R session is to restart the session. 😊\nTo unload a library from an R session you can use the detach function with the unload=TRUE option. For example, to remove the randomForest library without restarting the session:\n\ndetach(\"package:randomForest\",unload=TRUE)\n\n\n\n\nSession Information\nIt is a good practice to add at the end of R programs a listing of the environment in which the program executed. This will show others what packages were loaded and their version. If you use the RNG=TRUE option, the random number generators are also reported, more on this in Chapter 7.\nFor this session, the info is as follows:\n\nsinfo &lt;- sessionInfo()\nprint(sinfo,RNG=T)\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nRandom number generation:\n RNG:     Mersenne-Twister \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.4      reshape2_1.4.4     magrittr_2.0.3     readr_2.1.4       \n[5] Rfast_2.1.0        RcppParallel_5.1.7 RcppZiggurat_0.1.6 Rcpp_1.0.11       \n[9] dplyr_1.1.4       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      jsonlite_1.8.8    compiler_4.3.0    tidyselect_1.2.1 \n [5] stringr_1.5.0     parallel_4.3.0    scales_1.3.0      fastmap_1.1.1    \n [9] R6_2.5.1          plyr_1.8.9        generics_0.1.3    knitr_1.45       \n[13] htmlwidgets_1.6.2 tibble_3.2.1      munsell_0.5.0     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.3       utf8_1.2.4        stringi_1.7.12   \n[21] xfun_0.39         cli_3.6.2         withr_2.5.2       digest_0.6.33    \n[25] grid_4.3.0        hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_0.22     glue_1.7.0        fansi_1.0.6       colorspace_2.1-0 \n[33] rmarkdown_2.25    tools_4.3.0       pkgconfig_2.0.3   htmltools_0.5.5  \n\n\nYou can drill down into the details of the information, for example,\n\nsinfo$loadedOnly$rmarkdown\n\nType: Package\nPackage: rmarkdown\nTitle: Dynamic Documents for R\nVersion: 2.25\nAuthors@R: c( person(\"JJ\", \"Allaire\", , \"jj@posit.co\", role = \"aut\"),\n        person(\"Yihui\", \"Xie\", , \"xie@yihui.name\", role = c(\"aut\",\n        \"cre\"), comment = c(ORCID = \"0000-0003-0645-5666\")),\n        person(\"Christophe\", \"Dervieux\", , \"cderv@posit.co\", role =\n        \"aut\", comment = c(ORCID = \"0000-0003-4474-2498\")),\n        person(\"Jonathan\", \"McPherson\", , \"jonathan@posit.co\", role =\n        \"aut\"), person(\"Javier\", \"Luraschi\", role = \"aut\"),\n        person(\"Kevin\", \"Ushey\", , \"kevin@posit.co\", role = \"aut\"),\n        person(\"Aron\", \"Atkins\", , \"aron@posit.co\", role = \"aut\"),\n        person(\"Hadley\", \"Wickham\", , \"hadley@posit.co\", role = \"aut\"),\n        person(\"Joe\", \"Cheng\", , \"joe@posit.co\", role = \"aut\"),\n        person(\"Winston\", \"Chang\", , \"winston@posit.co\", role = \"aut\"),\n        person(\"Richard\", \"Iannone\", , \"rich@posit.co\", role = \"aut\",\n        comment = c(ORCID = \"0000-0003-3925-190X\")), person(\"Andrew\",\n        \"Dunning\", role = \"ctb\", comment = c(ORCID =\n        \"0000-0003-0464-5036\")), person(\"Atsushi\", \"Yasumoto\", role =\n        c(\"ctb\", \"cph\"), comment = c(ORCID = \"0000-0002-8335-495X\", cph\n        = \"Number sections Lua filter\")), person(\"Barret\", \"Schloerke\",\n        role = \"ctb\"), person(\"Carson\", \"Sievert\", role = \"ctb\",\n        comment = c(ORCID = \"0000-0002-4958-2844\")), person(\"Devon\",\n        \"Ryan\", , \"dpryan79@gmail.com\", role = \"ctb\", comment = c(ORCID\n        = \"0000-0002-8549-0971\")), person(\"Frederik\", \"Aust\", ,\n        \"frederik.aust@uni-koeln.de\", role = \"ctb\", comment = c(ORCID =\n        \"0000-0003-4900-788X\")), person(\"Jeff\", \"Allen\", ,\n        \"jeff@posit.co\", role = \"ctb\"), person(\"JooYoung\", \"Seo\", role\n        = \"ctb\", comment = c(ORCID = \"0000-0002-4064-6012\")),\n        person(\"Malcolm\", \"Barrett\", role = \"ctb\"), person(\"Rob\",\n        \"Hyndman\", , \"Rob.Hyndman@monash.edu\", role = \"ctb\"),\n        person(\"Romain\", \"Lesur\", role = \"ctb\"), person(\"Roy\",\n        \"Storey\", role = \"ctb\"), person(\"Ruben\", \"Arslan\", ,\n        \"ruben.arslan@uni-goettingen.de\", role = \"ctb\"),\n        person(\"Sergio\", \"Oller\", role = \"ctb\"), person(given = \"Posit\n        Software, PBC\", role = c(\"cph\", \"fnd\")), person(, \"jQuery UI\n        contributors\", role = c(\"ctb\", \"cph\"), comment = \"jQuery UI\n        library; authors listed in inst/rmd/h/jqueryui/AUTHORS.txt\"),\n        person(\"Mark\", \"Otto\", role = \"ctb\", comment = \"Bootstrap\n        library\"), person(\"Jacob\", \"Thornton\", role = \"ctb\", comment =\n        \"Bootstrap library\"), person(, \"Bootstrap contributors\", role =\n        \"ctb\", comment = \"Bootstrap library\"), person(, \"Twitter, Inc\",\n        role = \"cph\", comment = \"Bootstrap library\"),\n        person(\"Alexander\", \"Farkas\", role = c(\"ctb\", \"cph\"), comment =\n        \"html5shiv library\"), person(\"Scott\", \"Jehl\", role = c(\"ctb\",\n        \"cph\"), comment = \"Respond.js library\"), person(\"Ivan\",\n        \"Sagalaev\", role = c(\"ctb\", \"cph\"), comment = \"highlight.js\n        library\"), person(\"Greg\", \"Franko\", role = c(\"ctb\", \"cph\"),\n        comment = \"tocify library\"), person(\"John\", \"MacFarlane\", role\n        = c(\"ctb\", \"cph\"), comment = \"Pandoc templates\"), person(,\n        \"Google, Inc.\", role = c(\"ctb\", \"cph\"), comment = \"ioslides\n        library\"), person(\"Dave\", \"Raggett\", role = \"ctb\", comment =\n        \"slidy library\"), person(, \"W3C\", role = \"cph\", comment =\n        \"slidy library\"), person(\"Dave\", \"Gandy\", role = c(\"ctb\",\n        \"cph\"), comment = \"Font-Awesome\"), person(\"Ben\", \"Sperry\", role\n        = \"ctb\", comment = \"Ionicons\"), person(, \"Drifty\", role =\n        \"cph\", comment = \"Ionicons\"), person(\"Aidan\", \"Lister\", role =\n        c(\"ctb\", \"cph\"), comment = \"jQuery StickyTabs\"), person(\"Benct\n        Philip\", \"Jonsson\", role = c(\"ctb\", \"cph\"), comment =\n        \"pagebreak Lua filter\"), person(\"Albert\", \"Krewinkel\", role =\n        c(\"ctb\", \"cph\"), comment = \"pagebreak Lua filter\") )\nMaintainer: Yihui Xie &lt;xie@yihui.name&gt;\nDescription: Convert R Markdown documents into a variety of formats.\nLicense: GPL-3\nURL: https://github.com/rstudio/rmarkdown,\n        https://pkgs.rstudio.com/rmarkdown/\nBugReports: https://github.com/rstudio/rmarkdown/issues\nDepends: R (&gt;= 3.0)\nImports: bslib (&gt;= 0.2.5.1), evaluate (&gt;= 0.13), fontawesome (&gt;=\n        0.5.0), htmltools (&gt;= 0.5.1), jquerylib, jsonlite, knitr (&gt;=\n        1.22), methods, stringr (&gt;= 1.2.0), tinytex (&gt;= 0.31), tools,\n        utils, xfun (&gt;= 0.36), yaml (&gt;= 2.1.19)\nSuggests: digest, dygraphs, fs, rsconnect, downlit (&gt;= 0.4.0), katex\n        (&gt;= 1.4.0), sass (&gt;= 0.4.0), shiny (&gt;= 1.6.0), testthat (&gt;=\n        3.0.3), tibble, vctrs, cleanrmd, withr (&gt;= 2.4.2)\nVignetteBuilder: knitr\nConfig/Needs/website: rstudio/quillt, pkgdown\nEncoding: UTF-8\nRoxygenNote: 7.2.3\nSystemRequirements: pandoc (&gt;= 1.14) - http://pandoc.org\nNeedsCompilation: no\nPackaged: 2023-09-15 16:52:22 UTC; yihui\nAuthor: JJ Allaire [aut], Yihui Xie [aut, cre]\n        (&lt;https://orcid.org/0000-0003-0645-5666&gt;), Christophe Dervieux\n        [aut] (&lt;https://orcid.org/0000-0003-4474-2498&gt;), Jonathan\n        McPherson [aut], Javier Luraschi [aut], Kevin Ushey [aut], Aron\n        Atkins [aut], Hadley Wickham [aut], Joe Cheng [aut], Winston\n        Chang [aut], Richard Iannone [aut]\n        (&lt;https://orcid.org/0000-0003-3925-190X&gt;), Andrew Dunning [ctb]\n        (&lt;https://orcid.org/0000-0003-0464-5036&gt;), Atsushi Yasumoto\n        [ctb, cph] (&lt;https://orcid.org/0000-0002-8335-495X&gt;, Number\n        sections Lua filter), Barret Schloerke [ctb], Carson Sievert\n        [ctb] (&lt;https://orcid.org/0000-0002-4958-2844&gt;), Devon Ryan\n        [ctb] (&lt;https://orcid.org/0000-0002-8549-0971&gt;), Frederik Aust\n        [ctb] (&lt;https://orcid.org/0000-0003-4900-788X&gt;), Jeff Allen\n        [ctb], JooYoung Seo [ctb]\n        (&lt;https://orcid.org/0000-0002-4064-6012&gt;), Malcolm Barrett\n        [ctb], Rob Hyndman [ctb], Romain Lesur [ctb], Roy Storey [ctb],\n        Ruben Arslan [ctb], Sergio Oller [ctb], Posit Software, PBC\n        [cph, fnd], jQuery UI contributors [ctb, cph] (jQuery UI\n        library; authors listed in inst/rmd/h/jqueryui/AUTHORS.txt),\n        Mark Otto [ctb] (Bootstrap library), Jacob Thornton [ctb]\n        (Bootstrap library), Bootstrap contributors [ctb] (Bootstrap\n        library), Twitter, Inc [cph] (Bootstrap library), Alexander\n        Farkas [ctb, cph] (html5shiv library), Scott Jehl [ctb, cph]\n        (Respond.js library), Ivan Sagalaev [ctb, cph] (highlight.js\n        library), Greg Franko [ctb, cph] (tocify library), John\n        MacFarlane [ctb, cph] (Pandoc templates), Google, Inc. [ctb,\n        cph] (ioslides library), Dave Raggett [ctb] (slidy library),\n        W3C [cph] (slidy library), Dave Gandy [ctb, cph]\n        (Font-Awesome), Ben Sperry [ctb] (Ionicons), Drifty [cph]\n        (Ionicons), Aidan Lister [ctb, cph] (jQuery StickyTabs), Benct\n        Philip Jonsson [ctb, cph] (pagebreak Lua filter), Albert\n        Krewinkel [ctb, cph] (pagebreak Lua filter)\nRepository: CRAN\nDate/Publication: 2023-09-18 09:30:02 UTC\nBuilt: R 4.3.1; ; 2023-09-18 12:03:34 UTC; unix\n\n-- File: /Users/olivers/Library/R/arm64/4.3/library/rmarkdown/Meta/package.rds \n\n\n\n\nLaTeX (\\(\\LaTeX\\))\n\\(\\LaTeX\\) (pronounced “LAY-tek” or “LAH-tek”) is a high-quality typesetting system; it includes features designed for the production of technical and scientific documents. \\(\\LaTeX\\) is the de facto standard for the communication and publication of scientific documents and is available for free from here.\nIf you are working in mathematics or statistics, you will be producing \\(\\LaTeX\\) documents. You can write equations with other authoring tools as well—even the Microsoft Equation Editor has improved greatly over the years, in part because it now accepts \\(\\LaTeX\\) syntax! \\(\\LaTeX\\) is not a WYSIWYG—what you see is what you get—environment. Instead, you write a plain text file where text is interspersed with \\(\\LaTeX\\) commands. The document is processed (“compiled”) into an output file (usually pdf) by running it through a TeX engine. In other words, you focus on writing the contents of the document with \\(\\LaTeX\\) commands and let the Tex engine take care of converting the commands into a visual appearance.\nRStudio, Rmarkdown, and Quarto support \\(\\LaTeX\\) natively and this makes it very easy to combine text, code, and formulas. For example, to show the probability density function of a G(0,1) random variable in this Quarto document, I typed the \\(\\LaTeX\\) instructions\n$$\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp \n       \\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\n$$\nin the editor. When the document is rendered, these instructions produce \\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\n\\]\nRStudio does not add a \\(\\LaTeX\\) system to your computer, so you need to do that yourself. If you are planning to use \\(\\LaTeX\\) outside of R and RStudio, I recommend installing a full distribution. If you just want to get by with the minimal \\(\\LaTeX\\) needed to add formulas to html and pdf files created from RStudio, then tinytex will suffice.\n\nMacTex: This \\(\\LaTeX\\) distribution contains everything you need for MacOS.\nMicTex: For Windows, Linux, and MacOS\nTex Live: A basic Tex distribution for Windows, Linux, and MacOS.\nTinyTex: A small \\(\\LaTeX\\) distribution based on Tex Live that works well with R. The R package tinytex provides helper functions to work with TinyTex from R/RStudio. If you want to use TinyTex in R, first install the tinytex package\n\ninstall.packages(\"tinytex\")\nand then download and install TinyTex with\ntinytex::install_tinytex()\nBy default, install_tinytex() will fail the install if another \\(\\LaTeX\\) distribution is detected (you can overwrite this behavior with the force= argument of the function).\nYou can check if RStudio/R uses tinytex by executing this command at the prompt:\n\ntinytex::is_tinytex()\n\n[1] FALSE\n\n\nTo author pure \\(\\LaTeX\\) documents on MacOS, I use TexShop from the University of Oregon, available here. TexShop comes with a Tex Live distribution, so installing TexShop is one method of adding LaTeX to your system.\nIf you are new to \\(\\LaTeX\\), the online LaTeX editor Overleaf has excellent tutorials and documentation. For example, this \\(\\LaTeX\\) in 30-minutes tutorial.\nWhen you use \\(\\LaTeX\\) commands in an Rmarkdown or Quarto document, you do not need to start the document with a preamble (\\documentclass() …) or wrap the commands into a \\begin{document} \\end{document} block. You can enter \\(\\LaTeX\\) commands immediately. The most important application of using \\(\\LaTeX\\) with R is to add mathematical expressions to your document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting_started.html#sec-getting-started-Python",
    "href": "getting_started.html#sec-getting-started-Python",
    "title": "2  Getting Started",
    "section": "2.2 Getting Started with Python",
    "text": "2.2 Getting Started with Python\nTo get started with statistical programming in Python, you need the following:\n\nAccess to a version of Python, typically installed on your computer\nA development environment to write, interpret, and execute Python code. This is frequently some form of notebook interface, for example Jupyter Notebook or Google Colab.\nA package management system to add/update/remove Python libraries on your system.\n\nYou can download any version of Python from here. The latest version as of this writing is Python 3.12.4. Some organizations still use Python 2; because of breaking changes between Python 2 and Python 3 they might not have updated to Python 3. Moving from Python 2 code to Python 3 is time consuming. Running Python 2 these days is a serious red flag. Python 2 has been sunset since January 1, 2020, meaning that there will be no bug fixes, not even for security bugs.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt is a great question to ask a potential employer: what version of Python are you running and how do you manage your default stack of Python libraries?\nIt is very telling if they are still running Python 2 and have not upgraded to Python 3. This organization does not know how to handle technical debt—run like it is the plague.\n\n\n\n\npyenv Version Management\npyenv is a version management tool for Python. It makes it particularly easy to work with multiple Python versions on the same system. The Python ecosystem moves very quickly and you will find yourself in a situation where a particular library requires a different version of Python from the one installed. Running different Python kernels for different projects is an unfortunate reality for many Python developers. With pyenv you can install/uninstall Python versions, you can switch versions globally, per shell or locally (in certain directories), and create virtual environments.\nThe instructions to install pyenv on your system are here. Pay attention to also update shell configurations when you install pyenv. For example, my system uses zsh and my .zshrc file contains the lines (straight from the GitHub documentation)\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\nThe most common pyenv commands I use are\n\npyenv install to install a Python version on the system. For example, pyenv install 3.11.4 will install Python 3.11.4.\npyenv version to see the currently active version of Python\npyenv local ... to set a local (application-specific) version of Python, for example pyenv local 3.9 makes Python 3.9 the version in the applications started from the current (local) directory. Similarly, pyenv shell ... sets the Python version for the shell instance and pyenv global ... sets the Python version globally. You see that the global version of Python can be different from the version active in a particular shell or a directory.\npyenv --help to get help for the pyenv commands\npyenv help commnand_name to get help for a specific pyenv command, for example pyenv help local\n\n\n\nPackage Management\nThe most common management tools used with Python are conda and pip. The two are often seen as equivalent, but they serve different purposes. pip is a Python package manager, you use it to add/update/remove packages from your Python installation. conda is a system package manager that handles much more than Python libraries. You can manage entire development stacks with conda, but not with pip.\nFor example, to add jupyter to your system with conda use\nconda install jupyter\nand with pip use\npip install jupyter\nThere is a misconception that conda and pip cannot be used together on the same system. You can use them together, in fact a great way to manage your environment is to first install and set up conda for your project and to install the packages you need from conda channels. With conda activated, you can use the version of pip that is included with conda to install any required pip dependencies. The important point is that once conda is activated, you use its version of pip.\nCheck\nwhich pip\nto see which version of pip will be called.\nI personally use pip to manage Python packages, but it is not without issues. Managing the dependencies between Python libraries is a special kind of suffering. You install a new package A that happens to have a dependency on an earlier version of package B, which it downgrades upon installation to the earlier version. This can break code that depends on the newer version of package B. Once you realize this you upgrade B to the newer version, making A fail.\n\n\n\nFigure 2.1: RStudio IDE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "reading_data.html",
    "href": "reading_data.html",
    "title": "3  Reading Data",
    "section": "",
    "text": "3.1 Tabular Data\nThe basic function to read tabular data into R is read.table. read.csv is a special case of read.table for CSV files with defaults such as sep=\",\" that make sense for CSV (comma-separated values) files.\nHere is an example of reading a CSV file into R. The stringsAsFactors=TRUE argument requests that all character variables are converted into factors.\npisa &lt;- read.csv(file=\"data/pisa.csv\",stringsAsFactors=TRUE)\npisa[1:10,]\n\n                Country MathMean MathShareLow MathShareTop ReadingMean\n1        Shanghai-China      613          3.8         55.4         570\n2             Singapore      573          8.3         40.0         542\n3  Hong Kong SAR, China      561          8.5         33.7         545\n4        Chinese Taipei      560         12.8         37.2         523\n5                 Korea      554          9.1         30.9         536\n6      Macao SAR, China      538         10.8         24.3         509\n7                 Japan      536         11.1         23.7         538\n8         Liechtenstein      535         14.1         24.8         516\n9           Switzerland      531         12.4         21.4         509\n10          Netherlands      523         14.8         19.3         511\n   ScienceMean      GDPp  logGDPp HighIncome\n1          580   6264.60  8.74267      FALSE\n2          551  54451.21 10.90506       TRUE\n3          555  36707.77 10.51074       TRUE\n4          523        NA       NA         NA\n5          538  24453.97 10.10455       TRUE\n6          521  77145.04 11.25344       TRUE\n7          547  46701.01 10.75152       TRUE\n8          525 149160.76 11.91278       TRUE\n9          515  83208.69 11.32911       TRUE\n10         522  49474.71 10.80922       TRUE\nNote that there are missing values for Chinese Taipei in row 4. This is the result of correctly specifying the contents of the comma-separated file. For this row the entries of the CSV files read\nThe sequence of commas indicates that the values for the corresponding variables are unobserved and will be set to missing.\nBy default, read.csv looks for information about the column names in the first row of the CSV file. If names are not available in the first row, add the header=FALSE option to the function call. Other important options for read.csv and read.table are",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#tabular-data",
    "href": "reading_data.html#tabular-data",
    "title": "3  Reading Data",
    "section": "",
    "text": "On CSV Files\n\n\n\nOK, time for a rant. CSV files are ubiquitous and going through college one could get the impression that most data is stored in CSV format. First, that does not hold for the real world. Second, CSV is a horrible format for data. It does have some advantages\n\nUbiquitous: every data tool can read and write CSV files. It is thus a common format to exchange (export/import) data between tools and applications.\nHuman readable: since the column names and values are stored in plain text, it is easy to look at the contents of a CSV file. When data are stored in binary form, you need to know exactly how the data are laid out in the file to access it.\nCompression: it is easy to compress CSV files.\nExcel: CSV files are easily exported from and imported to Microsoft Excel.\nSimple: the structure of the files is straightforward to understand and can represent tabular data well if the data types can be converted to text characters.\n\nThere are some considerable disadvantages of CSV files, however:\n\nHuman readable: To prevent exposing the contents of the file you need to use access controls and/or encryption. It is not a recommended file format for sensitive data.\nSimple structure: Complex data types such as documents with multiple fields and sub-fields cannot be stored in CSV files.\nPlain text: Some data types cannot be represented as plain text, for example, images, audio, and video. If you kluge binary data into a text representation the systems writing and reading the data need to know how to kluge and un-kluge the information—it is not a recommended practice.\nEfficiency: much more efficient formats for storing data exist, especially for large data sets.\nBroken: CSV files can be easily broken by applications. Examples include inserting line breaks, limiting line width, not handling embedded quotes correctly, blank lines.\nMissing values (NaNs): The writer and reader of CSV files need to agree how to represent missing values and values representing not-a-number. Inconsistency between writing and reading these values can have disastrous consequences For example, it is a bad but common practice to code missing values with special numbers such as 99999 (called “sentinel values”). How does the application reading the file know this is the code for a missing value?\nEncodings: When CSV files contain more than plain ASCII text, for example, emojis or Unicode characters, the file cannot be read without knowing the correct encoding (UTF-8, UTF-16, EBCDIC, US-ASCII, etc.). Storing encoding information in the header section of the CSV file throws off CSV reader software that does not anticipate the extra information.\nMetadata: The only metadata supported by the CSV format are the column names in the first row of the file. This information is optional and you will find CSV files without column names. Additional metadata common about columns in a table such as data types, format masks, number-to-string maps, cannot be stored in a CSV file.\nData Types: Data types need to be inferred by the CSV reader software when scanning the file. There is no metadata in the CSV header to identify data types, only column names.\nLoss of Precision: Floating point numbers are usually stored in CSV files with fewer decimal places than their internal representation in the computer. A double-precision floating point number occupies 64-bits (8 bytes) and has 15 digits of precision. Although it is not necessary, floating-point numbers are often rounded or truncated when they are converted to plain text.\n\nDespite these drawbacks, CSV is one of the most common file formats. It is the lowest common denominator format to exchange data between disparate systems.\n\n\n\n\n\n\"Chinese Taipei\",560,12.8,37.2,523,523,,,\n\n\n\nsep: specifies the character that separates the values of the columns, for read.csv this defaults to sep=\",\". For read.table the separator defaults to sep=\"\".\nskip: how many lines to skip at the top of the file before reading data. This is useful if the CSV file has a comment section at the top.\nnrow: how many rows of data to read. Useful if you want to import only a portion of the file.\ndec: specifies the character that indicates a decimal point.\nna.strings: a vector of character values that are interpreted as missing (NA) values. I hope you do not need to specify those—using sentinel values to indicate missing values is very dangerous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#excel-files",
    "href": "reading_data.html#excel-files",
    "title": "3  Reading Data",
    "section": "3.2 Excel Files",
    "text": "3.2 Excel Files\nThe readxl library is part of the tidyverse.\n\nlibrary(readxl)\ndf_tesla &lt;- read_excel(\"data/TeslaDeaths.xlsx\", sheet=1)\ndf_tesla[1:10,]\n\n# A tibble: 10 × 26\n   `Case #`  Year Date  Country State Description          Deaths `Tesla driver`\n      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;         \n 1     426   2024 45457 USA     CA    Scooter hit               1 -             \n 2     425   2024 45454 USA     CA    Tesla flips               3 -             \n 3     424   2024 45452 USA     CA    Pedestrian hit            1 -             \n 4     423.  2024 45451 USA     FL    Motorcycle hits Tes…      1 -             \n 5     423   2024 45446 USA     TX    Tesla hits parked t…      1 1             \n 6     422   2024 45444 USA     MD    Tesla hit motorcycle      1 -             \n 7     421   2024 45441 USA     WA    Tesla runs red ligh…      1 -             \n 8     420   2024 45438 USA     FL    Tesla crashes into …      1 1             \n 9     419   2024 45436 Germany -     Tesla crashes under…      1 1             \n10     418   2024 45435 UK      -     Three-way crash           1 1             \n# ℹ 18 more variables: `Tesla occupant` &lt;chr&gt;, `Other vehicle` &lt;chr&gt;,\n#   `Cyclists/ Peds` &lt;chr&gt;, `TSLA+cycl / peds` &lt;chr&gt;, Model &lt;chr&gt;,\n#   `Autopilot claimed` &lt;chr&gt;, `Reported in NHTSA SGO` &lt;chr&gt;,\n#   `Verified Tesla Autopilot Deaths` &lt;chr&gt;,\n#   `Excerpt Verifying Tesla Autopilot Deaths` &lt;chr&gt;,\n#   `Verified FSD Beta Death` &lt;lgl&gt;, ...19 &lt;chr&gt;, ...20 &lt;chr&gt;, Source &lt;chr&gt;,\n#   Note &lt;chr&gt;, `Deceased 1` &lt;chr&gt;, `Deceased 2` &lt;chr&gt;, `Deceased 3` &lt;chr&gt;, …",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#sec-json-files",
    "href": "reading_data.html#sec-json-files",
    "title": "3  Reading Data",
    "section": "3.3 JSON Files",
    "text": "3.3 JSON Files\nJSON stands for JavaScript Object Notation, and although it was borne out of interoperability concerns for JavaScript applications, it is a language-agnostic data format. Initially used to pass information in human readable form between applications over APIs (Application Programmer Interfaces), JSON has grown into a general-purpose format for text-based, structured information. It is the standard for communicating data on the web. The correct pronunciation of JSON is like the name “Jason”, but “JAY-sawn” has become common.\nIn contrast to CSV, JSON is not based on rows of data but three basic data elements:\n\nValue: a string, number, reserved word, or one of the following:\nObject: a collection of name—value pairs similar to a key-value store.\nArray: An ordered list of values\n\nAll modern programming languages support key—values and arrays, they might be calling it by different names (object, record, dictionary, struct, list, sequence, map, hash table, …). This makes JSON documents highly interchangeable between programming languages—JSON documents are easy to parse (read) and write by computers. Any modern data processing system can read and write JSON data, making it a frequent choice to share data between systems and applications.\nA value in JSON can be a string in double quotes, a number, true, false, or null, an object or an array (Figure 3.1). An array is an ordered collection of values. Objects are unordered collection of name—value pairs. Since values can contain objects and arrays, JSON allows highly nested data structures that do not fit the tabular row–column structure of CSV files.\n\n\n\n\n\n\nFigure 3.1: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\n\n\n\nJSON documents are self-describing, the schema to make the data intelligible is built into the structures. It is also a highly flexible format that does not impose any structure on the data, except that it must comply with the JSON rules and data types.\n\n\n\n\n\n\nFigure 3.2: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\n\n\n\nAs a human-readable, non-binary format, JSON shares some of the advantages and disadvantages with CSV files. You do not want to pass sensitive information in JSON format without encryption. The level of human readability is lower for JSON files. The format is intended to make algorithms interoperable, not to make human interpretation simple.\nSince so much data is stored in JSON format, you need to get familiar and comfortable with working with JSON files. Data science projects are more likely consumers of JSON files rather than producer of files.\n\nThere are multiple packages for working with JSON files in R, for example, jsonlite and rjson. Here we use the jsonlite package. The toJSON and fromJSON functions are used to convert R objects to/from JSON. read_json and write_json read and write JSON files. They are similar to fromJSON and toJSON but recognize a file path as input.\nThe following example is taken from Datacarpentry.org. SAFI (Studying African Farmer-Led Irrigation) is a study of farming and irrigation methods in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017.\nIf you use read_json with the default settings, the JSON document is converted into a list (simplifyVector=FALSE). To convert JSON format into vectors and data frames, use simplifyVector=TRUE.\n\nlibrary(jsonlite)\njson_data &lt;- read_json(\"data/SAFI.json\", simplifyVector=TRUE)\n\nTo take a look at the data frame created with read_json we use the glimpse function from tidyverse. It works a bit like the str function in base R but has a more compact display for this case and shows more data points.\n\nlibrary(tidyverse)\nglimpse(json_data)\n\nRows: 131\nColumns: 74\n$ C06_rooms                      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 1, 3, 1, …\n$ B19_grand_liv                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ A08_ward                       &lt;chr&gt; \"ward2\", \"ward2\", \"ward2\", \"ward2\", \"wa…\n$ E01_water_use                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"y…\n$ B18_sp_parents_liv             &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"…\n$ B16_years_liv                  &lt;int&gt; 4, 9, 15, 6, 40, 3, 38, 70, 6, 23, 20, …\n$ E_yes_group_count              &lt;chr&gt; NA, \"3\", NA, NA, NA, NA, \"4\", \"2\", \"3\",…\n$ F_liv                          &lt;list&gt; [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[3 …\n$ `_note2`                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ instanceID                     &lt;chr&gt; \"uuid:ec241f2c-0609-46ed-b5e8-fe575f6ce…\n$ B20_sp_grand_liv               &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"…\n$ F10_liv_owned_other            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `_note1`                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ F12_poultry                    &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"…\n$ D_plots_count                  &lt;chr&gt; \"2\", \"3\", \"1\", \"3\", \"2\", \"1\", \"4\", \"2\",…\n$ C02_respondent_wall_type_other &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ C02_respondent_wall_type       &lt;chr&gt; \"muddaub\", \"muddaub\", \"burntbricks\", \"b…\n$ C05_buildings_in_compound      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, …\n$ `_remitters`                   &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 …\n$ E18_months_no_water            &lt;list&gt; &lt;NULL&gt;, &lt;\"Aug\", \"Sept\"&gt;, &lt;NULL&gt;, &lt;NULL…\n$ F07_use_income                 &lt;chr&gt; NA, \"AlimentaÃ§Ã£o e pagamento de educa…\n$ G01_no_meals                   &lt;int&gt; 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, …\n$ E17_no_enough_water            &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"yes\", \"yes\"…\n$ F04_need_money                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ A05_end                        &lt;chr&gt; \"2017-04-02T17:29:08.000Z\", \"2017-04-02…\n$ C04_window_type                &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"n…\n$ E21_other_meth                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ D_no_plots                     &lt;int&gt; 2, 3, 1, 3, 2, 1, 4, 2, 3, 2, 2, 2, 4, …\n$ F05_money_source               &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ A07_district                   &lt;chr&gt; \"district1\", \"district1\", \"district1\", …\n$ C03_respondent_floor_type      &lt;chr&gt; \"earth\", \"earth\", \"cement\", \"earth\", \"e…\n$ E_yes_group                    &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[3 …\n$ A01_interview_date             &lt;chr&gt; \"2016-11-17\", \"2016-11-17\", \"2016-11-17…\n$ B11_remittance_money           &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no…\n$ A04_start                      &lt;chr&gt; \"2017-03-23T09:49:57.000Z\", \"2017-04-02…\n$ D_plots                        &lt;list&gt; [&lt;data.frame[2 x 8]&gt;], [&lt;data.frame[3 …\n$ F_items                        &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[2 …\n$ F_liv_count                    &lt;chr&gt; \"1\", \"3\", \"1\", \"2\", \"4\", \"1\", \"1\", \"2\",…\n$ F10_liv_owned                  &lt;list&gt; \"poultry\", &lt;\"oxen\", \"cows\", \"goats\"&gt;, …\n$ B_no_membrs                    &lt;int&gt; 3, 7, 10, 7, 7, 3, 6, 12, 8, 12, 6, 7, …\n$ F13_du_look_aftr_cows          &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no…\n$ E26_affect_conflicts           &lt;chr&gt; NA, \"once\", NA, NA, NA, NA, \"never\", \"n…\n$ F14_items_owned                &lt;list&gt; &lt;\"bicycle\", \"television\", \"solar_panel…\n$ F06_crops_contr                &lt;chr&gt; NA, \"more_half\", NA, NA, NA, NA, \"more_…\n$ B17_parents_liv                &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ G02_months_lack_food           &lt;list&gt; \"Jan\", &lt;\"Jan\", \"Sept\", \"Oct\", \"Nov\", \"…\n$ A11_years_farm                 &lt;dbl&gt; 11, 2, 40, 6, 18, 3, 20, 16, 16, 22, 6,…\n$ F09_du_labour                  &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"no\", \"yes\", …\n$ E_no_group_count               &lt;chr&gt; \"2\", NA, \"1\", \"3\", \"2\", \"1\", NA, NA, NA…\n$ E22_res_change                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ E24_resp_assoc                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, NA, \"yes\", NA…\n$ A03_quest_no                   &lt;chr&gt; \"01\", \"01\", \"03\", \"04\", \"05\", \"6\", \"7\",…\n$ `_members`                     &lt;list&gt; [&lt;data.frame[3 x 12]&gt;], [&lt;data.frame[7…\n$ A06_province                   &lt;chr&gt; \"province1\", \"province1\", \"province1\", …\n$ `gps:Accuracy`                 &lt;dbl&gt; 14, 19, 13, 5, 10, 12, 11, 9, 11, 14, 1…\n$ E20_exper_other                &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"yes\", \"yes\"…\n$ A09_village                    &lt;chr&gt; \"village2\", \"village2\", \"village2\", \"vi…\n$ C01_respondent_roof_type       &lt;chr&gt; \"grass\", \"grass\", \"mabatisloping\", \"mab…\n$ `gps:Altitude`                 &lt;dbl&gt; 698, 690, 674, 679, 689, 692, 709, 700,…\n$ `gps:Longitude`                &lt;dbl&gt; 33.48346, 33.48342, 33.48345, 33.48342,…\n$ E23_memb_assoc                 &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, \"no\", \"yes\",…\n$ E19_period_use                 &lt;dbl&gt; NA, 2, NA, NA, NA, NA, 10, 10, 6, 22, N…\n$ E25_fees_water                 &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, \"no\", \"no\", \"…\n$ C07_other_buildings            &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"ye…\n$ observation                    &lt;chr&gt; \"None\", \"Estes primeiros inquÃ©ritos na…\n$ `_note`                        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ A12_agr_assoc                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ G03_no_food_mitigation         &lt;list&gt; &lt;\"na\", \"rely_less_food\", \"reduce_meals…\n$ F05_money_source_other         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `gps:Latitude`                 &lt;dbl&gt; -19.11226, -19.11248, -19.11211, -19.11…\n$ E_no_group                     &lt;list&gt; [&lt;data.frame[2 x 6]&gt;], [&lt;data.frame[0 …\n$ F14_items_owned_other          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ F08_emply_lab                  &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ `_members_count`               &lt;chr&gt; \"3\", \"7\", \"10\", \"7\", \"7\", \"3\", \"6\", \"12…\n\n\nBecause of the deeply nested structure of JSON documents, flattening the data into a two-dimensional data frame can go only so far. Several of the columns are lists and some are lists of data frames.\n\njson_data %&gt;%\n    select(where(is.list)) %&gt;%\n    glimpse()\n\nRows: 131\nColumns: 14\n$ F_liv                  &lt;list&gt; [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[3 x 2]&gt;], …\n$ `_remitters`           &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[0 x 0]&gt;], …\n$ E18_months_no_water    &lt;list&gt; &lt;NULL&gt;, &lt;\"Aug\", \"Sept\"&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL…\n$ F05_money_source       &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ E_yes_group            &lt;list&gt; [&lt;data.frame[0 x 0]&gt;], [&lt;data.frame[3 x 14]&gt;],…\n$ D_plots                &lt;list&gt; [&lt;data.frame[2 x 8]&gt;], [&lt;data.frame[3 x 8]&gt;], …\n$ F_items                &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[2 x 3]&gt;], …\n$ F10_liv_owned          &lt;list&gt; \"poultry\", &lt;\"oxen\", \"cows\", \"goats\"&gt;, \"none\", …\n$ F14_items_owned        &lt;list&gt; &lt;\"bicycle\", \"television\", \"solar_panel\", \"tabl…\n$ G02_months_lack_food   &lt;list&gt; \"Jan\", &lt;\"Jan\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"&gt;, &lt;…\n$ E22_res_change         &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ `_members`             &lt;list&gt; [&lt;data.frame[3 x 12]&gt;], [&lt;data.frame[7 x 12]&gt;]…\n$ G03_no_food_mitigation &lt;list&gt; &lt;\"na\", \"rely_less_food\", \"reduce_meals\", \"day_…\n$ E_no_group             &lt;list&gt; [&lt;data.frame[2 x 6]&gt;], [&lt;data.frame[0 x 0]&gt;], …\n\n\nYou access the data frames stored within the lists simply as any other list element in R.\n\nstr(json_data$F_liv[[2]])\n\n'data.frame':   3 obs. of  2 variables:\n $ F11_no_owned: int  4 3 4\n $ F_curr_liv  : chr  \"oxen\" \"cows\" \"goats\"\n\njson_data$F_liv[[2]]\n\n  F11_no_owned F_curr_liv\n1            4       oxen\n2            3       cows\n3            4      goats",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#parquet-files",
    "href": "reading_data.html#parquet-files",
    "title": "3  Reading Data",
    "section": "3.4 Parquet Files",
    "text": "3.4 Parquet Files\nThe Apache Parquet open-source file format is a binary format—data are not stored in plain text but in binary form. Originally conceived as a column-based file format in the Hadoop ecosystem, it has become popular as a general file format for analytical data inside and outside of Hadoop and its file system HDFS: for example, as an efficient analytic file format for data exported to data lakes or in data processing with Spark. Many organizations have switched to storing their data in Parquet files; loading Parquet files from AWS S3 buckets or from Google Cloud Storage or Microsoft Azure Blob storage has become a common access pattern.\nWorking with Parquet files for large data is an order of magnitude faster than working with CSV files. The drawbacks of CSV files discussed previously all melt away with Parquet files.\nParquet was designed from the ground up with complex data structures and read-heavy analytics in mind. It uses principally columnar storage but does it cleverly by storing chunks of columns in row groups rather than entire columns.\n\n\n\n\n\n\nFigure 3.3: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\n\n\n\nThis hybrid storage model is very efficient when queries select specific columns and filter rows at the same time; a common pattern in data science: compute the correlation between homeValue and NumberOfRooms for homes where ZipCode = 24060.\nParquet stores metadata about the row chunks to speed access to rows, the metadata tells the reader which row chunks to skip. Also, a single write to the Parquet format can generate multiple .parquet files. The total data is divided into multiple files collected within a folder. Like NoSQL and NewSQL databases, data are partitioned, but since Parquet is a file format and not a database engine, the partitioning results in multiple files. This is advantageous for parallel processing frameworks like Spark that can work on multiple partitions (files) concurrently.\nParquet uses several compression techniques to reduce the size of the files such as run-length encoding, dictionary encoding, Snappy, GZip, LZO, LZ4, ZSTD. Because of columnar storage, compression methods can be specified on a per-column basis; Parquet files compress much more than text-oriented CSV files.\nBecause of its complex file structure, Parquet files are relatively slow to write. The file format is optimized for the WORM paradigm: write-once, read many times.\n\nComparison of popular file formats in data science.\n\n\n\n\n\n\n\n\n\nCSV\nJSON\nParquet\n\n\n\n\nColumnar\nNo\nNo\nYes\n\n\nCompression\nYes\nYes\nYes\n\n\nHuman Readable\nYes\nYes\nNo\n\n\nNestable\nNo\nYes\nYes\n\n\nComplex Data Structures\nNo\nYes\nYes\n\n\nNamed Columns\nYes, if in header\nBased on scan\nYes, metadata\n\n\nData Types\nBased on scan\nBased on scan\nYes, metadata\n\n\n\nTo read a file in Parquet format into an R session, install and load the arrow package. Apache Arrow is an open-source development platform for in-memory analytics that supports many programming environments, including R and Python. The arrow libraries support reading and writing of the important file formats in this ecosystem.\n\n#install.packages(\"arrow\")\nlibrary(arrow)\nread_parquet(\"somefile.parquet\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#working-with-a-database",
    "href": "reading_data.html#working-with-a-database",
    "title": "3  Reading Data",
    "section": "3.5 Working with a Database",
    "text": "3.5 Working with a Database\nYou interface with a database from R with the DBI package. It provides a common syntax and functions to connect to supported databases, and to send queries to the database. In addition to the DBI package you need the driver packages specific to the database you are working with (RMySQL for MySQL, RSQLite for SQLite, and so on).\nMy favorite database for analytic work is duckDB, a highly efficient, embedded database designed for processing data analytically. The file ads.ddb, contains the database tables used in this and other courses in duckDB format. To add the duckDB driver package to your system, use\n\ninstall.packages(\"duckdb\")\n\nTo read a table from that database into an R data frame, follow the steps in the next code snippet:\n\n1library(\"duckdb\")\n2con &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\n3fit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n4dbDisconnect(con)\n\n\n1\n\nLoad the duckdb library\n\n2\n\nMake a connection to duckDB through the DBI interface, specifying the database you want to work with, here ads.ddb\n\n3\n\nSend a SELECT * FROM query to read the contents of the target table and assign the result to a dataframe in R\n\n4\n\nClose the connection to the database when you are done querying it.\n\n\n\n\nWe can now work with the fitness data set as a dataframe in R:\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\nsummary(fit)\n\n      Age            Weight          Oxygen         RunTime     \n Min.   :38.00   Min.   :59.08   Min.   :37.39   Min.   : 8.17  \n 1st Qu.:44.00   1st Qu.:73.20   1st Qu.:44.96   1st Qu.: 9.78  \n Median :48.00   Median :77.45   Median :46.77   Median :10.47  \n Mean   :47.68   Mean   :77.44   Mean   :47.38   Mean   :10.59  \n 3rd Qu.:51.00   3rd Qu.:82.33   3rd Qu.:50.13   3rd Qu.:11.27  \n Max.   :57.00   Max.   :91.63   Max.   :60.05   Max.   :14.03  \n   RestPulse        RunPulse        MaxPulse    \n Min.   :40.00   Min.   :146.0   Min.   :155.0  \n 1st Qu.:48.00   1st Qu.:163.0   1st Qu.:168.0  \n Median :52.00   Median :170.0   Median :172.0  \n Mean   :53.45   Mean   :169.6   Mean   :173.8  \n 3rd Qu.:58.50   3rd Qu.:176.0   3rd Qu.:180.0  \n Max.   :70.00   Max.   :186.0   Max.   :192.0  \n\n\nThe connection to the database can be left open until you are done working with the database. If you use the database to import tables at the beginning of a statistical program, it is recommended to close the connection as soon as that step is complete.\nBecause we are reading many tables from the ads.ddb database, you can write a function to wrap the database operations. The following function loads the duckdb library unless it is already loaded, and reads a table, possibly selecting rows with a WHERE filter, and returns the R dataframe.\n\nduckload &lt;- function(tableName, whereClause=NULL, dbName=\"ads.ddb\") {\n    if (!is.null(tableName)) {\n        if (!(\"duckdb\" %in% (.packages()))) {\n            suppressWarnings(libary(\"duckdb\"))\n        }\n        con &lt;- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)\n        query_string &lt;- paste(\"SELECT * from \", tableName)\n        if (!is.null(whereClause)) {\n            query_string &lt;- paste(query_string, \" WHERE \", whereClause)\n        }\n        df_ &lt;- dbGetQuery(con, query_string)\n        dbDisconnect(con)\n        return (df_)\n    } else {\n        return (NULL)\n    }\n}\n\n\n# Load the entire table\nfit &lt;- duckload(\"fitness\")\n\n# Load only the records where Age &gt; 50\nfit2 &lt;- duckload(\"fitness\",whereClause=\"Age &gt; 50\")\n\n# Load only the records where Age = 40\nfit3 &lt;- duckload(\"fitness\",\"Age = 40\")\n\nYou can modify the function to load a projection of columns from the database table by replacing the * in the SELECT query with the selected columns.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "reading_data.html#saving-and-loading-rdata",
    "href": "reading_data.html#saving-and-loading-rdata",
    "title": "3  Reading Data",
    "section": "3.6 Saving and Loading RData",
    "text": "3.6 Saving and Loading RData\nR has its own format (binary or ASCII) to read and write R objects. This is convenient to save a data frame to disk and later load it back into an R session.\nTo save any R object, use the save or saveRDS function, to load it into an R session use the load function. save can save one or more objects to a .RData file, while saveRDS creates an .RDS file from a single object.\n\na &lt;- matrix(rnorm(200),nrow=50,ncol=4)\nb &lt;- crossprod(a)\ncor_mat &lt;- cor(a)\nsave(a,b,cor_mat,file=\"data/matrixStuff.RData\")\n\n\nload(\"data/matrixStuff.RData\")\n\nWhen you load R objects from a .RData file, you do not assign the result of the function. The objects are created in the work environment with their original names.\n\n\n\n\n\n\nCaution\n\n\n\nThe load operation will overwrite any R objects in your environment by the same name as those in the RData file.\n\n\n.RData files can contain many objects, for example, all the objects in your environment when it is saved upon closing an R session. If you load from an .RDS file, you can assign the result to an R object which helps avoid name collisions.\nIn general, it is better coding practice to use saveRDS and readRDS with single objects. The code is cleaner and easier to follow if single objects are saved/loaded and these are explicitly named in the code.\na = rnorm(100)\nsaveRDS(a, file = \"stuff.RDS\") \n\nnormal_rvs &lt;- readRDS(\"stuff.RDS\")\n\n\n\nFigure 3.1: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\nFigure 3.2: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\nFigure 3.3: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html",
    "href": "wrangling_data.html",
    "title": "4  Wrangling Data",
    "section": "",
    "text": "4.1 Introduction\nWrangling data refers to the steps to organize the data in a structured format that is suitable for analytic processing. Typically, the result of data wrangling is data in a row-column layout where each analysis variable is in its own column and each observation is in its own row. Data often do not start out that way.\nWe saw a bit of data wrangling in the previous chapter (@#sec-json-files) when a nested JSON structure was flattened into a row-column format.\nIn addition to structuring the data, wrangling includes the following steps:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#introduction",
    "href": "wrangling_data.html#introduction",
    "title": "4  Wrangling Data",
    "section": "",
    "text": "Cleaning involves removing or correcting inaccurate data, handling duplicates, and addressing anomalies that could impact the reliability of analyses. The focus is on enhancing data accuracy and reliability for analytic processing.\nEnriching involves creating additional variables, incorporating external data, and combining the data with other data sources. Any new data sources you bring needs to be structured and cleaned as well.\nValidation checks for inconsistencies and verifies data integrity. Your organization will have standards, for example how regions, customer information, names, ages, etc. are represented in data sets, and this step ensures that the standards are met.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#tidyverse",
    "href": "wrangling_data.html#tidyverse",
    "title": "4  Wrangling Data",
    "section": "4.2 tidyverse",
    "text": "4.2 tidyverse\nIn this chapter we concentrate on structuring, enriching, and combining data with the libraries in the tidyverse. This is an “opinionated” collection of R packages for data science, including dplyr, ggplot2, tibble, tidyr, purr, stringr, and readr. dplyr, tidyr, and ggplot2 are arguably the most important packages in the collection (that is my “opinionated” view). For data wrangling we rely on dplyr and tidyr.\nYou can install the packages individually or grab all the tidyverse packages with\n\ninstall.packages(\"tidyverse\")\n\nA great cheat sheet for wrangling data in R with dplyr and tidyr can be found here. This chapter draws on the contents of this cheat sheet.\n\nPiping\nThe tidyverse packages share a common philosophy; that makes it easy to use code across multiple packages and to combine them. An example of this common philosophy is piping. The following pipeline starts with the iris data frame. The data is piped to the filter function with the pipe operator %&gt;%. The result of the filter operation is piped to the summarize function to compute the average petal width of the plants with sepal length greater than 6:\n\nlibrary(tidyverse)\niris %&gt;% \n    filter(Sepal.Length &gt; 6) %&gt;%\n    summarize(average=mean(Petal.Width))\n\n   average\n1 1.847541\n\n\nBy default the argument on the left side of the pipe operator is passed as the first argument to the function on the right side. The filter function call is really dplyr::filter(iris,Species==\"virginica). You can also pass the argument on the left of the pipe to a different argument on the right side of the pipe by using a dot:\nx %&gt;% f(y) is the same as f(x,y)\nx %&gt;% f(y, ., z) is the same as f(y,x,z)\nPiping not only shows analytic operations as a sequence of steps, but also reduces the amount of code and makes it generally more readable. Learning to write good pipelines.\nData wrangling with dplyr and tidyr can be organized into the following steps:\n\nShaping the data\nSubsetting observations and/or variables\nCreating new variables\nCombining data sets\nSummarization\n\nSummarizing data is discussed as a separate step in Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#shaping-data-into-tabular-form",
    "href": "wrangling_data.html#shaping-data-into-tabular-form",
    "title": "4  Wrangling Data",
    "section": "4.3 Shaping Data into Tabular Form",
    "text": "4.3 Shaping Data into Tabular Form\nThe goal of shaping the data is to change its structure into a tabular row-column form where each variable is in a separate column and each observation is in its own row.\nConsider the classic airline passenger time series data from Box, jenkins, and Reinsel (1976), showing monthly totals of international airline passengers between 1994 and 1960 (in thousands).\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe data are arranged very neatly, each year in a separate row, each month in a separate column. This is a great layout for time series analysis but maybe not the best layout for general analytic processing. The analysis variables are year, month, and the passenger count. Restructuring the AirPassengers data into a data frame with variables for year and count is easy because this data set is stored as an R time series.\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\n\ndat &lt;- data.frame(count=as.matrix(AirPassengers), \n                  time=time(AirPassengers),\n                  year=round(time(AirPassengers),0))\nhead(dat)\n\n  count     time year\n1   112 1949.000 1949\n2   118 1949.083 1949\n3   132 1949.167 1949\n4   129 1949.250 1949\n5   121 1949.333 1949\n6   135 1949.417 1949\n\n\n\nGapminder Data\nIn general, the structure of a non-tabular format is not known to R and must be wrangled by the user. Consider the famous Gapminder dataset. The Gapminder Foundation is a Swedish non-profit that promotes the United Nations sustainability goals through use of statistics. The Gapminder data set tracks economic and social indicators like life expectancy and the GDP per capita of countries over time.\nA wide format of the Gapminder data is available on GitHub here. We are following the material in this excellent online resource in wrangling the data in the wide format.\n\n\ngap_wide &lt;- read.csv(file=\"data/gapminder_wide.csv\")\nstr(gap_wide)\n\n'data.frame':   142 obs. of  38 variables:\n $ continent     : chr  \"Africa\" \"Africa\" \"Africa\" \"Africa\" ...\n $ country       : chr  \"Algeria\" \"Angola\" \"Benin\" \"Botswana\" ...\n $ gdpPercap_1952: num  2449 3521 1063 851 543 ...\n $ gdpPercap_1957: num  3014 3828 960 918 617 ...\n $ gdpPercap_1962: num  2551 4269 949 984 723 ...\n $ gdpPercap_1967: num  3247 5523 1036 1215 795 ...\n $ gdpPercap_1972: num  4183 5473 1086 2264 855 ...\n $ gdpPercap_1977: num  4910 3009 1029 3215 743 ...\n $ gdpPercap_1982: num  5745 2757 1278 4551 807 ...\n $ gdpPercap_1987: num  5681 2430 1226 6206 912 ...\n $ gdpPercap_1992: num  5023 2628 1191 7954 932 ...\n $ gdpPercap_1997: num  4797 2277 1233 8647 946 ...\n $ gdpPercap_2002: num  5288 2773 1373 11004 1038 ...\n $ gdpPercap_2007: num  6223 4797 1441 12570 1217 ...\n $ lifeExp_1952  : num  43.1 30 38.2 47.6 32 ...\n $ lifeExp_1957  : num  45.7 32 40.4 49.6 34.9 ...\n $ lifeExp_1962  : num  48.3 34 42.6 51.5 37.8 ...\n $ lifeExp_1967  : num  51.4 36 44.9 53.3 40.7 ...\n $ lifeExp_1972  : num  54.5 37.9 47 56 43.6 ...\n $ lifeExp_1977  : num  58 39.5 49.2 59.3 46.1 ...\n $ lifeExp_1982  : num  61.4 39.9 50.9 61.5 48.1 ...\n $ lifeExp_1987  : num  65.8 39.9 52.3 63.6 49.6 ...\n $ lifeExp_1992  : num  67.7 40.6 53.9 62.7 50.3 ...\n $ lifeExp_1997  : num  69.2 41 54.8 52.6 50.3 ...\n $ lifeExp_2002  : num  71 41 54.4 46.6 50.6 ...\n $ lifeExp_2007  : num  72.3 42.7 56.7 50.7 52.3 ...\n $ pop_1952      : num  9279525 4232095 1738315 442308 4469979 ...\n $ pop_1957      : num  10270856 4561361 1925173 474639 4713416 ...\n $ pop_1962      : num  11000948 4826015 2151895 512764 4919632 ...\n $ pop_1967      : num  12760499 5247469 2427334 553541 5127935 ...\n $ pop_1972      : num  14760787 5894858 2761407 619351 5433886 ...\n $ pop_1977      : num  17152804 6162675 3168267 781472 5889574 ...\n $ pop_1982      : num  20033753 7016384 3641603 970347 6634596 ...\n $ pop_1987      : num  23254956 7874230 4243788 1151184 7586551 ...\n $ pop_1992      : num  26298373 8735988 4981671 1342614 8878303 ...\n $ pop_1997      : num  29072015 9875024 6066080 1536536 10352843 ...\n $ pop_2002      : int  31287142 10866106 7026113 1630347 12251209 7021078 15929988 4048013 8835739 614382 ...\n $ pop_2007      : int  33333216 12420476 8078314 1639131 14326203 8390505 17696293 4369038 10238807 710960 ...\n\n\nThe data are stored in wide format, annual values for the variables GDP, life expectancy, and population appear in separate columns. This is not unlike how the AirPassengers data is displayed, but gap_wide is not a time series object. The desired (“tidy”) way of structuring the data, where each variable is a column and each observation is a row is a tabular structure with variables\n\nContinent\nCountry\nYear\nGDP\nLife Expectancy\nPopulation\n\nFor each combination of continent and country there will be 12 observations, one for each year.\n\nGather columns\nTo move from wide to the desired long format, we use the dplyr::gather function. To do the opposite, moving from long to wide format, use the dplyr::spread function. To restructure the Gapminder data set from wide format into the desired format takes several steps.\nThe first step is to gather the columns that contain the values for GDP, life expectancy, and population into separate rows. Those are all columns in gap_wide except continent and country, so we can exclude those from the gathering operation with -c(continent, country).\n\ngap_step1 &lt;- gap_wide %&gt;% \n    gather(key=vartype_year,\n          value=values,\n          -c(continent, country))\nhead(gap_step1)\n\n  continent      country   vartype_year    values\n1    Africa      Algeria gdpPercap_1952 2449.0082\n2    Africa       Angola gdpPercap_1952 3520.6103\n3    Africa        Benin gdpPercap_1952 1062.7522\n4    Africa     Botswana gdpPercap_1952  851.2411\n5    Africa Burkina Faso gdpPercap_1952  543.2552\n6    Africa      Burundi gdpPercap_1952  339.2965\n\ntail(gap_step1)\n\n     continent        country vartype_year   values\n5107    Europe         Sweden     pop_2007  9031088\n5108    Europe    Switzerland     pop_2007  7554661\n5109    Europe         Turkey     pop_2007 71158647\n5110    Europe United Kingdom     pop_2007 60776238\n5111   Oceania      Australia     pop_2007 20434176\n5112   Oceania    New Zealand     pop_2007  4115771\n\n\n\n\nSeparating character variables\nThe gather function turns all variables into key-value pairs; the key is the name of the variable. The column that contains the names of the variables after the gathering is called vartype_year. In the next step dplyr::separate is called to split the character column vartype_year into two columns, one for the variable type and one for the year. The convert=TRUE option attempts to convert the data type of the new columns from character to numeric data—this will fail for the vartype column but succeed for the year column.\n\ngap_step2 &lt;- gap_wide %&gt;% \n    gather(key =vartype_year,\n          value=values,\n          -c(continent, country)) %&gt;%\n    separate(vartype_year,\n           into   =c('vartype','year'),\n           sep    =\"_\",\n           convert=TRUE)\nstr(gap_step2)\n\n'data.frame':   5112 obs. of  5 variables:\n $ continent: chr  \"Africa\" \"Africa\" \"Africa\" \"Africa\" ...\n $ country  : chr  \"Algeria\" \"Angola\" \"Benin\" \"Botswana\" ...\n $ vartype  : chr  \"gdpPercap\" \"gdpPercap\" \"gdpPercap\" \"gdpPercap\" ...\n $ year     : int  1952 1952 1952 1952 1952 1952 1952 1952 1952 1952 ...\n $ values   : num  2449 3521 1063 851 543 ...\n\nhead(gap_step2)\n\n  continent      country   vartype year    values\n1    Africa      Algeria gdpPercap 1952 2449.0082\n2    Africa       Angola gdpPercap 1952 3520.6103\n3    Africa        Benin gdpPercap 1952 1062.7522\n4    Africa     Botswana gdpPercap 1952  851.2411\n5    Africa Burkina Faso gdpPercap 1952  543.2552\n6    Africa      Burundi gdpPercap 1952  339.2965\n\ntail(gap_step2)\n\n     continent        country vartype year   values\n5107    Europe         Sweden     pop 2007  9031088\n5108    Europe    Switzerland     pop 2007  7554661\n5109    Europe         Turkey     pop 2007 71158647\n5110    Europe United Kingdom     pop 2007 60776238\n5111   Oceania      Australia     pop 2007 20434176\n5112   Oceania    New Zealand     pop 2007  4115771\n\n\n\n\nSpreading rows into columns\nWe are almost there, but not quite. We now have columns for continent, country, and year, but the values column contains values of different types: 1,704 observations for GDP, 1704 observations for life expectancy, and 1,704 observations for population. To create separate columns from the rows we can reverse the gather operation with the spread function—it splits key-value pairs across multiple columns. The entries in the vartype column are used by the spreading operation as the names of the new columns.\n\ngapminder &lt;- \n    gap_wide %&gt;% \n    gather(key =vartype_year,\n          value=values,\n          -c(continent, country)) %&gt;%\n    separate(vartype_year,\n           into   =c('vartype','year'),\n           sep    =\"_\",\n           convert=TRUE) %&gt;%\n    spread(vartype, values)\n\nhead(gapminder)\n\n  continent country year gdpPercap lifeExp      pop\n1    Africa Algeria 1952  2449.008  43.077  9279525\n2    Africa Algeria 1957  3013.976  45.685 10270856\n3    Africa Algeria 1962  2550.817  48.303 11000948\n4    Africa Algeria 1967  3246.992  51.407 12760499\n5    Africa Algeria 1972  4182.664  54.518 14760787\n6    Africa Algeria 1977  4910.417  58.014 17152804\n\ntail(gapminder)\n\n     continent     country year gdpPercap lifeExp     pop\n1699   Oceania New Zealand 1982  17632.41  73.840 3210650\n1700   Oceania New Zealand 1987  19007.19  74.320 3317166\n1701   Oceania New Zealand 1992  18363.32  76.330 3437674\n1702   Oceania New Zealand 1997  21050.41  77.550 3676187\n1703   Oceania New Zealand 2002  23189.80  79.110 3908037\n1704   Oceania New Zealand 2007  25185.01  80.204 4115771\n\n\nIn summary, we used gather to create one key-value pair of the variable_year columns and values in step 1, separated out the year in step 2, and spread the remaining variable types back out into columns in step 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#subsetting",
    "href": "wrangling_data.html#subsetting",
    "title": "4  Wrangling Data",
    "section": "4.4 Subsetting",
    "text": "4.4 Subsetting\nA subset operation reduces the number of observations (rows) or variables (columns) of data set. While filter is the most important operation to subset rows, there are a number of other functions in dplyr that reduce the rows. On the other hand, to subset columns there is only one function, dplyr::select.\n\nSubsetting Rows\nThe following statements create a data frame of dimension 100 x 2, containing two columns of integers randomly selected from 1–10 with replacement:\n\nset.seed(76)\ndat &lt;- data.frame(\n  x = sample(10, 100, rep = TRUE),\n  y = sample(10, 100, rep = TRUE)) %&gt;% \n    glimpse()\n\nRows: 100\nColumns: 2\n$ x &lt;int&gt; 5, 1, 2, 10, 7, 5, 6, 10, 8, 6, 4, 6, 3, 1, 8, 5, 7, 6, 4, 1, 3, 10,…\n$ y &lt;int&gt; 3, 5, 9, 8, 7, 10, 4, 1, 10, 6, 9, 6, 2, 9, 3, 8, 9, 7, 4, 3, 2, 3, …\n\n\n\nfilter\nfilter extracts rows that meet a logical condition. The following statement selects the rows for which \\(x \\in (1,5)\\) and \\(y \\in (3,8)\\).\n\ndat %&gt;% filter(x %in% c(1,5) & y %in% c(3,8))\n\n  x y\n1 5 3\n2 5 8\n3 1 3\n4 5 8\n5 5 8\n6 5 3\n\n\nYou can also list the conditions that are combined with logical “and”, separated with commas:\n\ndat %&gt;% filter(x %in% c(1,5), y %in% c(3,8))\n\n  x y\n1 5 3\n2 5 8\n3 1 3\n4 5 8\n5 5 8\n6 5 3\n\n\nThe following statement extracts the rows where the value of y exceeds 3.5 times its standard deviation:\n\ndat %&gt;% filter(y &gt; 3.5*sd(y))\n\n   x  y\n1  5 10\n2  8 10\n3  7 10\n4  1 10\n5  8 10\n6  1 10\n7  9 10\n8  9 10\n9  6 10\n10 7 10\n11 4 10\n12 5 10\n\n\nAnother subsetting operation is to remove duplicate observations from a data set with distinct. When applied to a subset of the columns, distinct returns the combinations of their unique values in the data.\n\n\ndistinct\n\ndat %&gt;% distinct(x)\n\n    x\n1   5\n2   1\n3   2\n4  10\n5   7\n6   6\n7   8\n8   4\n9   3\n10  9\n\n\nIf you specify .keep_all=TRUE, all other variables in the data set are kept as well. If multiple combinations occur, the function retains the first occurence.\n\ndat %&gt;% distinct(x, .keep_all=TRUE)\n\n    x  y\n1   5  3\n2   1  5\n3   2  9\n4  10  8\n5   7  7\n6   6  4\n7   8 10\n8   4  9\n9   3  2\n10  9  3\n\n\nTo remove all duplicates in a data frame, simply call distinct on the data frame.\n\ndat %&gt;% distinct() %&gt;% summarize(count=n())\n\n  count\n1    65\n\n\nThere are 62 unique rows of data in the dat data frame.\n\n\nOther subsetting functions\nOther functions subsetting rows in dplyr are\n\ndplyr::sample_frac: randomly select a proportion of the rows\ndplyr::sample_n: randomly select a fixed number of rows\ndplyr::slice: select rows by position, for example slice(dat,4:10) extracts rows 4–10\ndplyr::slice_head: selects the first rows\ndplyr::slice_tail: selects the last rows\ndplyr:slice_min: select rows with the smallest values\ndplyr:slice_max: select rows with the largest values\n\nFor example, the next statement selects the observation with the five largest values for the Sepal.Width variable in the iris data set.\n\niris %&gt;% slice_max(Sepal.Width, n=5)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.7         4.4          1.5         0.4  setosa\n2          5.5         4.2          1.4         0.2  setosa\n3          5.2         4.1          1.5         0.1  setosa\n4          5.8         4.0          1.2         0.2  setosa\n5          5.4         3.9          1.7         0.4  setosa\n6          5.4         3.9          1.3         0.4  setosa\n\n\n\n\n\nSubsetting (selecting) Columns\n\nselect\nTo subset columns there is only one statement in dplyr, the select statement. However, it is very versatile because of its many helper functions.\nThe basic usage is to list the column names being selected:\n\ngapminder %&gt;% select(gdpPercap, lifeExp) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\nYou can also specify an exclusion with a negative selection\n\ngapminder %&gt;% select(-country, -continent, -pop) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\nor\n\ngapminder %&gt;% select(-c(country, continent, pop)) %&gt;%\n    summarize(mnGDP=mean(gdpPercap), sdLife=sd(lifeExp))\n\n     mnGDP   sdLife\n1 7215.327 12.91711\n\n\n\n\nHelper functions\nHere are important helper functions for dplyr::select\n\nselect( contains(\".\")): select columns whose name contains a character string\nselect( ends_with(\"Length\")): select columns whose name ends in the specified string\nselect( starts_with(\"Sepal\")): select columns whose name starts with the specified string\nselect( everything()): select all columns\nselect( matches(\".t.\")): select the columns whose name matches a regular expression\nselect( num_range(\"x\",1:5)): select the columns named x1, x2, …, x5\nselect( one_off(\"Species\",\"Genus\")): select columns whose names are in the specified group of names\nselect( Sepal.Length:Petal.Width): Select all columns between Sepal.Length and Petal.Width, including those columns",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#creating-new-variables",
    "href": "wrangling_data.html#creating-new-variables",
    "title": "4  Wrangling Data",
    "section": "4.5 Creating New Variables",
    "text": "4.5 Creating New Variables\nThe principal function to create new variables in a data frame is dplyr::mutate. Variations are mutate_each which applies a function to every column and transmute which drops the original columns.\nThe following statements compute the GPD as the product of per-capita GDP and population size and finds the top-5 countries by GDP in Asia in 2007:\n\ngapminder %&gt;%\n    filter(continent == \"Asia\", year == 2007) %&gt;%\n    mutate(GDP = gdpPercap * pop) %&gt;%\n    select(country, year, gdpPercap, GDP) %&gt;%\n    slice_max(GDP,n=5)\n\n     country year gdpPercap          GDP\n1      China 2007  4959.115 6.539501e+12\n2      Japan 2007 31656.068 4.035135e+12\n3      India 2007  2452.210 2.722925e+12\n4 Korea Rep. 2007 23348.140 1.145105e+12\n5       Iran 2007 11605.714 8.060583e+11\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe variable GDP created in this operation is transient. It is instantiated for the duration of the pipeline and is not added to the gapminder data frame. If you want to add a new variable to an existing data frame you need to assign the result to a return object.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "wrangling_data.html#combining-data-sets",
    "href": "wrangling_data.html#combining-data-sets",
    "title": "4  Wrangling Data",
    "section": "4.6 Combining Data Sets",
    "text": "4.6 Combining Data Sets\nThe process of combining tables is based on joins, set operations, or bindings. A join uses the values in specific columns of one data set to match records with another data set. A set operation is a merging of columns without considering the values in the columns. Appending rows rows of one table to another table or columns of one table to another table are binding operations. What happens to columns that exist in one data set but not in the other during the append depends on the implementation. Similarly, what happens to columns that share the same name when tables are merged horizontally depends on the implementation.\nTo show the various join and set operations, let’s create three small data frames of cities and weather information.\n\ncapitals1 &lt;- data.frame(city=c('Amsterdam','Berlin'),\n                       country=c('NL','Germany'))\n\ncapitals2 &lt;- data.frame(city=c('Amsterdam','Washington, D.C.'),\n                       country=c('NL','U.S.A.'))\n\nweather1 &lt;- data.frame(city=c('Amsterdam','Seattle'),\n                      degrees=c(10,8),\n                      date=c(\"2022-10-14\",\"2022-10-12\"))\n\ncapitals1\n\n       city country\n1 Amsterdam      NL\n2    Berlin Germany\n\ncapitals2 \n\n              city country\n1        Amsterdam      NL\n2 Washington, D.C.  U.S.A.\n\nweather1\n\n       city degrees       date\n1 Amsterdam      10 2022-10-14\n2   Seattle       8 2022-10-12\n\n\n\nSet Operations\nWe distinguish three set operations: the intersection of rows that appear in two data sets, the union of the rows, and the set difference of the rows. dplyr supports set operations with the following functions:\n\nintersect(x, y): finds all rows in both x and y.\nunion(x, y): finds all rows in either x or y, excluding duplicates.\nunion_all(x, y): finds all rows in either x or y, including duplicates.\nsetdiff(x, y): finds all rows in x that are not in y.\nsymdiff(x, y): computes the symmetric difference, i.e. all rows in x that are not in y and all rows in y that are not in x.\nsetequal(x, y): returns TRUE if x and y contain the same rows (ignoring order).\n\nNote that except for union_all the functions that return rows remove duplicates in x and y.\nFor the two data frames of capitals, here are the results of the various set operations.\n\ndplyr::intersect(capitals1, capitals2)\n\n       city country\n1 Amsterdam      NL\n\ndplyr::setdiff(capitals1, capitals2)\n\n    city country\n1 Berlin Germany\n\n\n\ndplyr::union(capitals1, capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3 Washington, D.C.  U.S.A.\n\ndplyr::union_all(capitals1, capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3        Amsterdam      NL\n4 Washington, D.C.  U.S.A.\n\n\n\ndplyr::symdiff(capitals1, capitals2)\n\n              city country\n1           Berlin Germany\n2 Washington, D.C.  U.S.A.\n\n\n\n\nJoins\nSet operations combine or reduce rows of data (vertically). Join operations combine data sets horizontally. Joins typically are based on the values in columns of the data sets to find matches.\nJoins are categorized as outer or inner joins depending on whether rows with matches are returned. An outer join returns rows that do not have any matches whereas the inner join returns only rows that get paired. The two data frames in a join are called the left and right sides of the relation and outer joins are further classified as\n\nLeft outer join: all rows from the left side of the relation appear at least once.\nRight outer join: all rows from the right side of the relation appear at least once.\nFull outer join: all rows from both sides of the relation appear at least once.\n\nTo demonstrate the joins in dplyr let’s set up some simple tables:\n\nweather &lt;- data.frame(name=c('San Francisco','San Francisco','Hayward'),\n                      temp_lo=c(46,43,37),\n                      temp_hi=c(50,57,54),\n                      prcp=c(0.25,0.0,NA),\n                      date=c('1994-11-27','1994-11-29','1994-11-29'))\n\ncities &lt;- data.frame(name=c('San Francisco'),\n                     lat=c(-194.0),\n                     lon=c(53.0))\n\n\nweather\n\n           name temp_lo temp_hi prcp       date\n1 San Francisco      46      50 0.25 1994-11-27\n2 San Francisco      43      57 0.00 1994-11-29\n3       Hayward      37      54   NA 1994-11-29\n\ncities\n\n           name  lat lon\n1 San Francisco -194  53\n\n\nAn inner join between the data frames on the columns that contain the city names will match the records for San Francisco:\n\ndplyr::inner_join(weather,cities,by=\"name\")\n\n           name temp_lo temp_hi prcp       date  lat lon\n1 San Francisco      46      50 0.25 1994-11-27 -194  53\n2 San Francisco      43      57 0.00 1994-11-29 -194  53\n\n\nNote that the values for lat and lon are repeated for every row in the weather table that matches the join in the relation. Because this is an inner join and the weather table had no matching row for city Hayward, this city does not appear in the join result. We can change that by modifying the type of join to a left outer join:\n\ndplyr::left_join(weather,cities,by=\"name\")\n\n           name temp_lo temp_hi prcp       date  lat lon\n1 San Francisco      46      50 0.25 1994-11-27 -194  53\n2 San Francisco      43      57 0.00 1994-11-29 -194  53\n3       Hayward      37      54   NA 1994-11-29   NA  NA\n\n\nBecause the join is an outer join, rows that do not have matches in the relation are returned. Because the outer join is a left join, every row on the left side of the relation is returned (at least once). If you change the left- and right-hand side of the relation you can achieve the same result by using a right outer join:\n\ndplyr::right_join(cities,weather,by=\"name\")\n\n           name  lat lon temp_lo temp_hi prcp       date\n1 San Francisco -194  53      46      50 0.25 1994-11-27\n2 San Francisco -194  53      43      57 0.00 1994-11-29\n3       Hayward   NA  NA      37      54   NA 1994-11-29\n\n\nThe left join retains all observations in the left data frame (first argument). The right join retains all observations in the right data frame (second argument).\nNow let’s add another record to the cities data frame without a matching record in the weather data frame:\n\ncities &lt;- dplyr::bind_rows(cities,data.frame(name=\"New York\",\n                                             lat=40.7,\n                                             lon=-73.9))\ncities \n\n           name    lat   lon\n1 San Francisco -194.0  53.0\n2      New York   40.7 -73.9\n\n\nA full outer join between the cities and weather data frames ensures that rows from both sides of the relation appear at least once:\n\ndplyr::full_join(cities,weather,by=\"name\")\n\n           name    lat   lon temp_lo temp_hi prcp       date\n1 San Francisco -194.0  53.0      46      50 0.25 1994-11-27\n2 San Francisco -194.0  53.0      43      57 0.00 1994-11-29\n3      New York   40.7 -73.9      NA      NA   NA       &lt;NA&gt;\n4       Hayward     NA    NA      37      54   NA 1994-11-29\n\n\n\n\nBindings\nFor data scientists working with rectangular data frames in which observations have a natural order, merging data horizontally is a standard operation. Observations are matched by position and not according to the values in key columns. In the world of relational databases, such a merge is called a positional join and a somewhat unnatural operation because relational tables do not work from a natural ordering of the data, they are based on keys and indices.\nWhen working with statistical data sets merging by position is not uncommon as data sets do not have keys. The positional join—or column binding—matches data frames row-by-row such that rows from both tables appear at least once:\n\ndplyr::bind_cols(capitals1,weather1)\n\nNew names:\n• `city` -&gt; `city...1`\n• `city` -&gt; `city...3`\n\n\n   city...1 country  city...3 degrees       date\n1 Amsterdam      NL Amsterdam      10 2022-10-14\n2    Berlin Germany   Seattle       8 2022-10-12\n\n\nNote that both data frames contribute a city variable and these are renamed to resolve name collision.\nA similar operation binding rows stacks one data frame on top of another. The result from dplyr::bind_rows contains all columns that appear in any of the inputs, unobserved combinations are set to NA.\n\ndplyr::bind_rows(capitals1,capitals2)\n\n              city country\n1        Amsterdam      NL\n2           Berlin Germany\n3        Amsterdam      NL\n4 Washington, D.C.  U.S.A.\n\ndplyr::bind_rows(capitals1,weather1)\n\n       city country degrees       date\n1 Amsterdam      NL      NA       &lt;NA&gt;\n2    Berlin Germany      NA       &lt;NA&gt;\n3 Amsterdam    &lt;NA&gt;      10 2022-10-14\n4   Seattle    &lt;NA&gt;       8 2022-10-12\n\n\n\n\n\n\nBox, G. E. P., G. M. jenkins, and G. C. Reinsel. 1976. Time Series Analysis, Forecasting and Control. 3rd. Ed. Holden-Day.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "summarization.html",
    "href": "summarization.html",
    "title": "5  Summarization",
    "section": "",
    "text": "5.1 Introduction\nA single statistic such as the sample standard deviation is a summary, as is a cross-tabulation of the levels of two categorical variables, as is a series of box plots. The purposes of data summarization are many:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#introduction",
    "href": "summarization.html#introduction",
    "title": "5  Summarization",
    "section": "",
    "text": "Definition: Data Summarization\n\n\nData summarization is the numerical, tabular, and graphical distillation of the essence of a data set and the relationships between its variables through aggregation.\n\n\n\n\nProfiling. Borne (2021) refers to it as “having that first date with your data.” We want to know what we are dealing with.\nDescription. What are the central tendencies and the dispersion of the variables? For example, what does the comparison of statistics measuring the central tendency tell us about the distribution of the data, the presence of outliers? What distributional assumptions are reasonable for the data. Are transformations in a feature processing step called for?\nAggregation. Suppose you could not store the raw data but you need to retain information for future statistical processing. What kind of information would you compute and squirrel away? A sufficient statistic is a function of the data that contains all information toward estimating a parameter of the data distribution. For example, the sample mean \\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\) is sufficient to estimate the mean of a set of \\(n\\) independent and identically distributed random variables. If \\(Y\\) is uniform on \\([0,\\theta]\\), then \\(\\max\\{Y_i\\}\\) is sufficient for \\(\\theta\\). The quantities we compute during summarization are frequently sufficient statistics; examples are sums, sums of squares, sums of crossproducts.\nRelationships. Summarization is not only about individual variables, but also about their relationship. The correlation matrix of \\(p\\) numerical variables is a frequent summary that describes the pairwise linear relationships in the data.\nDimension Reduction. In high-dimensional statistical problems the number of potential input variables is larger than what we can handle (on computational grounds) or should handle (on statistical grounds). Summarization can reduce a \\(p\\)-dimensional problem to an \\(m\\)-dimensional problem where \\(m \\ll p\\). Principal component analysis (PCA) relies on matrix factorization (eigenvalue or singular value decomposition) of a crossproduct matrix to find a set of \\(m\\) linear combinations of the \\(p\\) input variables that explain a substantial amount of variability in the data. The \\(m\\) linear combinations summarize the essence of the relationships in the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#location-and-dispersion-statistics",
    "href": "summarization.html#location-and-dispersion-statistics",
    "title": "5  Summarization",
    "section": "5.2 Location and Dispersion Statistics",
    "text": "5.2 Location and Dispersion Statistics\nCommon location and dispersion measures for quantitative variables are shown in Table 5.1 and Table 5.2.\n\n\n\nTable 5.1: Important statistics measuring location attributes of a variable in a sample of size \\(n\\). Sample mean, sample median, and sample mode are measures of the central tendency of a variable. \\(Y^*\\) denotes the ordered sequence of observations and \\(Y^*[k]\\) the value at the \\(k\\)th position in the ordered sequence. The min is defined as the smallest non-missing value because NaNs often sort as the smallest values in software packages.\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nMin\n\n\\(Y^*[1]\\)\nThe smallest non-missing value\n\n\nMax\n\n\\(Y^*[n]\\)\nThe largest value\n\n\nMean\n\\(\\overline{Y}\\)\n\\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\)\nMost important location measure, but can be affected by outliers\n\n\nMedian\nMed\n\\[\\left \\{    \\begin{array}{cc}       Y^* \\left[ \\frac{n+1}{2} \\right ] & n \\text{ is even} \\\\        \\frac{1}{2} \\left( Y^* \\left[\\frac{n}{2} \\right] + Y^* \\left[\\frac{n}{2}+1\\right] \\right) & n\\text{ is odd} \\end{array}\\right .\\]\nHalf of the observations are smaller than the median; robust against outliers\n\n\nMode\nMode\n\nThe most frequent value; not useful when real numbers are unique\n\n\n1st Quartile\n\n\\(Y^*\\left[\\frac{1}{4}(n+1) \\right]\\)\n25% of the observations are smaller than \\(Q_1\\)\n\n\n2nd Quartile\n\nSee Median\n50% of the observations are smaller than \\(Q_2\\). This is the median\n\n\n3rd Quartile\n\n\\(Y^*\\left[\\frac{3}{4}(n+1) \\right]\\)\n75% of the observations are smaller than \\(Q_3\\)\n\n\nX% Percentile\n\n\\(Y^*\\left[\\frac{X}{100}(n+1) \\right]\\)\nFor example, 5% of the observations are larger than \\(P_{95}\\), the 95% percentile\n\n\n\n\n\n\n\n\n\nTable 5.2: Important statistics measuring dispersion (variability) of a variable in a sample of size \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nRange\n\\(R\\)\n\\(Y^*[n] - Y^*[1]\\)\nSimply largest minus smallest value\n\n\nInter-quartile Range\nIQR\n\\(Q_3 - Q_1\\)\nUsed in constructing box plots; covers the central 50% of the data\n\n\nStandard Deviation\n\\(S\\)\n\\(\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left( Y_i - \\overline{Y}\\right)^2}\\)\nMost important dispersion measure; in the same units as the sample mean (the units of \\(Y\\))\n\n\nVariance\n\\(S^2\\)\n\\(\\frac{1}{n-1}\\sum_{i=1}^n \\left( Y_i - \\overline{Y} \\right)^2\\)\nImportant statistical measure of dispersion; in squared units of \\(Y\\)\n\n\n\n\n\n\n\nstats Package\nThe following list of functions provides basic location and dispersion statistics in R (stats package). The min and max functions are provided by the base package.\n\n\n\nTable 5.3: R summarization functions in stats.\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nNotes\n\n\n\n\nmean\nSample (arithmetic) mean\ncan be trimmed (trim=)\n\n\nmedian\nSample median\n\n\n\nsd\nStandard deviation\nuses \\(n-1\\) as denominator\n\n\nvar\nSample variance\nuses \\(n-1\\) as denominator. Also can calculate variance-covariance matrix of vectors\n\n\nrange\nMinimum and maximum\nreturns a vector with two values\n\n\nIQR\nInterquartile range\n\n\n\nmad\nMedian absolute deviation\ndefault metric for center is the median\n\n\nquantile\nSample quantiles\ngive list of probabilities in probs= argument; default is minimum, maximum, \\(Q_1\\), \\(Q_2\\), and \\(Q_3\\)\n\n\nfivenum\nTukey’s five number summary\nMin, lower hinge, median, upper hinge, maximum\n\n\nboxplot.stats\nStatistics to construct box plot\nstats value contains extreme of lower whisker, lower hinge, median, upper hinge, extreme of upper whisker\n\n\ncov\nSample covariance\nSingle statistic or covariance matrix\n\n\ncor\nSample correlation\nSingle statistic or correlation matrix\n\n\n\n\n\n\nThe summary function in R is a generic function that produces summaries of an R object. The return value depends on the type of its argument. When summary is called on a data frame, it returns basic location statistics for the numeric variables and the counts-per-level for factors. Interestingly, it does not compute any measures of dispersion.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nTo compute some of the functions in Table 5.3 for multiple columns in a matrix or data frame, the apply function is very helpful. The following function call request the mean for the numeric columns (the first four columns) of the iris data set.\n\napply(iris[,-5],2,mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nThe next statement requests the standard deviations\n\napply(iris[,-5],2,sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\nThe second argument specifies the margin over which the function will be applied: 1 implies calculations for rows, 2 implies calculations for columns. The following statements compute column and row sums for the 50 I. setosa observations in the iris data set\n\ncol.sums &lt;- apply(iris[iris$Species==\"setosa\",1:4], 2, sum)\ncol.sums\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       250.3        171.4         73.1         12.3 \n\nrow.sums &lt;- apply(iris[iris$Species==\"setosa\",1:4], 1, sum)\nrow.sums\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n10.2  9.5  9.4  9.4 10.2 11.4  9.7 10.1  8.9  9.6 10.8 10.0  9.3  8.5 11.2 12.0 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n11.0 10.3 11.5 10.7 10.7 10.7  9.4 10.6 10.3  9.8 10.4 10.4 10.2  9.7  9.7 10.7 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n10.9 11.3  9.7  9.6 10.5 10.0  8.9 10.2 10.1  8.4  9.1 10.7 11.2  9.5 10.7  9.4 \n  49   50 \n10.7  9.9 \n\n\n\n\ndplyr Package\nThe dplyr package is part of the tidyverse, an “opinionated” collection of R packages for data science. Packages in the tidyverse include dplyr, ggplot2, tibble, tidyr, purr, stringr, and readr. dplyr, tidyr and ggplot2 are arguably the most important packages in the collection (that is my “opinionated” view).\nThe tidyverse packages share a common philosophy; that makes it easy to use code across multiple packages and to combine them. An example of this common philosophy is piping.\nSuppose we use dplyr to compute the mean and standard deviation for the petal width of I. virginica observations in the iris data set. You can put this together in a single function call to the dplyr::summarize function.\n\nlibrary(dplyr)\nsummarize(filter(iris,Species==\"virginica\"), \n          count=n(),\n          mean=mean(Petal.Width), \n          stdDev=sd(Petal.Width))\n\n  count  mean    stdDev\n1    50 2.026 0.2746501\n\n\nA more elegant way of processing the data is with a series of steps, where the result of one step is passed automatically to the next step. In tidyverse syntax, such a step is called a pipe and indicated with %&gt;%. Rewriting the previous summarize statement using pipes leads to\n\niris %&gt;% filter(Species==\"virginica\") %&gt;%\n    summarize(count =n(),\n              mean  =mean(Petal.Width),\n              stdDev=sd(Petal.Width)) \n\n  count  mean    stdDev\n1    50 2.026 0.2746501\n\n\nThe pipeline starts with the data frame iris. Since a pipe operation passes its input as the first argument to the next operation, the filter statement is really filter(iris,Species==\"virginica).\nIf you want to save the result of this piping operation, simply assign it to an object:\n\nvirg_summ &lt;- iris %&gt;% filter(Species==\"virginica\") %&gt;%\n                  summarize(count =n(),\n                            mean  =mean(Petal.Width),\n                            stdDev=sd(Petal.Width)) \n\nOr, you can keep going, piping the result into other tidyverse functions, for example\n\niris %&gt;% filter(Species==\"virginica\") %&gt;%\n    summarize(count =n(),\n              mean  =mean(Petal.Width),\n              stdDev=sd(Petal.Width)) %&gt;%\n    glimpse()\n\nRows: 1\nColumns: 3\n$ count  &lt;int&gt; 50\n$ mean   &lt;dbl&gt; 2.026\n$ stdDev &lt;dbl&gt; 0.2746501\n\n\nTable 5.4 lists summarization functions in dplyr.\n\n\n\nTable 5.4: Summarization functions in dplyr.\n\n\n\n\n\nFunction\nDescription\nNotes\n\n\n\n\nmean\nSample (arithmetic) mean\n\n\n\nmedian\nSample median\n\n\n\nsd\nStandard deviation\nuses \\(n-1\\) as denominator\n\n\nvar\nSample variance\nuses \\(n-1\\) as denominator\n\n\nmin\nMinimim\n\n\n\nmax\nMaximum\n\n\n\nIQR\nInterquartile range\n\n\n\nfirst\nFirst value of a vector\n\n\n\nlast\nLast value of a vector\n\n\n\nnth\n\\(n\\)th value of a vector\n\n\n\nn\nNumber of values in a vector\n\n\n\nn_distinct\nNumber of distinct (unique) values in a vector\n\n\n\n\n\n\n\n\nIn the previous examples we filtered and summarized on a single variable. If you want to calculate statistics for multiple variables you can either repeat the statements or indicate that the operation should apply across multiple columns. In the early release of dplyr the summarize_each function applied summary calculations for more than one column. This function has been deprecated in favor of across, which gathers multiple columns and can be applied to other dplyr statements as well (filter, group_by, etc.).\nThe following statement computes the mean for all numeric columns of the iris data set.\n\niris %&gt;% summarize(across(where(is.numeric), mean))\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.843333    3.057333        3.758    1.199333\n\n\nThe next form of summarize(across()) computes sample mean and median for the variables whose name begins with Sepal.\n\niris %&gt;% summarize(across(starts_with(\"Sepal\"), \n                          list(mn=mean, md=median)))\n\n  Sepal.Length_mn Sepal.Length_md Sepal.Width_mn Sepal.Width_md\n1        5.843333             5.8       3.057333              3\n\n\n\nThe order in which the data manipulations occur matter greatly for the result. In the following statement, the observations are filtered for which the Sepal.Length exceeds the average Sepal.Length by 10%.\n\niris %&gt;% filter(Sepal.Length &gt; 1.1*mean(Sepal.Length))\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1           7.0         3.2          4.7         1.4 versicolor\n2           6.9         3.1          4.9         1.5 versicolor\n3           6.5         2.8          4.6         1.5 versicolor\n4           6.6         2.9          4.6         1.3 versicolor\n5           6.7         3.1          4.4         1.4 versicolor\n6           6.6         3.0          4.4         1.4 versicolor\n7           6.8         2.8          4.8         1.4 versicolor\n8           6.7         3.0          5.0         1.7 versicolor\n9           6.7         3.1          4.7         1.5 versicolor\n10          7.1         3.0          5.9         2.1  virginica\n11          6.5         3.0          5.8         2.2  virginica\n12          7.6         3.0          6.6         2.1  virginica\n13          7.3         2.9          6.3         1.8  virginica\n14          6.7         2.5          5.8         1.8  virginica\n15          7.2         3.6          6.1         2.5  virginica\n16          6.5         3.2          5.1         2.0  virginica\n17          6.8         3.0          5.5         2.1  virginica\n18          6.5         3.0          5.5         1.8  virginica\n19          7.7         3.8          6.7         2.2  virginica\n20          7.7         2.6          6.9         2.3  virginica\n21          6.9         3.2          5.7         2.3  virginica\n22          7.7         2.8          6.7         2.0  virginica\n23          6.7         3.3          5.7         2.1  virginica\n24          7.2         3.2          6.0         1.8  virginica\n25          7.2         3.0          5.8         1.6  virginica\n26          7.4         2.8          6.1         1.9  virginica\n27          7.9         3.8          6.4         2.0  virginica\n28          7.7         3.0          6.1         2.3  virginica\n29          6.9         3.1          5.4         2.1  virginica\n30          6.7         3.1          5.6         2.4  virginica\n31          6.9         3.1          5.1         2.3  virginica\n32          6.8         3.2          5.9         2.3  virginica\n33          6.7         3.3          5.7         2.5  virginica\n34          6.7         3.0          5.2         2.3  virginica\n35          6.5         3.0          5.2         2.0  virginica\n\n\nThe average is computed across all 150 observations in the data set (50 observations for each species). By adding a group_by statement prior to the filter, the sepal lengths are being compared to the species-specific means. The presence of the group_by statement conditions the subsequent filter to be applied separately for each of the three species.\n\niris %&gt;% group_by(Species) %&gt;% filter(Sepal.Length &gt; 1.1*mean(Sepal.Length))\n\n# A tibble: 19 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1          5.8         4            1.2         0.2 setosa    \n 2          5.7         4.4          1.5         0.4 setosa    \n 3          5.7         3.8          1.7         0.3 setosa    \n 4          7           3.2          4.7         1.4 versicolor\n 5          6.9         3.1          4.9         1.5 versicolor\n 6          6.6         2.9          4.6         1.3 versicolor\n 7          6.7         3.1          4.4         1.4 versicolor\n 8          6.6         3            4.4         1.4 versicolor\n 9          6.8         2.8          4.8         1.4 versicolor\n10          6.7         3            5           1.7 versicolor\n11          6.7         3.1          4.7         1.5 versicolor\n12          7.6         3            6.6         2.1 virginica \n13          7.3         2.9          6.3         1.8 virginica \n14          7.7         3.8          6.7         2.2 virginica \n15          7.7         2.6          6.9         2.3 virginica \n16          7.7         2.8          6.7         2   virginica \n17          7.4         2.8          6.1         1.9 virginica \n18          7.9         3.8          6.4         2   virginica \n19          7.7         3            6.1         2.3 virginica \n\n\nNone of the I. setosa observations exceeded the overall sepal length by 10%. But three of the I. setosa observations exceeded the petal length of that species by 10%.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "summarization.html#group-by-summarization",
    "href": "summarization.html#group-by-summarization",
    "title": "5  Summarization",
    "section": "5.3 Group-by Summarization",
    "text": "5.3 Group-by Summarization\n\nUsing stats::aggregate\nThe preceding dplyr pipeline is an example of group-by processing: computing summaries separately for the levels of a qualitative variable (Species).\nGroup-by summarization is also possible with functions in the stats package. The aggregate function allows the use of the formula syntax that is common in many stats function. In addition to the “model”, you need to specify the aggregation function you wish to apply. For example, to compute the means for petal length by species and the standard deviations for petal length and petal width by species, use the following syntax:\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"mean\")\n\n     Species Petal.Length\n1     setosa        1.462\n2 versicolor        4.260\n3  virginica        5.552\n\naggregate(cbind(Petal.Length,Petal.Width) ~ Species, data=iris, FUN=\"sd\")\n\n     Species Petal.Length Petal.Width\n1     setosa    0.1736640   0.1053856\n2 versicolor    0.4699110   0.1977527\n3  virginica    0.5518947   0.2746501\n\n\nYou can also use aggregate with functions that have more complex return arguments, quantile and summary, for example.\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"quantile\")\n\n     Species Petal.Length.0% Petal.Length.25% Petal.Length.50% Petal.Length.75%\n1     setosa           1.000            1.400            1.500            1.575\n2 versicolor           3.000            4.000            4.350            4.600\n3  virginica           4.500            5.100            5.550            5.875\n  Petal.Length.100%\n1             1.900\n2             5.100\n3             6.900\n\naggregate(Petal.Length ~ Species, data=iris, FUN=\"summary\")\n\n     Species Petal.Length.Min. Petal.Length.1st Qu. Petal.Length.Median\n1     setosa             1.000                1.400               1.500\n2 versicolor             3.000                4.000               4.350\n3  virginica             4.500                5.100               5.550\n  Petal.Length.Mean Petal.Length.3rd Qu. Petal.Length.Max.\n1             1.462                1.575             1.900\n2             4.260                4.600             5.100\n3             5.552                5.875             6.900\n\n\n\n\nUsing dplyr::group_by\nThe group_by statement in dplyr is a simple technique to apply group-specific operations; it is thus one of the first statements you see in summarization after the data frame:\n\nstarwars %&gt;% group_by(gender) %&gt;% \n    filter(mass &gt; median(mass,na.rm=TRUE)) %&gt;%\n    summarize(count=n())\n\n# A tibble: 3 × 2\n  gender    count\n  &lt;chr&gt;     &lt;int&gt;\n1 feminine      3\n2 masculine    19\n3 &lt;NA&gt;          1\n\n\nYou can combine group-by execution with summarization across multiple columns. The following pipeline computes the arithmetic mean of all numeric columns in the iris data set and arranges the result by descending value of the average sepal width:\n\niris %&gt;% \n    group_by(Species) %&gt;% \n    summarize(across(where(is.numeric), mean)) %&gt;%\n    arrange(desc(Sepal.Width))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 virginica          6.59        2.97         5.55       2.03 \n3 versicolor         5.94        2.77         4.26       1.33 \n\n\n\n\nContinuous Grouping Variable\nGroup-by computations can also be used if the grouping variable is continuous. We first create bins of the continuous variable, assign observations to the bins, and then group the summaries by the bins. The following code assigns observations to four bins based on the quartiles of Sepal.Length an then computes the average Sepal.Width for each level of Sepal.Length.\n\nlibrary(dplyr)\n\niris_data &lt;- iris\n1qs &lt;- quantile(iris$Sepal.Length, probs=c(0, 0.25, 0.5, 0.75, 1))\nqs\n2iris_data$sep.len.cut &lt;- cut(x=iris$Sepal.Length, breaks = qs)\n\n# Fix the assignment of the minimum value to the first category\niris_data$sep.len.cut[which.min(iris$Sepal.Length)] &lt;- \n    attr(iris_data$sep.len.cut,\"levels\")[1]\n\niris_data %&gt;% group_by(sep.len.cut) %&gt;%\n         summarize(count=n(), mean=mean(Sepal.Width))\n\n\n1\n\nThe quantile function computes the sample quantiles for the requested vector of probabilities. 0 and 1 are included to capture the minimum and maximum value.\n\n2\n\nThe cut function applies the computed quantiles as break points to bin the values of Sepal.Length.\n\n\n\n\n  0%  25%  50%  75% 100% \n 4.3  5.1  5.8  6.4  7.9 \n# A tibble: 4 × 3\n  sep.len.cut count  mean\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n1 (4.3,5.1]      41  3.18\n2 (5.1,5.8]      39  3.09\n3 (5.8,6.4]      35  2.87\n4 (6.4,7.9]      35  3.07\n\n\nYou can include the creation of the categories directly into the group_by statement if the cutpoints require no further processing.\n\nmtcars %&gt;%\n    group_by(hp_cut = cut(hp, 3)) %&gt;%\n    summarize(count=n(), mn_mpg=mean(mpg), mean_disp=mean(disp))\n\n# A tibble: 3 × 4\n  hp_cut     count mn_mpg mean_disp\n  &lt;fct&gt;      &lt;int&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 (51.7,146]    17   24.2      135.\n2 (146,241]     11   15.7      339.\n3 (241,335]      4   14.6      340.\n\n\n\n\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Date with Your Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarization</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "6  Vectors and Matrices",
    "section": "",
    "text": "6.1 Introduction\nWorking with vectors and matrices is essential for statisticians. We express the mathematics of statistics in terms of scalars, vectors, and matrices. As you move into machine learning, in particular deep learning, the horizon expands from matrices to tensors (multi-dimensional arrays). For now we stick with one-dimensional (=vectors) and two-dimensional (=matrices) arrays of real numbers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#introduction",
    "href": "matrices.html#introduction",
    "title": "6  Vectors and Matrices",
    "section": "",
    "text": "Creating Vectors and Matrices\nVectors and matrices in R are special cases of the array data type. An array can have one, two, or more dimensions as indicated by its dim() attribute. Arrays with one dimensions are vectors, two-dimensional arrays are matrices.\nIf you create a one-dimensional array, you are automatically creating a vector\n\na &lt;- c(1:5)\na\n## [1] 1 2 3 4 5\nis.vector(a)\n## [1] TRUE\n\nFrequently we create vectors of constant values, the rep function helps with that:\n\nrep(1:4)\n\n[1] 1 2 3 4\n\nrep(1:4,times=2)\n\n[1] 1 2 3 4 1 2 3 4\n\n\nTo create a regular sequence of values, use seq\n\nseq(1:5)\n## [1] 1 2 3 4 5\nseq(from=1,to=10,by=0.5)\n##  [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n## [16]  8.5  9.0  9.5 10.0\nseq(length.out=10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nTo create a matrix you can use different approaches:\n\nUse the matrix function\nConvert a vector by changing its dimensions\nCoerce a numeric object into a matrix with as.matrix\n\n\nmatrix function\n\nB &lt;- matrix(1:10,ncol=2) # default is ncol=1, byrow=FALSE\nB\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\nE &lt;- matrix(1:10,ncol=2,byrow=TRUE)\nE\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n[4,]    7    8\n[5,]    9   10\n\n\n\n\nConverting a vector into a matrix\nSince a matrix is a two-dimensional array, and a vector is one-dimensional, a simple technique for creating a matrix from a vector is to make an assignment to the dimension attribute. In the following code, a one-dimensional vector of length 20 is shaped into a \\((10 \\times 2)\\) matrix. Note that the elements of the matrix are filled in column order: the first 10 elements of the vector are assigned to the rows of column 1, the next 10 elements are assigned to the rows of column 2.\n\nset.seed(876)\nx &lt;- round(rnorm(20),3)\nx\n##  [1]  0.171  0.782 -1.064 -0.264  0.114  1.940  0.461  1.226 -0.415  0.013\n## [11]  1.154  0.837  0.247 -0.916  0.378 -1.406 -0.367  0.127 -0.848 -0.146\ndim(x) &lt;- c(10,2)\nx\n##         [,1]   [,2]\n##  [1,]  0.171  1.154\n##  [2,]  0.782  0.837\n##  [3,] -1.064  0.247\n##  [4,] -0.264 -0.916\n##  [5,]  0.114  0.378\n##  [6,]  1.940 -1.406\n##  [7,]  0.461 -0.367\n##  [8,]  1.226  0.127\n##  [9,] -0.415 -0.848\n## [10,]  0.013 -0.146\nis.matrix(x)\n## [1] TRUE\n\nWhat if you want to create a \\((10 \\times 2)\\) matrix but fill the matrix in row-order: the first two elements in row 1, the next two elements in row 2, and so forth? The solution is to assign the dimensions in reverse order and transpose the result.\n\nx &lt;- round(rnorm(20),3)\nx\n##  [1]  0.742  1.415  1.603 -0.124 -0.828 -0.138  0.152  0.425 -0.159 -0.837\n## [11]  0.364 -0.373 -0.375  0.194  0.238 -0.740  2.435  1.573  1.117 -1.773\ndim(x) &lt;- c(2,10)\nt(x)\n##         [,1]   [,2]\n##  [1,]  0.742  1.415\n##  [2,]  1.603 -0.124\n##  [3,] -0.828 -0.138\n##  [4,]  0.152  0.425\n##  [5,] -0.159 -0.837\n##  [6,]  0.364 -0.373\n##  [7,] -0.375  0.194\n##  [8,]  0.238 -0.740\n##  [9,]  2.435  1.573\n## [10,]  1.117 -1.773\n\nConverting a vector into a matrix by changing its dimensions has the advantage that the object is not copied, saving memory.\n\n\nCoercion\nR is good at coercion, the implicit conversion from one data type to another. Coercion can happens implicitly when you pass an object of a different type to a function. Coercion can also be done explicitly using as.*-style functions.\n\n\n\n\n\n\nNote\n\n\n\n\n\nI really meant “as.*-style functions” as in as.matrix, as.data.frame, as.Date, as.dendrogram, etc. Not as*-style functions.\n\n\n\nFor example, to coerce an R object into a matrix, use the as.matrix function. A common usage is to convert a dataframe of numerical data:\n\ndf &lt;- data.frame(int=rep(1,4), x1=c(1,2,3,4), x2=rnorm(4))\nis.matrix(df)\n\n[1] FALSE\n\nis.matrix(as.matrix(df))\n\n[1] TRUE\n\n\n\n\n\nBasic Operations\nWhen operating on matrices and vectors, we need to distinguish elementwise operations from true matrix operations. For example, take the \\((5 \\times 2)\\) matrices B and E created earlier. Their elementwise product is the matrix with typical element \\([b_{ij}*e_{ij}]\\). In other words, elements of the matrices are matched up and the multiplication is performed separately in each cell.\nThe matrix product \\(\\textbf{B}* \\textbf{E}\\) is not possible because the matrices do not conform to matrix multiplication. However, the product \\(\\textbf{B}* \\textbf{E}^\\prime\\) is possible, the result is a (5 )$ matrix.\n\nElementwise operations\n\nB\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\n\nE\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\n## [3,]    5    6\n## [4,]    7    8\n## [5,]    9   10\n\n# Elementwise addition\n5 + B\n##      [,1] [,2]\n## [1,]    6   11\n## [2,]    7   12\n## [3,]    8   13\n## [4,]    9   14\n## [5,]   10   15\na + B\n##      [,1] [,2]\n## [1,]    2    7\n## [2,]    4    9\n## [3,]    6   11\n## [4,]    8   13\n## [5,]   10   15\nB + E\n##      [,1] [,2]\n## [1,]    2    8\n## [2,]    5   11\n## [3,]    8   14\n## [4,]   11   17\n## [5,]   14   20\n\n# Elementwise multiplication\n5 * B\n##      [,1] [,2]\n## [1,]    5   30\n## [2,]   10   35\n## [3,]   15   40\n## [4,]   20   45\n## [5,]   25   50\na * B\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    4   14\n## [3,]    9   24\n## [4,]   16   36\n## [5,]   25   50\nB * E\n##      [,1] [,2]\n## [1,]    1   12\n## [2,]    6   28\n## [3,]   15   48\n## [4,]   28   72\n## [5,]   45  100\n\nNote that when the dimensions of the two arrays do not match up, values are repeated as necessary. For example, in 5+B, the scalar 5 is applied to each cell of B. The operation is essentially the same as\n\nmatrix(rep(5,10),5,2) + B\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    7   12\n[3,]    8   13\n[4,]    9   14\n[5,]   10   15\n\n\nSimilarly, in a+B, where a is a vector of length 5, the vector is added elementwise to the first column of B, then to the second column of B.\nYou can also perform elementwise logical operations, creating vectors and matrices of TRUE/FALSE values.\n\n(B==3) | (B==2)\n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n[3,]  TRUE FALSE\n[4,] FALSE FALSE\n[5,] FALSE FALSE\n\n\n\n\nMatrix operations\n\nTransposition\nA matrix is transposed, its rows and columns exchanged, with the t() operator. Transposition exchanges the dimensions of the underlying array.\n\nB\n##      [,1] [,2]\n## [1,]    1    6\n## [2,]    2    7\n## [3,]    3    8\n## [4,]    4    9\n## [5,]    5   10\ndim(B)\n## [1] 5 2\nt(B)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\ndim(t(B))\n## [1] 2 5\n\n\n\nMultiplication\nMatrix multiplication is performed with the %*% operator. For this operation to succeed, the matrices have to conform to multiplication, that is, the number of columns of the matrix on the left side of the multiplication must equal the number of rows on the right side.\nThis will fail:\n\nB %*% E\n\nError in B %*% E: non-conformable arguments\n\n\nAnd this product succeeds, since \\(\\textbf{B}\\) and \\(\\textbf{E}^\\prime\\) conform to multiplication.\n\nB %*% t(E)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   13   27   41   55   69\n[2,]   16   34   52   70   88\n[3,]   19   41   63   85  107\n[4,]   22   48   74  100  126\n[5,]   25   55   85  115  145\n\n\n\n\nDiagonal matrices\nIn statistics we frequently encounter diagonal matrices, square matrices with zeros in off-diagonal cells. Programmatically, two important situations arise:\n\nExtracting the diagonal elements of a matrix\nForming a diagonal matrix from a vector\n\nThe diag function handles both cases\n\n# The (3 x 3) identity matrix\ndiag(1,3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n# A (3 x 3) diagonal matrix with 1, 2, 3 on the diagonal\ndiag(1:3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n\n\n diag(B %*% t(E))\n## [1]  13  34  63 100 145\n\nC_ &lt;- matrix(1:9 + rnorm(9,0,1e-3),ncol=3)\nC_\n##          [,1]     [,2]     [,3]\n## [1,] 0.999239 4.000457 7.000228\n## [2,] 1.999412 4.999930 7.999253\n## [3,] 3.000382 6.000139 9.000183\ndiag(C_)\n## [1] 0.999239 4.999930 9.000183\n\nThe trace of a matrix is the sum of its diagonal elements. You can compute the trace by combining sum and diag functions:\n\n# The trace of matrix C_\nsum(diag(C_))\n\n[1] 14.99935\n\n\n\n\nCrossproduct matrix\nThe crossproduct of matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) is \\(\\textbf{A}^\\prime \\textbf{B}\\) provided that \\(\\textbf{A}^\\prime\\) and \\(\\textbf{B}\\) are conformable for multiplication. The most important crossproduct matrices in statistics are crossproducts of a matrix with itself: \\(\\textbf{A}^\\prime\\textbf{A}\\). These crossproducts are square, symmetric matrices.\nYou can calculate a crossproduct matrix directly using matrix multiplication, or a dedicated function (base::crossprod or Matrix::crossprod). The dedicated functions are slightly smaller than computing the product directly, but I have found the difference to be pretty negligible, even for large matrices.\n\nX &lt;- matrix(rnorm(300),nrow=100,ncol=3)\n\n# Computing X`X by direct multiplication\nXpX &lt;- t(X) %*% X\nXpX\n\n          [,1]      [,2]       [,3]\n[1,]  81.91635 12.696119 -13.566906\n[2,]  12.69612 83.278958  -1.086481\n[3,] -13.56691 -1.086481 102.548803\n\n# Computing X`X using the crossprod() function\ncrossprod(X)\n\n          [,1]      [,2]       [,3]\n[1,]  81.91635 12.696119 -13.566906\n[2,]  12.69612 83.278958  -1.086481\n[3,] -13.56691 -1.086481 102.548803\n\n\n\n\nInverse matrix\nThe inverse of square matrix \\(\\textbf{A}\\), denoted \\(\\textbf{A}^{-1}\\), if it exists, is the multiplicative identity: \\[\n\\textbf{A}^{-1}\\textbf{A}= \\textbf{A}\\textbf{A}^{-1} = \\textbf{I}\n\\] The inverse exists if \\(\\textbf{A}\\) is of full rank–we say that than the matrix is non-singular.\n\nlibrary(Matrix)\nrankMatrix(XpX)[1]\n\n[1] 3\n\n\nThe matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) in our example has rank 3, which equals its number of rows (columns). The matrix is thus of full rank and can be inverted. Computing the inverse matrix is a special case of using the solve function. solve(a,b,...) solves a linear system of equation of the form \\[\n\\textbf{A}\\textbf{X}= \\textbf{B}\n\\] When the function is called without the b argument, it returns the inverse of a:\n\nXpX_inverse &lt;- solve(XpX)\n\nXpX_inverse\n##             [,1]          [,2]          [,3]\n## [1,]  0.01278294 -0.0019270000  0.0016707297\n## [2,] -0.00192700  0.0122999860 -0.0001246209\n## [3,]  0.00167073 -0.0001246209  0.0099711670\n\nround(XpX %*% XpX_inverse,4)\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#least-squares-from-scratch",
    "href": "matrices.html#least-squares-from-scratch",
    "title": "6  Vectors and Matrices",
    "section": "6.2 Least Squares from Scratch",
    "text": "6.2 Least Squares from Scratch\nCoding algorithms in statistical programming often starts with reproducing the formulas on paper in a programming language.\nSuppose we wish to fit a multiple linear regression model with target variable (output) \\(Y\\) and predictor variables (inputs) \\(X_1, \\cdots, X_p\\). The \\(n\\) data points for this analysis are arranged in an \\((n \\times 1)\\) vector \\[\n\\textbf{Y} = [Y_1, \\cdots, Y_n]^\\prime\n\\] an \\((n \\times (p+1))\\) matrix \\[\n\\textbf{X} = \\left [ \\begin{array}{ccc}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{1n} & \\cdots & x_{pn}\n\\end{array}\\right]\n\\] and an \\((n \\times 1)\\) vector of error terms \\[\n\\boldsymbol{\\epsilon} = [\\epsilon_1, \\cdots, \\epsilon_n]^\\prime\n\\]\nThe complete regression model can be written as \\[\n\\textbf{Y} = \\textbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\qquad \\boldsymbol{\\epsilon} \\sim (\\textbf{0},\\sigma^2 \\textbf{I})\n\\] The statement on the right says that the model errors have mean zero, equal variance \\(\\sigma^2\\) and are uncorrelated—this is also sometimes called the iid assumption (identically and independently distributed), although lack of correlation does not strictly imply independence.\nThe parameter vector \\(\\boldsymbol{\\beta}\\) in this model is typically estimated by ordinary least squares (OLS), the solution is \\[\n\\widehat{\\boldsymbol{\\beta}} = \\left (\\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] provided that \\(\\textbf{X}\\) is of full column rank (which implies \\(\\textbf{X}^\\prime\\textbf{X}\\) is non-singular and (\\(\\textbf{X}^\\prime\\textbf{X})^{-1}\\) exists) and the predicted values are \\[\n\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\n\\]\nLet’s use a data set and compute the OLS estimates and the predicted values using matrix–vector operations, then compare the results to the standard output of the linear modeling function lm() in R.\nThe data set for this exercise is the fitness data set. The data comprise measurements of aerobic capacity and other attributes on 31 men involved in a physical fitness course at N.C. State University.\nAerobic capacity is the ability of the heart and lungs to provide the body with oxygen. It is a measure of fitness and expressed as the oxygen intake in ml per kg body weight per minute. Measuring aerobic capacity is expensive and time consuming compared to attributes such as age, weight, and pulse. The question is whether aerobic capacity can be predicted from the easily measurable attributes. If so, a predictive equation can reduce time and effort to assess aerobic capacity.\nThe variables are\n\nAge: age in years\nWeight: weight in kg\nOxygen: oxygen intake rate (ml per kg body weight per minute)\nRunTime: time to run 1.5 miles (minutes)\nRestPulse: heart rate while resting\nRunPulse: heart rate while running (same time Oxygen rate measured)\nMaxPulse: maximum heart rate recorded while running\n\nThe linear model we have in mind is \\[\n\\text{Oxygen}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\beta_2\\text{Weight}_i + \\beta_3\\text{RunTime}_i + \\beta_4\\text{RestPulse}_i + \\beta_5\\text{RunPulse}_i + \\beta_6\\text{MaxPulse}_i + \\epsilon_i\n\\] \\(i=1,\\cdots,31\\).\nThe following code makes a connection to the ads DuckDB database, loads the fitness table into an R dataframe, displays the first 6 observations, and closes the connection to the database again.\n\nlibrary(\"duckdb\")\ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nfit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\ndbDisconnect(con)\n\nThe target variable for the linear model is Oxygen, the remaining variables are inputs to the regression. The next statements create the \\(\\textbf{y}\\) vector and the \\(\\textbf{X}\\) matrix for the model. Note that the first column of \\(\\textbf{X}\\) is a vector of ones, representing the intercept \\(\\beta_0\\).\n\ny &lt;- as.matrix(fit[,which(names(fit)==\"Oxygen\")])\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\nhead(X)\n\n     Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\n[1,]      1  44  89.47   11.37        62      178      182\n[2,]      1  40  75.07   10.07        62      185      185\n[3,]      1  44  85.84    8.65        45      156      168\n[4,]      1  42  68.15    8.17        40      166      172\n[5,]      1  38  89.02    9.22        55      178      180\n[6,]      1  47  77.45   11.63        58      176      176\n\n\nNext we are building the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix and compute its inverse, \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\), with the solve() function. t() transposes a matrix and %*% indicates that we are performing matrix multiplication rather than elementwise multiplication.\n\nXpX &lt;- t(X) %*% X\nXpXInv &lt;- solve(XpX)\n\nWe can verify that XpxInv is indeed the inverse of XpX by multiplying the two. This should yield the identity matrix\n\nround(XpX %*% XpXInv,3)\n\n          Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\nIntcpt         1   0      0       0         0        0        0\nAge            0   1      0       0         0        0        0\nWeight         0   0      1       0         0        0        0\nRunTime        0   0      0       1         0        0        0\nRestPulse      0   0      0       0         1        0        0\nRunPulse       0   0      0       0         0        1        0\nMaxPulse       0   0      0       0         0        0        1\n\n\nNext we compute the OLS estimate of \\(\\boldsymbol{\\beta}\\) and the predicted values \\(\\widehat{\\textbf{y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\).\n\nbeta_hat &lt;- XpXInv %*% t(X) %*% y\nbeta_hat\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\ny_hat &lt;- X %*% beta_hat\n\nThe estimate of the intercept is \\(\\widehat{\\beta}_0\\) = 102.9345, the estimate of the coefficient for Age is \\(\\widehat{\\beta}_1\\) = -0.227 and so on.\nWe could have also used the solve function in its intended application, to solve a system of linear equations–we abused the behavior of solve a bit by using it with only one argument; it will then return the inverse matrix of the argument.\nThe linear system to solve in the ordinary least squares problem is \\[\n\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}= \\textbf{X}^\\prime\\textbf{Y}\n\\] This system of equations is called the normal equations. The solution can be computed with the solve function:\n\nsolve(XpX,crossprod(X,y))\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\n\nThe results match beta_hat computed earlier.\nThe residuals \\(\\widehat{\\boldsymbol{\\epsilon}} = \\textbf{y}- \\widehat{\\textbf{y}}\\) and the error sum of squares\n\\[\n\\text{SSE} = (\\textbf{y}- \\widehat{\\textbf{y}} )^\\prime (\\textbf{y}- \\widehat{\\textbf{y}}) = \\sum_{i=1}^n \\left(y_i - \\widehat{y}_i\\right)^2\\] and the estimate of the residual variance \\[\\widehat{\\sigma}^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\n\\]\nare computed as\n\nresiduals &lt;- y - y_hat\nSSE &lt;- sum(residuals^2)\nn &lt;- nrow(fit)\nrankX &lt;- rankMatrix(XpX)[1]\nsigma2_hat &lt;- SSE/(n - rankX)\n\nSSE\n\n[1] 128.8379\n\nsigma2_hat\n\n[1] 5.368247\n\n\nWe used the rankMatrix function in the Matrix package to compute the rank of \\(\\textbf{X}\\), which is identical to the rank of \\(\\textbf{X}^\\prime\\textbf{X}\\). With these quantities available, the variance-covariance matrix of \\(\\widehat{\\boldsymbol{\\beta}}\\), \\[\\\n\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\textbf{X}^\\prime\\textbf{X})^{-1}\n\\] can be estimated by substituting \\(\\widehat{\\sigma}^2\\) for \\(\\sigma^2\\). The standard errors of the regression coefficient estimates are the square roots of the diagonal values of this matrix.\n\nVar_beta_hat &lt;- sigma2_hat * XpXInv\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nse_beta_hat\n\n     Intcpt         Age      Weight     RunTime   RestPulse    RunPulse \n12.40325810  0.09983747  0.05459316  0.38456220  0.06605428  0.11985294 \n   MaxPulse \n 0.13649519 \n\n\nNow let’s compare our results to the output from the lm() function in R.\n\nlinmod &lt;- lm(Oxygen ~ ., data=fit)\nsummary(linmod)\n\n\nCall:\nlm(formula = Oxygen ~ ., data = fit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4026 -0.8991  0.0706  1.0496  5.3847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 102.93448   12.40326   8.299 1.64e-08 ***\nAge          -0.22697    0.09984  -2.273  0.03224 *  \nWeight       -0.07418    0.05459  -1.359  0.18687    \nRunTime      -2.62865    0.38456  -6.835 4.54e-07 ***\nRestPulse    -0.02153    0.06605  -0.326  0.74725    \nRunPulse     -0.36963    0.11985  -3.084  0.00508 ** \nMaxPulse      0.30322    0.13650   2.221  0.03601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.317 on 24 degrees of freedom\nMultiple R-squared:  0.8487,    Adjusted R-squared:  0.8108 \nF-statistic: 22.43 on 6 and 24 DF,  p-value: 9.715e-09\n\n\nBased on the quantities calculated earlier, the following code reproduces the lm summary.\n\ntvals &lt;- beta_hat/se_beta_hat\npvals &lt;- 2*(1-pt(abs(tvals),n-rankX))\nresult &lt;- cbind(beta_hat, se_beta_hat, tvals, pvals)\ncolnames(result) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"Pr(&gt;|t|)\")\nround(result,5)\n\n           Estimate Std. Error  t value Pr(&gt;|t|)\nIntcpt    102.93448   12.40326  8.29899  0.00000\nAge        -0.22697    0.09984 -2.27343  0.03224\nWeight     -0.07418    0.05459 -1.35873  0.18687\nRunTime    -2.62865    0.38456 -6.83544  0.00000\nRestPulse  -0.02153    0.06605 -0.32600  0.74725\nRunPulse   -0.36963    0.11985 -3.08401  0.00508\nMaxPulse    0.30322    0.13650  2.22145  0.03601\n\ncat(\"\\nResidual standard error: \", sqrt(sigma2_hat),\" on \", n-rankX, \"degrees of freedom\\n\")\n\n\nResidual standard error:  2.316948  on  24 degrees of freedom\n\nSST &lt;- sum( (y -mean(y))^2 )\ncat(\"Multiple R-squared: \", 1-SSE/SST, \n    \"Adjusted R-squared: \", 1 - (SSE/SST)*(n-1)/(n-rankX), \"\\n\")\n\nMultiple R-squared:  0.8486719 Adjusted R-squared:  0.8108399 \n\nFstat &lt;- ((SST-SSE)/(rankX-1)) / (SSE/(n-rankX))\ncat(\"F-statistic: \", Fstat, \"on \", \n    rankX-1, \"and\", n-rankX, \"DF, p-value:\", 1-pf(Fstat,rankX-1,n-rankX))\n\nF-statistic:  22.43263 on  6 and 24 DF, p-value: 9.715305e-09",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#building-a-model-matrix",
    "href": "matrices.html#building-a-model-matrix",
    "title": "6  Vectors and Matrices",
    "section": "6.3 Building a Model Matrix",
    "text": "6.3 Building a Model Matrix\nIn the previous example, the model matrix \\(\\textbf{X}\\) was formed in code by appending a \\((31 \\times 6)\\) matrix of input variables to a \\(31 \\times 1\\) vector of ones:\n\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\n\nThe lm function used a special syntax to specify the model, called a model formula: Oxygen ~ .. The formula specifies the target variable (the dependent variable) on the left side of the tilde and the input (predictor, independent) variables on the right hand side of the tilde. The special dot syntax implies to include all variables in the data frame (except for Oxygen) as input variables for the right hand side of the model. Also, the intercept is automatically included in model formulas, because not having an intercept is a special case in statistical modeling.\nYou can generate a model matrix easily by using the model.matrix function with a model formula. In the fitness example,\n\nX_ &lt;- model.matrix(Oxygen ~ ., data=fit)\n\nThe two matrices are identical, as you can see with\n\nsum(X - X_)\n\n[1] 0\n\n\nIf we wanted to repeat the regression calculations for a model that contains only Age and MaxPulse as predictor variables, it would be easy to construct the model matrix with\n\nX_small_model &lt;- model.matrix(Oxygen ~ Age + MaxPulse, data=fit)\n\nUsing model.matrix is very convenient when you work with factors, classification input variables that are not represented in the model by their actual values (which could be strings) but by converting a variable with \\(k\\) unique values into \\(k\\) columns in \\(\\textbf{X}\\). These columns use 0/1 values to encode which level of the classification input variable matches the value for an observation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "7  Random Number Generation",
    "section": "",
    "text": "7.1 Pseudo Random Number Generation\nMost random number generators (RNGs) are not generating true random numbers in the sense that the sequence of numbers is impossible to foresee. They are pseudo random number generators (PRNGs); they produce predetermined streams of numbers that appear random.\nAn example of a true random number generator, and probably a surprising one, is the wall of lava lamps in the lobby of the San Francisco office of internet infrastructure company Cloudflare.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "random.html#pseudo-random-number-generation",
    "href": "random.html#pseudo-random-number-generation",
    "title": "7  Random Number Generation",
    "section": "",
    "text": "Example: Cloudflare Lava Lamps: LavaRand\n\n\nIn the lobby of the San Francisco office of internet service provider Cloudflare is a wall of eighty lava lamps (Figure 7.1). This is not some retro thing, but part of the mission-critical operations of the internet infrastructure company. The lava lamps are used to generate cryptographic keys to secure the internet communications and services. Cloudflare is so convinced that the keys generated are unbreakable, because they are truly unpredictable, that they blog about how the lava-lamp key generator works; see here.\nTrue cryptography requires that keys are unpredictable, something that computers are not good at. By definition, the operations of a computer are predictable, they follow a precise program. The same input will produce the same output—every time. The reliability of computers is bad for cryptography. You can make things better by making the input to a RNG random itself, known as a cryptographically-secure pseudorandom number generators (CSPRNGs). That would make the sequence less predictable but where does the random input come from? Another “predictable” random number generator?\n\n\n\n\n\n\nFigure 7.1: Lava lamps in the lobby of the headquarters of Cloudflare. Source.\n\n\n\nCloudflare uses the wall of 80 lava lamps to generate unpredictable input using a camera that continuously looks at the wall and takes video of the state of the lamps. From the Cloudflare blog:\n\nLavaRand is a system that uses lava lamps as a secondary source of randomness for our production servers. A wall of lava lamps in the lobby of our San Francisco office provides an unpredictable input to a camera aimed at the wall. A video feed from the camera is fed into a CSPRNG, and that CSPRNG provides a stream of random values that can be used as an extra source of randomness by our production servers. Since the flow of the “lava” in a lava lamp is very unpredictable, “measuring” the lamps by taking footage of them is a good way to obtain unpredictable randomness.\n\nComputers are predictable, the physical real world is not!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "random.html#how-to-generate-random-numbers",
    "href": "random.html#how-to-generate-random-numbers",
    "title": "7  Random Number Generation",
    "section": "7.2 How to Generate Random Numbers",
    "text": "7.2 How to Generate Random Numbers\n\nQuantile Functions\nCan you generate random numbers for one distribution from random numbers for another distribution? If the answer is “Yes” this could make it easier to write random number generators, a basic generator could be used to transform random numbers from one distribution into those of another. Fortunately, the answer is indeed “Yes” and the results leading there are very cool properties of cumulative distribution (c.d.f.) and quantile functions.\n\n\nDefinition: Quantile Function\n\n\nIf \\(Y\\) is a random variable with c.d.f. \\(F(y) = \\Pr(Y \\le y)\\), then the quantile function \\(Q(p)\\) is the inverse of the cumulative distribution function; for a given probability \\(p\\), it returns the value \\(y\\) for which \\(F(y) = p\\): \\[\nQ(p) = F^{-1}(p)\n\\]\n\n\nSo, if \\(F\\) maps \\(F:\\mathbb{R} \\rightarrow [0,1]\\), the quantile function maps \\(Q:[0,1] \\rightarrow \\mathbb{R}\\).\nHere is a possibly surprising result: if \\(Y\\) has c.d.f. \\(F(y)\\), then we can think of the c.d.f as a transformation of \\(Y\\). What would its distribution look like? The following R code draws 1,000 samples from a G(\\(2,1.5^2\\)) distribution and plots the histogram of the c.d.f. values \\(F(y)\\).\n\n# See section below on setting random number seed values\nset.seed(455675)\nyn &lt;- rnorm(1000,mean=2,sd=1.5)\nF_yn &lt;- pnorm(yn,mean=2,sd=1.5)    \nhist(F_yn,main=\"\")\n\n\n\n\n\n\n\n\nThe distribution of \\(F(y)\\) is uniform on (0,1)—this is true for any distribution, not just the Gaussian.\nWe can combine this result with the following, possibly also surprising, result: If \\(U\\) is a uniform random variable, and \\(Q(p)\\) is the quantile function of \\(Y\\), then \\(Q(u)\\) has the same distribution as \\(Y\\). This suggests a method to generate random numbers from any distribution if you have a generator of uniform random numbers: plug the uniform random numbers into the quantile function. Figure 7.2 shows this for G(2,1.5^2) random numbers and Figure 7.3 for Beta(1.5,3) random numbers.\n\nnorm_rv &lt;- qnorm(runif(1000),mean=2,sd=1.5)\nhist(norm_rv,main=\"\",xlab=\"Y\")\n\n\n\n\n\n\n\nFigure 7.2: G(2,1.5^2) random variables via quantile transform of U(0,1) random numbers.\n\n\n\n\n\n\nbeta_rv &lt;- qbeta(runif(1000),shape1=1.5,shape2=3)\nhist(beta_rv,main=\"\",xlab=\"Y\")\n\n\n\n\n\n\n\nFigure 7.3: Beta(1.5,3) random variables via quantile transform of U(0,1) random numbers.\n\n\n\n\n\nBecause the quantile function is the inverse c.d.f., this method of generating random numbers is also known as the inversion method.\n\n\nTransformations\nAnother approach to generate random numbers based on uniform numbers applies known transformations. The Box-Muller method (Box and Muller 1958), for example, generates pairs of independent standard Gaussian random variables from pairs of independent uniform random variables. If \\(U_1\\) and \\(U_2\\) are independent draws from a uniform distribution on [0,1], then \\[\n\\begin{align*}\nZ_1 &= \\sqrt{-2\\log U_1} \\cos(2\\pi U_2) \\\\\nZ_2 &= \\sqrt{-2\\log U_1} \\sin(2\\pi U_2)\n\\end{align*}\n\\] are independent random variables with G(0,1) distribution. The Box-Muller random number generator is an example of transforming uniform variables into normal variables without going through the quantile function.\nA general, and very intuitive, method to create random numbers from any distribution is based on acceptance-rejection sampling.\n\n\nAcceptance-Rejection Method\nWhat if we do not know the quantile function \\(Q(p)\\), and all we have is the density or mass function \\(p(y)\\) of the variable whose random numbers we wish to generate? Such a situation might arise when we observe an empirical distribution function and now want to generate a sequence of random numbers from the particular distribution. Again, we can use uniform random numbers and “transform” them in such a way that the long-run frequency distribution of the transform matches the target distribution. Figure 7.4 depicts the idea graphically.\nThe target distribution we wish to draw random numbers from is \\(p(y)\\). We know the density to the extent that we can evaluate whether a pair of points \\((y,U(y))\\) falls above or below \\(p(y)\\). This allows us to transform a sequence of uniform random numbers drawn between \\([\\min(y),\\max(y)]\\) into a sequence of random numbers from \\(p(y)\\).\n\n\n\n\n\n\nFigure 7.4: Random numbers via acceptance-rejection sampling.\n\n\n\nThe acceptance-rejection algorithm to sample from \\(p(y)\\) based on \\(U(\\min(y),\\max(y))\\) random variables is very simple (Figure 7.5):\n\nDraw a random number \\(X \\sim U(\\min\\{y\\}, \\max\\{y\\})\\)\nDraw a random number \\(Y \\sim U(\\min\\{p(y)\\}, \\max\\{p(y)\\})\\)\nIf \\(Y \\le p(X)\\), accept \\(X\\) as a draw from \\(p(y)\\). Otherwise, reject \\(X\\) and return to 1.\n\n\n\n\n\n\n\nFigure 7.5: Random numbers via acceptance-rejection sampling.\n\n\n\nYou continue this acceptance-rejection decision until you have accepted enough random numbers.\n\n\nExample: Rejection Method for Beta(\\(\\alpha,\\beta\\))\n\n\nSuppose we wish to generate 1,000 random numbers from a Beta distribution and we do not have access to a quantile function. However, we can evaluate the density of the Beta random variable \\(Y\\), \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\]\nIn R we can calculate this with the dbeta(y,alpha,beta) function. The following code uses a simple while loop to fill the pre-allocated vector rbet with Beta(1.5,3) random variables. The maximum density of a Beta(1.5,3) is about \\(\\max\\{p(y)\\} = 1.87\\); this value is used in generating the random number for the vertical axis.\n\nrbet &lt;- rep(NA,1000)\nnumacc &lt;- 0\nntries &lt;- 0\nwhile (numacc &lt; length(rbet)) {\n    # Draw independent random numbers for abscissa and ordinate\n    x &lt;- runif(1,0,1   )\n    y &lt;- runif(1,0,1.87)\n    # Check whether to accept or reject X depending on \n    # whether Y is below or above the target density\n    if (y &lt;= dbeta(x,1.5,3)) {\n        numacc &lt;- numacc+1\n        rbet[numacc] &lt;- x\n    }\n    ntries &lt;- ntries + 1\n}\nntries\n\n[1] 1848\n\nhist(rbet,main=\"\")\n\n\n\n\n\n\n\nFigure 7.6: Beta(1.5,3) random variables by acceptance-rejection sampling.\n\n\n\n\n\nThe distribution of the random numbers generated via quantile transform (Figure 7.3) and via acceptance-rejection (Figure 7.6) are very similar. Note that the acceptance method performs 1848 attempts, each requires two uniform random numbers. In total, the method uses 3696 uniform random numbers and 1848 evaluations of the Beta density function to create the random sequence of 1,000 Beta(1.5,3) draws. The U(0,1) is a special case of the Beta distributions with \\(\\alpha = \\beta = 1\\). The target distribution in this example is not that different from the sampled distribution. When the two distributions are more distinct, you will reject more samples along the way.\n\n\nIf you were to use acceptance-rejection sampling to generate data from a Beta(1,1) distribution, what would be the proportion of accepted samples?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "random.html#random-numbers-in-r",
    "href": "random.html#random-numbers-in-r",
    "title": "7  Random Number Generation",
    "section": "7.3 Random Numbers in R",
    "text": "7.3 Random Numbers in R\nOne of the most commonly used random number generators for uniform random variables is the Mersenne Twister (MT) algorithm of Matsumoto and Nishimura (1998). The MT algorithm is the default RNG in many commercial and open source packages, for example, in R, SAS, SPSS, Stata.\nThe period of the Mersenne Twister algorithm is based on a Mersenne prime, from which it derives its name. The period of a PRNG is the sequence length after which the generated numbers repeat themselves. The period of the MT algorithm is very long, \\(2^{19937}-1\\). Obviously, you want the period to be very large, but that in itself does not guarantee a high quality random number generator. The algorithms also need to pass statistical tests that analyze the distribution of the generated sequences, for example the battery of “Diehard Tests”.\nOne such test examines whether successive pairs of observations drawn from U(0,1) evenly fill the unit square (Figure 7.7).\n\nm = 40000\npar(mfrow=c(1,2))\nfor (i in 1:2) {\n    u = runif(m);  \n    plot(u[1:(m-1)], u[2:m], pch=\".\",\n         xlab=\"Value\",\n         ylab=\"Next value\")\n}\n\n\n\n\n\n\n\nFigure 7.7: Two sets of succesive pairs of 40,000 U(0,1) random numbers plotted on the unit square.\n\n\n\n\n\nThe MT algorithm performs well on these tests, but it is not perfect—no pseudo-random number generator is. For example, if you wish to generate independent sequences of random numbers, choosing different seed values for each sequence is not the way to go. You should instead use different segments of the same sequence.\nThe RNGkind() function in R returns a list of three strings that identify\n\nthe default random number generator\nthe method to create G(0,1) random numbers\nthe algorithm used to create random numbers from a discrete uniform distribution\n\n\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"       \n\n\nIn this setting, uniform random numbers are generated with the Mersenne Twister algorithm, Gaussian variates are generated by quantile inversion, and discrete uniform random numbers are produced with an acceptance-rejection algorithm.\nThe RNGkind() function is also used to change the RNGs. For example, to use the Box-Muller algorithm instead of inversion for Gaussian random variables, specify\n\nRNGkind(normal.kind=\"Box-Muller\")\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Box-Muller\"       \"Rejection\"       \n\n\nTo reset the assignment to the default, use\n\nRNGkind(normal.kind=\"default\")\nRNGkind()\n\n[1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"       \n\n\nThe state of a pseudo RNG is represented by a series of numbers, for the MT algorithm this is a vector of 624 integers. Once the state has been initialized, random numbers are generated according to the algorithm. Each time a random number is drawn, the state of the generator advances.\nThe state of the RNG can be seen with the .Random.seed variable, a vector of integers. The first value is an encoding of the three elements of RNGkind(). For the MT, .Random.seed[3:626] is the 624-dimensional integer vector of the RNG’s state. It is not recommended to make direct assignments to .Random.seed. Instead, you should initialize the RNG to a particular state through the seed value.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "random.html#seed-values",
    "href": "random.html#seed-values",
    "title": "7  Random Number Generation",
    "section": "7.4 Seed Values",
    "text": "7.4 Seed Values\nThe random number seed is an integer that initializes the random number generator. What that initialization looks like depends on the RNG. The goal is the same, however: to place the RNG in a known state that generates a known sequence of random numbers. This makes statistical programs that depend on random numbers reproducible in the sense that the exact same results are generated each time the program runs. This might be necessary for auditing, to verify what the program does, and to lock the results for a production environment.\nThe set.seed function in R accomplishes that. Once the seed is set to a given value, the same random number sequence results.\n\nset.seed(123)\nrunif(4)\n\n[1] 0.2875775 0.7883051 0.4089769 0.8830174\n\nrunif(2)\n\n[1] 0.9404673 0.0455565\n\nset.seed(123)\nrunif(6)\n\n[1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565\n\n\nWhether you draw 4 and then 2 random numbers or 6 numbers at once, the same sequence results.\nIf you do not set the seed, the system starts with an initial seed, usually chosen based on the system clock and/or the process ID, and the state of the RNG will differ from session to session. Calling set.seed(NULL) re-initializes the state of the RNG as if no seed had been set.\nWhen it comes to seed values, here are some best practices\n\nDo not go “seed hunting”. Changing the seed values until the results support a particular narrative is bad practice and borders on unethical behavior. If the simulation does not show what you expect, after setting a seed value, the problem might be that you are not drawing enough samples or your program might be incorrect. Do not try to “fix” things by experimenting with seed values.\nTo generate independent sequences of random numbers, do not use separate seeds for the sequences. Use a single seed and extract different segments from a single stream. Choosing an RNG with a long period is important in this case.\nSome functions allow that seeds for their internal random number generation are passed as function arguments. It becomes confusing to use multiple methods throughout a program to affect random number streams. RNGs should be initialized once using set.seed(), preferably at the beginning of the program, and this should be clearly documented.\nDo not use the same seed over and over. A best practice is to choose the seed itself based on a (true) random number generator. An acceptable method is to use the default initialization of the RNG and use one of the state variables as the seed; for example:\n\n\nset.seed(NULL)\n.Random.seed[10]\n\n[1] -1620200365\n\nset.seed(.Random.seed[10])\n\nYou need to keep track of the chosen seed value outside of the program, otherwise the results are not reproducible.\n\nBe aware that random numbers can be involved in subtle and unexpected ways in many parts of the code. Jittering observations on plots to avoid overlaying data points involves adding small random amounts—this will advance the internal state of the RNG. Subsampling, cross-validation, bootstrapping, train:test splits, measuring feature importance by permutation, are other examples of RNG-dependent techniques.\nWhen you work with a program interactively, and execute the code in chunks, remember that the internal state of the RNG advances with each call to a random number function. Setting the seed once, then executing a code chunk with RNG three times in a row while debugging, then moving on to a second chunk that depends on random numbers will yield different results compared to running the code top-to-bottom or in batch mode.\n\n\nIt is not always possible to take complete control of random number streams in your statistical program. Stochastic elements can be involved in ways that you cannot fix with a set.seed() command. For example, when you fit artificial neural networks (ANNs) in R using the keras package, the code executes the underlying routines written in Python. Setting a seed in the R session does not affect the random number generator in the NumPy Python library. Even worse, Keras supports multiple machine learning frameworks. If it runs on TensorFlow, for example, TensorFlow has its own random number generator that is used in addition to the NumPy generator.\nWhen code runs on multi-core CPUs or on GPUs (graphical processing units) it can (should) take advantage of threaded execution. This introduces a non-deterministic element to the computations that can result in slightly different numerical results on each run.\nOne suggestion to deal with this “inherent” lack of reproducibility is to run the analysis 30 times and take the average. This does not eliminate the lack of reproducibility, it only lessens it. In most cases the computational demand of this approach is prohibitive.\n\n\n\nFigure 7.1: Lava lamps in the lobby of the headquarters of Cloudflare. Source.\nFigure 7.4: Random numbers via acceptance-rejection sampling.\nFigure 7.5: Random numbers via acceptance-rejection sampling.\n\n\n\nBox, G. E. P., and M. E. Muller. 1958. “A Note on the Generation of Normal Random Deviates.” Annals of Mathematical Statistics 29: 610–11. doi:10.1214/aoms/1177706645.\n\n\nMatsumoto, M., and T. Nishimura. 1998. “Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator.” ACM Transactions on Modeling and Computer Simulation 8: 3–30.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Number Generation</span>"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "8  Debugging in R",
    "section": "",
    "text": "8.1 Profiling\nProfiling code gives you an idea of how much time is spent executing the lines of code. This sounds simple, but is actually quite tricky. First, you need to have a way of reliably measure time, and you need to distinguish different definitions of “time” on a computer. In most situations we are interested in wall-clock time, referring to the amount of time elapsed when the code execution is measured by taking the difference between the time of day just prior and after the run.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#profiling",
    "href": "debugging.html#profiling",
    "title": "8  Debugging in R",
    "section": "",
    "text": "Measuring Execution Time\nTo time the code\n\nfor (i in 1:200) { mad(rnorm(1000)) }\n\nwe could wrap it in calls to Sys.time and take the difference:\n\nst &lt;- Sys.time()\nfor (i in 1:200) { mad(rnorm(1000)) }\ndiff &lt;- Sys.time() - st\ndiff\n\nTime difference of 0.01855612 secs\n\n\nSys.time returns the absolute date and time of day, the precision is on the order of milliseconds but depends on the operating system. In order to measure execution time of code, it is recommended to use proc.time() instead, which measures the time the R process has spend on execution. proc.time() returns three numbers, the CPU time charged for the execution of the user instructions, the system time charged for execution by the system on behalf of the calling process, and the elapsed (wall-clock time).\n\nproc.time()\n\n   user  system elapsed \n  0.337   0.031   0.391 \n\n\nWrapping the loop timed previously with Sys.time with calls to proc.time results in the following:\n\nst &lt;- proc.time()\nfor (i in 1:200) { mad(rnorm(1000)) }\ndiff &lt;- proc.time() - st\ndiff\n\n   user  system elapsed \n  0.017   0.000   0.017 \n\n\nYou can simplify this operation by using system.time instead of proc.time. system.time makes calls to proc.time at the beginning and the end of the code execution and reports the difference:\n\nsystem.time(for(i in 1:200) mad(rnorm(1000)))\n\n   user  system elapsed \n  0.016   0.000   0.016 \n\n\n\n\nMicro-benchmarking\nBenchmarking is the comparison of code alternatives. Sometimes, one of the alternatives is an established set of tests or conditions against which a product is evaluated. TPC, for example, is a non-profit organization that establishes benchmark tests for databases. Right or wrong, if you develop a new database, eventually you will have to evaluate the database against the various TPC benchmarks.\nMore often benchmarking comes into play in evaluating alternative ways of accomplishing the same result. Which one is faster? Which one requires less memory? Does it matter? This form of benchmarking uses packages designed to evaluate code execution with sub-millisecond precision. One such package in R is microbenchmark. It gives a more accurate comparison than the frequently seen system.time(replicate(1000,expr)) expression. However, the package is not available for all versions of R. For example, it failed to install on my M2 Mac R 4.3.0. Another package is bench, which we will use here.\nThe idea of micro-benchmarking is to run a piece of code multiple times, either a fixed number of times or for a certain length of time and to describe the statistical properties of the run times (min, max, mean, std. deviation, median) as well as other characteristics (memory consumption, garbage collection, etc.).\nConsider the following three approaches to compute the square root of the elements of a vector:\n\nx &lt;- rep(1,1000)\ns1 &lt;- sqrt(x)\ns2 &lt;- x^0.5\ns3 &lt;- exp(log(x) * 0.5)\n\nMathematically, they are identical. Numerically, they are equivalent in the sense that they lead to the same answer within the limits of finite precision:\n\nsum(s1-s2)\n\n[1] 0\n\nsum(s1-s3)\n\n[1] 0\n\n\nTo see which of the three methods is fastest, we can run a micro-benchmark with bench:mark():\n\nlibrary(bench)\nbm &lt;- mark(\n        sqrt(x),\n        x^0.5,\n        exp(log(x)*0.5),\n        time_unit='us',\n)\nbm\n\n# A tibble: 3 × 6\n  expression          min median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 sqrt(x)           0.902   1.23   619946.    7.86KB     62.0\n2 x^0.5             3.53    3.69   245918.    7.86KB     24.6\n3 exp(log(x) * 0.5) 5.45    5.90   166042.    7.86KB     33.2\n\nplot(bm, type=\"violin\")\n\n\n\n\n\n\n\nFigure 8.1: Results of benchmarking three ways of taking the square root of a vector.\n\n\n\n\n\nThe time_unit='us' option requests that all times are reported in microseconds. If you do not specify this parameter, the times can be reported in different units for the code alternatives. By default, bench::mark runs each expression for at least (min_time=) 0.5 seconds and for up to (max_iterations=) 10,000 iterations.\nThe built-in sqrt function is faster than the power computation and much faster than exponentiation the logs.\nThe violin plot of the results shows that the distribution of the run times is heavily skewed to the right (Figure 8.1). Some individual function calls can exceed the majority of the calls by orders of magnitude—note that the horizontal axis of the plot is on the log scale! You should avoid benchmarking based on the mean run time and instead use the median run time.\n\n\nVisual Profiler in RStudio\nWrapping code in system.time is convenient if you want to know the (user, system, elapsed) time for a chunk of code. Micro-benchmarking is helpful to evaluate alternative ways of writing expressions. To measure the performance of larger pieces of code, down to the level of function calls and individual lines of code we use a profiler.\nTo get a detailed analysis of how much time is spent on lines of user-written code, and how much memory is allocated/deallocated for those lines of code, you can use the profiler tool that is built into RStudio. You first need to load the profvis library, then wrap the code you wish to profile in a call to profvis({code-goes-here}). After the profiling run RStudio opens a window with two tabs. The flame graph provides a visual representation of the time spent in code execution, stacking functions that call each other. The Data tab provides a tabular breakdown—many users find that easier to comprehend than the flame graph.\n\nThe following example calls the profiler for code that computes the histogram, mean, and standard deviation of 1000 bootstrap samples of the trimmed mean in a sample of \\(n=20,000\\) observations from a \\(G(0,2)\\).\n\nlibrary(profvis)\n\nprofvis({\n\n  set.seed(542)\n  n &lt;- 20000\n  x &lt;- rnorm(n, mean=0, sd=sqrt(2))\n\n  bagger &lt;- function(x,b=1000) {\n      n &lt;- length(x)\n      estimates &lt;- rep(0,b)\n      for (i in 1:b) {\n          # Draw a bootstrap sample\n          bsample &lt;- sample(n,n,replace=TRUE)\n          # Compute the trimmed mean of the bootstrap sample\n          estimates[i] &lt;- mean(x[bsample],trim=0.15)\n      }\n      # Compute mean and standard deviation of the estimator\n      mn = mean(estimates)\n      sdev = sd(estimates)\n      hist(estimates)\n      return (list(B=b, mean=mn, sd=sdev))\n  }\n\n  bagger(x,b=1000)\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach block in the flame graph represents a call to a function, or possibly multiple calls to the same function. The width of the block is proportional to the amount of time spent in that function. When a function calls another function, a block is added on top of it in the flame graph. Memory reported in the Data view shows allocated memory as positive values and deallocated memory as negative values (in megabytes).\nThe profiler is a great tool for long-running code, and can point you at opportunities for optimizing execution time. However, you need to be aware of how it works and the resulting issues:\n\nThe profiler samples the code. Every few milliseconds the profiler stops the interpreters and records which function call it is in and traces back through the call stack. You will get different results from run to run, depending on which lines of code and which functions are being sampled. The variability in profiling results affects the functions the most that execute quickly—they might get sampled once or a few times in one profiling run and might get skipped in another run. Fortunately, those are the functions of least interest in code optimization—you focus on the code where the program spends most of the time.\nIf your code runs fast, the profiler might not hit any lines of code. On the other hand, you should be able to reliably hit the long-running parts of the code with the profiler.\nThe profiler does not record some built-in R functions or code written in other languages. Many routines are calling into C, C++, or even Fortran code. The profiler records an overall time for that code, but does not provide a breakdown.\nThe call stack might appear to be in a reverse order from the code. This is because Rs lazy execution model might call a routine only when it is needed, rather then where it is specified in the code.\nYou might see a GC entry in the profiler. This represents the garbage collector which frees resources no longer needed. Garbage collection is unpredictable, depends on the overall state of the system, and is also time consuming—it can throw off the profiling results if it occurs during a run. However, if you consistently find garbage collection in the profiling run, it can be a sign that the code is spending a lot of time freeing and reallocating memory. There will be corresponding memory allocations and this represents an opportunity to optimize the code to be more memory efficient. For example, instead of adding elements to lists inside a loop, pre-allocating the required memory and filling in the elements will trigger fewer reallocations of memory.\n\nThe following code adds elements to a list in a loop. R has to increase the size of the vector several times, leading to memory allocation and garbage collection (freeing) of previously allocated memory.\n\nprofvis({\n    x &lt;- integer()\n    for (i in 1:10000) {\n        x &lt;- c(x, i)\n    }\n}\n)\n\n\n\n\n\n\nYou could achieve the same result with a single memory allocation and without garbage collection with this code\n\nx &lt;- rep(1,10000)\n\nIf you fill values in a loop, pre-allocate the result vector to avoid garbage collection:\n\nx &lt;- integer(10000)\nfor (i in 1:length(x)) {x[i] = i}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#tracebacks",
    "href": "debugging.html#tracebacks",
    "title": "8  Debugging in R",
    "section": "8.2 Tracebacks",
    "text": "8.2 Tracebacks\n\nA traceback is a listing of the call stack of the program. Tracebacks are helpful to see which functions were called when an error occurs. Consider the following example from “Advanced R” by Wickham (2019). Function f calls function g which calls function h and so forth.\n\nf &lt;- function(a) g(a)\ng &lt;- function(b) h(b)\nh &lt;- function(c) i(c)\ni &lt;- function(d) {\n  if (!is.numeric(d)) {\n    stop(\"`d` must be numeric\", call. = FALSE)\n  }\n  d + 10\n}\n\nWhen we call with a non-numeric argument, an error occurs.\n\nf(\"a\")\n\nError: `d` must be numeric\n\n\nIf you are in RStudio, you can now click the “Show Traceback” button next to the error message (Figure 8.2).\n\n\n\n\n\n\nFigure 8.2: The Show Traceback icon in RStudio after an error occured\n\n\n\nAlternatively, you can see the traceback with the traceback() function.\n\ntraceback()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "debugging.html#the-interactive-debugger",
    "href": "debugging.html#the-interactive-debugger",
    "title": "8  Debugging in R",
    "section": "8.3 The Interactive Debugger",
    "text": "8.3 The Interactive Debugger\nThe RStudio IDE has an interactive debugging tool. Interactive debugging allows you to stop code execution at breakpoints, step through the code, interact with function evaluations, see and change the value of variables, and so forth. It is a valuable tool to find problems in programs and to learn how code works. It is highly recommended to step into the functions you frequently use and examine the code line by line—you learn a lot about how your favorite packages work.\nSuppose you want to run a classification model using adaptive boosting on the banana quality data. This data set contains ratings of the fruit quality (Good, Bad) and fruit attributes such as size, weight, sweetness, etc., for 4,000 bananas.\n\nlibrary(\"duckdb\")  \ncon &lt;- dbConnect(duckdb(),dbdir = \"ads.ddb\",read_only=TRUE)\nban_train &lt;- dbGetQuery(con, \"SELECT * FROM banana_train\") \ndbDisconnect(con)\n\nhead(ban_train)\n\n       Size      Weight   Sweetness    Softness HarvestTime    Ripeness\n1  1.706644 -0.03692624 -4.46344950 -1.51004720   4.5640225 -0.04171263\n2  3.703947  1.11884890 -3.04337740  0.02899801  -0.9705806 -1.46996620\n3 -3.888493  1.32609500  0.04608246  2.25944950   0.5068415  0.73710510\n4 -3.052952 -0.58796227 -1.63466790  1.04902060  -0.2098602 -1.81881750\n5  1.965954 -1.37867620 -3.14279270 -3.24607060  -0.6116899  1.81526600\n6  2.306789 -3.69348800 -0.04255614  0.70101670   2.7060213  3.96723440\n     Acidity Quality\n1  4.3266883    Good\n2 -0.5881153    Good\n3  2.5832198     Bad\n4  3.9573660     Bad\n5 -0.5000507     Bad\n6 -0.9244213    Good\n\n\nSuppose we want to debug the code for adaptive boosting in the ada package. The following statements load the library and invoke the ada::ada function on the banana training data. Prior to executing the code, execute debugonce(ada::ada) in the Console. This will set a breakpoint at the entry point of the function.\n\nlibrary(ada)\nadab &lt;- ada(Quality ~ ., \n            data=ban_train,\n            control=rpart.control(maxdepth=1, cp=-1, minsplit=0, xval=0)\n            )\n\nWhen the code above is executed, a browser window pops up when the breakpoint at ada::ada() is hit. The browser shows the R source code of the function and highlights the current line of the code (Figure 8.3). The frame of the Console window now shows five icons through which you can navigate the code (Figure 8.4).\n\n\n\n\n\n\nFigure 8.3: Hitting the breakpoint at the ada:ada entry point.\n\n\n\n\nNext (n): Execute the line of code and move to the next line\nStep Into (s): Step into the function called on the line you are on.\nStep Out (f): Run to the end of the current function (or loop) and step back out to the calling function\nContinue (c) : Stop interactive debugging and continue execution of the code\nStop (Q): Stop interactive debugging and terminate execution of the code\n\n\n\n\n\n\n\nFigure 8.4: Controls for moving through the code in the browser window.\n\n\n\nFigure 8.5 shows the result of stepping into the ada function. This brings you to the ada.formula function where the model expression is parsed. Advancing with several Next steps will land the cursor on the call to ada.default. If you now click on Step Into the source file for ada.default is loaded and you can advance through the function. If you click instead on Next, the call to ada.default function is completed and debugging resumes on line 22.\n\n\n\n\n\n\nFigure 8.5: About to execute the call to ada.default.\n\n\n\nDuring interactive debugging the Environment pane shows the values of the variables and objects in the code and the call stack (Figure 8.6).\n\n\n\n\n\n\nFigure 8.6: A view of the environment during interactive debugging.\n\n\n\nYou can invoke the interactive debugger through other methods as well:\n\nSetting a break point in the code by either clicking on the line number or with Shift-F9. I have found this method to not work reliably in .Rmd and .qmd documents. It does work in R scripts (.R files).\nInvoking the browser() command anywhere in the code.\nUsing debug or debugonce. The former sets a breakpoint that invokes the interactive debugger at every invocation of the function, until the undebug function removes the breakpoint. debugonce sets a breakpoint for single execution.\n\nDebugging works on the interpreted R statements. Many functions call complied code in other languages, C and C++, for example. The interactive RStudio debugger cannot step through this compiled code.\n\n\n\nFigure 8.2: The Show Traceback icon in RStudio after an error occured\nFigure 8.3: Hitting the breakpoint at the ada:ada entry point.\nFigure 8.4: Controls for moving through the code in the browser window.\nFigure 8.5: About to execute the call to ada.default.\nFigure 8.6: A view of the environment during interactive debugging.\n\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Debugging in `R`</span>"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "9  Reproducible Research and Data Analysis",
    "section": "",
    "text": "9.1 What, me Worry?\nReproducibility in research means to give others the ability to use the same materials as were used by an original investigator in an attempt to arrive at the same results. This is not the same as replicability, the ability to repeat a study, collect new data, and duplicate the results of an original investigator. To reproduce results, we need to have access to the same data, analytics, and code. To replicate results, we need to know exactly how a study or experiment was conducted, which methods and instruments of measurements were used, and so forth, in order to set up a new study or experiment.\nAs a first-year graduate student in statistics or data Science, why would you worry about reproducibility in research? You are unlikely to have your own research project that involves data collection. You are working on data analytics problems by yourself, why share your code with the world? You are planning to work as a data scientist in industry, why worry about concepts of reproducible research?\nFirst, reproducibility in research and data analysis is not about research; it is about the need to perform complex tasks in an organized workflow that builds trust, transparency, and accountability. Here are some reasons why you need to worry about reproducibility:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#what-me-worry",
    "href": "reproducibility.html#what-me-worry",
    "title": "9  Reproducible Research and Data Analysis",
    "section": "",
    "text": "You hardly ever work alone. Team members and supervisors need to be able to understand and judge your work.\nYou need to plan for someone else working on your stuff. Taking over someone else’s code can be a miserable experience. If that task falls on you, what information would you have liked to have to pick up the pieces?\nSoftware changes. Even if you are the only one working on the code—for now—do you ever made a mistake and wished you could revert back to a previous working version? Have you ever lost code or had to deal with incompatible versions on different machines? How do you prove (to yourself) that a change fixed or broke the code? How do you maintain the analyses for different versions of the same data?\nData and analytics are key to reproducibility. If you do not have the data or the program, you cannot validate much in today’s data-driven world. As a statistical programmer, you are at the center of someone else’s reproducibility story.\nUnless you work in purely methodological, theoretical work, as a statistician or data scientist you will be working on data problems of others. That might be an employer or a consulting client. Your work feeds into processes and systems they are accountable for. You need to ensure that there is accountability for your part of the work.\nMany analytic methods are inherently non-reproducible because they depend on stochastic elements. Making a statistical program reproducible means to understand all instances in which random numbers play a role (see Chapter 7). You might not be able to control all of them and need to decide when a sufficient level of reproducibility has been reached. It might be necessary to fix random number seeds, but sacrificing performance by turning off multi-threading is a bridge too far.\nReproducibility is not guaranteed by just reading the research paper. A common misconception is that methods sections in papers are sufficiently detailed to reproduce a result. Data analyses are almost never described in sufficient detail to be reproducible. Put yourself in the shoes of the reader and imagine to provide them with a data set and the pdf of the published paper. Would they be able to analyze the data and derive the effect sizes and \\(p\\)-values in Table 3 of the paper? Even a nod at the software used is not enough. “Analyses were performed with PROC MIXED in SAS.” So what?! There are a million ways to analyze an experiment incorrectly with PROC MIXED by fitting the wrong model, messing up comparisons, misinterpreting parameter estimates, etc.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#improving-reproducibility-of-statistical-programs",
    "href": "reproducibility.html#improving-reproducibility-of-statistical-programs",
    "title": "9  Reproducible Research and Data Analysis",
    "section": "9.2 Improving Reproducibility of Statistical Programs",
    "text": "9.2 Improving Reproducibility of Statistical Programs\nHere are some points to increase the reproducibility of your programs and data analysis\n\nHave a Plan\n\n\nDocument\n\n\nLiterate Programming\n\nWhat it is\nThe concept of literate programming was introduced by Knuth (1984) and refers to programs that are not written with a focus on the compiler but are organized in the order demanded by the logic of the programmer’s thoughts. In literate programming the program logic is written out in natural language, supplemented by code snippets or chunks. While literate programming was a concept at the time when Knuth proposed it, it is commonplace today, in particular in data science. Programming environments that combine markdown with code are a manifestation of literate programming: Jupyter Notebook, R Markdown, Quarto, and many more.\nLiterate programs are different from well-documented code or code that includes documentation as part of the file. The flow of these programs still follow the logic of the computer, not the logic of human thought. In literate programming the code follows the structure of the documentation. Chapter 10 is a literate program that implements from scratch an important iterative statistical algorithm: iteratively reweighted least squares (IRLS).\n\n\nComments in code\nIf you do not write a literate program you should add comments to your code. The purpose of comments is to express the intent of the code, not to explain what the code does. Explanatory comments are OK if the code is not obvious in some way. If you have a chance to refactor or rewrite nonobvious code, do not hesitate.\nComments should clarify why code is written a certain way and what the code is supposed to accomplish. If you feel that many lines of comments are needed to clarify some code, it can be an indication that the code should be simplified. If you are struggling to explain the intent of a function at the time you write it, imagine how difficult it is to divine that intent from the comment or code in six months or for a programmer not familiar with the code.\n\n\n\nVersion Control\n\n\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reproducible Research and Data Analysis</span>"
    ]
  },
  {
    "objectID": "irls_literate.html",
    "href": "irls_literate.html",
    "title": "10  Iteratively Reweighted Least Squares",
    "section": "",
    "text": "10.1 Generalized Linear Models\nThe generalization of GLMs is found in a comparison to the classical linear model with Gaussian errors: \\[\nY = \\textbf{x}^\\prime\\boldsymbol{\\beta}+ \\epsilon, \\quad \\epsilon \\sim \\textit{ iid } G(0, \\sigma^2)\n\\tag{10.1}\\]\nHere \\(Y\\) is the target variable of interest (dependent variable), \\(\\textbf{x}= [1, x_1,\\cdots,x_p]^\\prime\\) is a \\((p \\times 1)\\) vector of input variables, \\(\\boldsymbol{\\beta}\\) is the vector of parameters of the mean function, and \\(\\epsilon\\) is an error term. These error terms are independently and identically distributed as Gaussian random variables with mean zero and common variance \\(\\sigma^2\\).\nGLMs relax several elements of this model:\nHowever, the relaxation of the model conditions is not without limits. Rather than allowing \\(Y\\) to have any distribution, its distribution has to be a member of a special family of probability distributions known as the exponential family of distributions. Rather than allowing any arbitrary nonlinear relationship between inputs and target, only certain (invertible) transformations are permitted; the effect of the inputs remains linear on some scale, although it is usually not the scale of the mean. Rather than allowing any arbitrary variance, the variance of the targets can be unequal but it is determined through the distribution itself.\nThe specification of a GLM includes the following components:\nThe parameters \\(\\boldsymbol{\\beta}\\) in Equation 10.1 can be estimated by least squares in closed form. That means we can compute the parameter estimates directly. If \\(\\textbf{X}\\) is of full column rank, the ordinary least squares estimates is \\[\n\\widehat{\\boldsymbol{\\beta}} = \\left(\\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] and this is also the maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#generalized-linear-models",
    "href": "irls_literate.html#generalized-linear-models",
    "title": "10  Iteratively Reweighted Least Squares",
    "section": "",
    "text": "The distribution of \\(Y\\) does not have to be Gaussian.\nThe relationship between the inputs and the mean of \\(Y\\) does not have to be linear in the coefficients.\nThe model does not have an additive error structure.\nThe target variables do not have to have the same variance.\n\n\n\n\n\\(Y \\sim P_{expo}\\): \\(Y\\) follows a distribution in the exponential family, this family includes important distributions such as the Bernoulli, Binomial, Negative Binomial, Poisson, Geometric, Exponential, Gamma, Beta, Gaussian.\n\\(\\text{E}[Y] = \\mu\\)\n\\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\): a predictor \\(\\eta\\) that is linear in the parameters\n\\(g(\\mu) = \\eta\\): A transformation of the mean is linearly related to the parameters. The function \\(g()\\) is called the link function of the GLM and is invertible, that is, \\(\\mu = g^{-1}(\\eta)\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#iteratively-reweighted-least-squares",
    "href": "irls_literate.html#iteratively-reweighted-least-squares",
    "title": "10  Iteratively Reweighted Least Squares",
    "section": "10.2 Iteratively Reweighted Least Squares",
    "text": "10.2 Iteratively Reweighted Least Squares\nFor other GLMs, the maximum likelihood estimates have to be found numerically. McCullagh and Nelder Frs (1989) show that the following procedure, known as iteratively reweighted least squares, converges to the maximum likelihood estimates. To motivate the algorithm we start by a linear approximation (a first-order Taylor series) of the linked observations about an estimate of the mean: \\[\n\\begin{align*}\ng(y) &= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[ \\frac{\\partial g(y)}{\\partial y}\\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= g(\\widehat{\\mu}) + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}} \\\\\n&= \\widehat{\\eta} + (y-\\widehat{\\mu})\\left[\\frac{\\partial \\eta}{\\partial \\mu} \\right]_{\\vert_{\\widehat{\\mu}}}  \\\\\n&= z\n\\end{align*}\n\\tag{10.2}\\]\n\\(z\\) is called an adjusted dependent variable or a working response variable or a pseudo-response. The final expression in Equation 10.2 can be viewed as a linear model with response variable \\(z\\), systematic part \\(\\widehat{\\eta} = \\textbf{x}^\\prime\\widehat{\\boldsymbol{\\beta}}\\) and error term \\((y-\\mu)[\\partial \\eta/\\partial \\mu]\\). The variance of this error term is \\[\n\\text{Var}[z] = \\left[\\frac{\\partial \\eta}{\\partial \\mu}\\right]^2 \\text{Var}[Y]\n\\]\nThe iterative procedure is as follows: given an initial value of \\(z\\), which requires an initial estimate of \\(\\mu\\), fit a weighted linear model with inputs \\(\\textbf{x}= [x_1,\\cdots,x_p]^\\prime\\) and weights given by the inverse of \\(\\text{Var}[z]\\). The solution to the weighted linear model is an updated parameter vector \\(\\boldsymbol{\\beta}\\). Re-calculate \\(z\\) and the weights and repeat the weighted linear regression fit. Continue until the relative change in the parameter estimates, the log likelihood function, the deviance, or some other criterion is negligible.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#literate-programming",
    "href": "irls_literate.html#literate-programming",
    "title": "10  Iteratively Reweighted Least Squares",
    "section": "10.3 Literate Programming",
    "text": "10.3 Literate Programming\nIteratively reweighted least squares (IRLS) can be implemented without explicit specification of the probability distribution of the target variable \\(Y\\). All we need for the algorithm is the following:\n\nThe input variables \\(\\textbf{x}= [1, x_1, \\cdots, x_p]^\\prime\\) that form the linear predictor \\(\\eta = \\textbf{x}^\\prime\\boldsymbol{\\beta}\\)\nThe link function \\(g(\\mu) = \\eta\\) and its inverse \\(\\mu = g^{-1}(\\eta)\\)\nThe variance of an observation, \\(\\text{Var}[Y]\\). Note that in the exponential family the variance is related to the mean and the variance function is found as the derivative of the inverse canonical link function \\(b^\\prime(\\theta) = \\mu\\). The details are not important here, we can always find the variance as a function of the mean \\(\\mu\\) if we know which member of the exponential family we are dealing with.\nThe derivative of the linear predictor \\(\\eta\\) with respect to the mean \\(\\mu\\).\n\nTo compute the log likelihood of the data for a particular value of the parameter estimates, we also need to know the distribution and need to evaluate the log likelihood function.\n\nThe Data\nThe data for this application is the Bikeshare data that comes with the ISLR2 library and contains 8,645 records of the number of bike rentals per hour in Washington, DC along with time/date and weather information.\n\nlibrary(ISLR2)\nstr(Bikeshare)\n\n'data.frame':   8645 obs. of  15 variables:\n $ season    : num  1 1 1 1 1 1 1 1 1 1 ...\n $ mnth      : Factor w/ 12 levels \"Jan\",\"Feb\",\"March\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ day       : num  1 1 1 1 1 1 1 1 1 1 ...\n $ hr        : Factor w/ 24 levels \"0\",\"1\",\"2\",\"3\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ holiday   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ weekday   : num  6 6 6 6 6 6 6 6 6 6 ...\n $ workingday: num  0 0 0 0 0 0 0 0 0 0 ...\n $ weathersit: Factor w/ 4 levels \"clear\",\"cloudy/misty\",..: 1 1 1 1 1 2 1 1 1 1 ...\n $ temp      : num  0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.2 0.24 0.32 ...\n $ atemp     : num  0.288 0.273 0.273 0.288 0.288 ...\n $ hum       : num  0.81 0.8 0.8 0.75 0.75 0.75 0.8 0.86 0.75 0.76 ...\n $ windspeed : num  0 0 0 0 0 0.0896 0 0 0 0 ...\n $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...\n $ registered: num  13 32 27 10 1 1 0 2 7 6 ...\n $ bikers    : num  16 40 32 13 1 1 2 3 8 14 ...\n\n\nThe target variable is bikers, the total number of daily bikers. Because this is a count variable on a per unit (daily) basis, we implement a Poisson GLM with a log link. The input variables of interest are the factors month (mnth), weather situation (weathersit) and the temperature (temp).\n\n\nStarting Values\nSince the algorithm is iterative, we need to get it off the ground with starting values. From Equation 10.2 we can spy two different approaches for starting the iterations: with an initial value \\(\\boldsymbol{\\beta}^{(0)}\\) from which we calculate \\(\\mu^{(0)} =  g^{-1})(\\textbf{x}^\\prime\\boldsymbol{\\beta}^{(0)})\\) or with an initial value \\(\\mu^{(0)}\\) determined independently from \\(\\boldsymbol{\\beta}\\).\nThe second approach works if the data can be evaluated at the link function, possibly after a small adjustment. With a log link we need to worry about whether any of the counts are zero.\n\ninitial &lt;- function(values, link=\"log\", c=0.5) {\n    if (!is.null(values) && !anyNA(values)) {\n        if (toupper(link)==\"LOG\") {\n            if (sum(values == 0)) {\n                eta0 &lt;- log(ifelse(values==0,c,values))\n            } else {\n                eta0 &lt;- log(values)\n            }\n        }\n        return(eta0)\n    } else {\n        return (NULL)\n    }\n}\n\n\n\nFunctions\nSome additional functions will help evaluate the necessary pieces of the working variable \\(z\\) and the weights in the linear model step. The function dmu_deta returns the derivative of the mean with respect to the linear predictor, evaluated at the current estimates: \\[\n\\frac{\\partial \\mu}{\\partial \\eta} \\biggr\\vert_{\\widehat{\\mu}}\n\\] It is easy to extend that function to handle other link functions. The function get_mu returns the mean of the response based on the link function and the linear predictor. The function get_var computes the variance of the target variable based on the distribution in the exponential family, the function get_z constructs the working response variable, and get_w computes the weight for the reweighted least squares step.\n\nget_var &lt;- function(mu, dist=\"Poisson\") {\n    if (toupper(dist)==\"POISSON\") {\n        return (mu)\n    }\n    return (NULL)\n}\n\nget_mu &lt;- function(eta, link=\"log\") {\n    if (toupper(link)==\"LOG\") {\n        return (exp(eta))\n    }\n    return (NULL)\n}\n\ndeta_dmu &lt;- function(eta, link=\"log\") {\n    if (toupper(link)==\"LOG\") {\n        return(detadmu = 1/exp(eta))\n    }\n    return (NULL)\n}\n\nget_z &lt;- function(y, eta, link) {\n    if (is.null(y) || is.null(eta)) {\n        stop(\"null values not allowed\")\n    }\n    if (anyNA(y) || anyNA(eta)) {\n        stop(\"cannot handle missing values\")\n    }\n    z &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\n    return(z)\n}\n\n# The weight for the linear model is the inverse of the variance of\n# the working variable z.\nget_w &lt;- function(eta, link=\"log\", dist=\"poisson\") {\n    var_z &lt;- deta_dmu(eta,link)^2 * get_var(get_mu(eta,link),dist)\n    return (1/var_z)    \n}\n\n\n\nIterations\nThe iterations can be carried out in a loop. The algorithm stops when the max number of iterations is exceeded or when the largest relative absolute change in a parameter estimates between iterations \\(t+1\\) and \\(t\\) is smaller than a tolerance: \\[\n\\frac{\\max\\left\\{|\\widehat{\\beta}_j^{(t)} - \\widehat{\\beta}_j^{(t-1)}|\\right\\}}\n{\\max\\left\\{|\\widehat{\\beta}_j^{(t)}|,|\\widehat{\\beta}_j^{(t-1)}|\\right\\}} \\le \\text{tol}\n\\] The function converged checks whether the relative parameter convergence citerion is met:\n\nconverged &lt;- function(newbeta,oldbeta,tol=1e-6) {\n    diff &lt;- abs(newbeta - oldbeta)\n    denom &lt;- pmax(abs(newbeta),abs(oldbeta))\n    return (max(diff/denom) &lt; tol)\n}\n\nIn that case we say that the iterations have converged to a solution and report the parameter estimates.\nThe iterations call lm.wfit to fit a weighted linear model with model matrix \\(\\textbf{X}\\), target vector \\(\\textbf{z}= [z_1,\\cdots,z_n]^\\prime\\) and weight vector \\(\\textbf{w}= [w_1,\\cdots,w_n]^\\prime\\).\n\nmaxiter &lt;- 50\ntol &lt;- 1e-6\nY &lt;- Bikeshare$bikers\nX &lt;- model.matrix( ~ mnth + weathersit + temp, data = Bikeshare)\neta &lt;- initial(Y,\"log\")\n\nfor (iter in 1:maxiter) {\n    z &lt;- get_z(Y,eta,\"log\")\n    w &lt;- get_w(eta,\"log\",\"Poisson\")\n    # Fit a weighted linear model with response z, model X, and weight w\n    linreg &lt;- lm.wfit(X,z,w)\n    # Check whether the model has converged\n    if ((iter &gt; 1) && converged(linreg$coefficients,beta,tol)) {\n        beta &lt;- linreg$coefficients\n        cat(\"Converged after \", iter, \"iterations\")\n        print(beta)\n        break\n    } else {\n        beta &lt;- linreg$coefficients\n    }\n    # update eta for the next go-around\n    eta &lt;- linreg$fitted.values\n}\n\nConverged after  6 iterations              (Intercept)                   mnthFeb                 mnthMarch \n              3.367063899              -0.046719502              -0.006319815 \n                mnthApril                   mnthMay                  mnthJune \n             -0.109689766              -0.139946963              -0.428625482 \n                 mnthJuly                   mnthAug                  mnthSept \n             -0.714615564              -0.523849543              -0.213759334 \n                  mnthOct                   mnthNov                   mnthDec \n              0.163239847               0.242305655               0.321518995 \n   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow \n             -0.077249678              -0.474060776              -0.529583958 \n                     temp \n              3.391086355 \n\n\nThe algorithm converges after six iterations, not counting the initial setup. The parameter estiamtes can be validated with the glm function.\n\nmod.pois &lt;- glm(bikers ~ mnth + weathersit + temp,\n                data = Bikeshare, \n                family = \"poisson\")\nmod.pois$coefficients\n\n              (Intercept)                   mnthFeb                 mnthMarch \n              3.367063899              -0.046719502              -0.006319815 \n                mnthApril                   mnthMay                  mnthJune \n             -0.109689766              -0.139946963              -0.428625482 \n                 mnthJuly                   mnthAug                  mnthSept \n             -0.714615564              -0.523849543              -0.213759334 \n                  mnthOct                   mnthNov                   mnthDec \n              0.163239847               0.242305655               0.321518995 \n   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow \n             -0.077249678              -0.474060776              -0.529583958 \n                     temp \n              3.391086355",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "irls_literate.html#to-do",
    "href": "irls_literate.html#to-do",
    "title": "10  Iteratively Reweighted Least Squares",
    "section": "10.4 To Do",
    "text": "10.4 To Do\nYou can improve on the above implementation in a number of ways and you should give it a try:\n\nMonitor the log likelihood or deviance instead or in addition to the parameter estimates.\nReport the log likelihood, null deviance, and residual deviance at convergence.\nCompute approximate standard errors and approximate significance tests for the coefficients.\nStart the iterations from a vector of initial parameters \\(\\boldsymbol{\\beta}^{(0)}\\).\nExtend the program to other link functions and distributions.\nHow would you accommodate a Binomial(\\(n,\\pi\\)) distribution?\nRewrite the code in terms of family() objects, see ?family() for details. As an example, poisson()$mu.eta reports the function that computes \\(\\partial \\mu /\\partial \\eta\\) for the default link function of the poisson family.\nAdd an offset variable to the generalized linear model.\nAccommodate a scale parameter (two-parameter exponential family).\n\n\n\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear Models, 2nd Ed. Chapman & Hall, New York.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Iteratively Reweighted Least Squares</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021.\nModern Data Science with r, 2nd Ed. Chapman & Hall/CRC\nPress. https://mdsr-book.github.io/mdsr3e/.\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Date with\nYour Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.\n\n\nBox, G. E. P., G. M. jenkins, and G. C. Reinsel. 1976. Time Series\nAnalysis, Forecasting and Control. 3rd. Ed. Holden-Day.\n\n\nBox, G. E. P., and M. E. Muller. 1958. “A Note on the Generation\nof Normal Random Deviates.” Annals of Mathematical\nStatistics 29: 610–11. doi:10.1214/aoms/1177706645.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for\nVisualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nKnuth, D. E. 1984. “Literate Programming.”\nThe Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMatsumoto, M., and T. Nishimura. 1998. “Mersenne Twister: A\n623-Dimensionally Equidistributed Uniform Pseudo-Random Number\nGenerator.” ACM Transactions on Modeling and Computer\nSimulation 8: 3–30.\n\n\nMcCullagh, P., and J. A. Nelder Frs. 1989. Generalized Linear\nModels, 2nd Ed. Chapman & Hall, New York.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with\nPandas, NumPy and Jupyter, 3rd Ed. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook. O’Reilly\nMedia. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC\nPress. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data\nScience: Import, Tidy, Transform, Visualize, and Model Data, 2nd\nEd. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown:\nThe Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R\nMarkdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "References"
    ]
  }
]