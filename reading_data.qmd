
# Reading Data {#sec-reading-data}

Before you can work with a set of data, you have to bring it into the programming environment. 

If you are working with proprietary data analytics tools such as SAS, SPSS, Stata, etc., you probably do not worry much about the ways in which data are stored, read, and written by the software, except that you want the tool to support import from and export to your favorite non-proprietary format.

Most statistics and data science today is performed on (a) data in databases and (b) data stored in open-source or open standard file formats. Proprietary formats are not easily exchanged between tools and applications and are increasingly frowned upon. Data science spans many technologies and storing data in a form accessible by only a subset of the tools is counterproductive. You end up reformatting the data into an exchangeable format at some point anyway.

The most important file formats in data science are CSV, JSON, and Parquet, but also Avro and ORC. CSV and JSON are text-based, human-readable formats, whereas Parquet, Avro, and ORC are binary formats. The last three were designed specifically to handle Big Data use cases and are Apache open-source projects.

The second important source for data are databases. They store the data in a form specific to the database engine and you interact with the data through a database management system (DBMS). The syntax of the DBMS is typically a SQL (Structured Querying Language; pronounced "sequel", like the follow-up movie) if the database architecture is relational. Even No-SQL non-relational databases such as MongoDB have a specific syntax to interface with the information in the database. In other words, when your data is stored in a database you need to use database-specific tools to read and write the data. In the case of analytic databases you should also use the computational power of the database engine to perform analytic calculations in the database.

## Tabular Data

The basic function to read tabular data into `R` is `read.table`. `read.csv` is a special case of `read.table` for CSV files with defaults such as `sep=","` that make sense for CSV (comma-separated values) files.

:::{.callout-important title="On CSV Files"}
OK, time for a rant. CSV files are ubiquitous and going through college one could get the impression that most data is stored in CSV format. First, that does not hold for the real world. Second, CSV is a horrible format for data. It does have some advantages

-   **Ubiquitous**: every data tool can read and write CSV files. It is thus a common format to exchange (export/import) data between tools and applications.

-   **Human readable**: since the column names and values are stored in plain text, it is easy to look at the contents of a CSV file. When data are stored in binary form, you need to know exactly how the data are laid out in the file to access it.

-   **Compression**: it is easy to compress CSV files.

-   **Excel**: CSV files are easily exported from and imported to Microsoft Excel.

-   **Simple**: the structure of the files is straightforward to understand and can represent tabular data well if the data types can be converted to text characters.

There are some considerable disadvantages of CSV files, however:

-   **Human readable**: To prevent exposing the contents of the file you need to use access controls and/or encryption. It is not a recommended file format for sensitive data.

-   **Simple structure**: Complex data types such as documents with multiple fields and sub-fields cannot be stored in CSV files.

-   **Plain text**: Some data types cannot be represented as plain text, for example, images, audio, and video. If you kluge binary data into a text representation the systems writing and reading the data need to know how to kluge and un-kluge the information—it is not a recommended practice.

-   **Efficiency**: much more efficient formats for storing data exist, especially for large data sets.

-   **Broken**: CSV files can be easily broken by applications. Examples include inserting line breaks, limiting line width, not handling embedded quotes correctly, blank lines.

-   **Missing values** (NaNs): The writer and reader of CSV files need to agree how to represent missing values and values representing not-a-number. Inconsistency between writing and reading these values can have disastrous consequences For example, it is a bad but common practice to code missing values with special numbers such as 99999 (called "sentinel values"). How does the application reading the file know this is the code for a missing value?

-   **Encodings**: When CSV files contain more than plain ASCII text, for example, emojis or Unicode characters, the file cannot be read without knowing the correct encoding (UTF-8, UTF-16, EBCDIC, US-ASCII, etc.). Storing encoding information in the header section of the CSV file throws off CSV reader software that does not anticipate the extra information.

-   **Metadata**: The only metadata supported by the CSV format are the column names in the first row of the file. This information is optional and you will find CSV files without column names. Additional metadata common about columns in a table such as data types, format masks, number-to-string maps, cannot be stored in a CSV file.

-   **Data Types**: Data types need to be inferred by the CSV reader software when scanning the file. There is no metadata in the CSV header to identify data types, only column names.

-   **Loss of Precision**: Floating point numbers are usually stored in CSV files with fewer decimal places than their internal representation in the computer. A double-precision floating point number occupies 64-bits (8 bytes) and has 15 digits of precision. Although it is not necessary, floating-point numbers are often rounded or truncated when they are converted to plain text.

Despite these drawbacks, CSV is one of the most common file formats. It is the lowest common denominator format to exchange data between disparate systems.
:::

Here is an example of reading a CSV file into `R`. The `stringsAsFactors=TRUE` argument requests that all character variables are converted into factors.

```{r}
pisa <- read.csv(file="data/pisa.csv",stringsAsFactors=TRUE)
pisa[1:10,]
```
Note that there are missing values for `Chinese Taipei` in row 4. This is the result of correctly specifying the contents of the comma-separated file. For this row the entries of the CSV files read

```{.default}
"Chinese Taipei",560,12.8,37.2,523,523,,,
```

The sequence of commas indicates that the values for the corresponding variables are unobserved and will be set to missing.

By default, `read.csv` looks for information about the column names in the first row of the CSV file. If names are not available in the first row, add the `header=FALSE` option to the function call. Other important options for `read.csv` and `read.table` are

- `sep`: specifies the character that separates the values of the columns, for `read.csv` this defaults to `sep=","`. For `read.table` the separator defaults to `sep=""`.
- `skip`: how many lines to skip at the top of the file before reading data. This is useful if the CSV file has a comment section at the top.
- `nrow`: how many rows of data to read. Useful if you want to import only a portion of the file.
- `dec`: specifies the character that indicates a decimal point. 
- `na.strings`: a vector of character values that are interpreted as missing (NA) values. I hope you do not need to specify those---using sentinel values to indicate missing values is very dangerous.

## Excel Files

The `readxl` library is part of the `tidyverse`.

```{r, message=FALSE}
library(readxl)
df_tesla <- read_excel("data/TeslaDeaths.xlsx", sheet=1)
df_tesla[1:10,]
```

## JSON Files {#sec-json-files}

[JSON](https://www.json.org/json-en.html) stands for JavaScript Object Notation, and although it was borne out of interoperability concerns for JavaScript applications, it is a language-agnostic data format. Initially used to pass information in human readable form between applications over APIs (Application Programmer Interfaces), JSON has grown into a general-purpose format for text-based, structured information. It is the standard for communicating data on the web. The correct pronunciation of JSON is like the name “Jason”, but “JAY-sawn” has become common.

In contrast to CSV, JSON is not based on rows of data but three basic data elements:

-   **Value**: a string, number, reserved word, or one of the following:

-   **Object**: a collection of name—value pairs similar to a key-value store.

-   **Array**: An ordered list of values

All modern programming languages support key—values and arrays, they might be calling it by different names (object, record, dictionary, struct, list, sequence, map, hash table, ...). This makes JSON documents highly interchangeable between programming languages---JSON documents are easy to parse (read) and write by computers. Any modern data processing system can read and write JSON data, making it a frequent choice to share data between systems and applications.

A value in JSON can be a string in double quotes, a number, true, false, or null, an object or an array (@fig-data-json-structure). An array is an ordered collection of values. Objects are unordered collection of name—value pairs. Since values can contain objects and arrays, JSON allows highly nested data structures that do not fit the tabular row--column structure of CSV files.

![Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.](images/JSON_Structure.png){#fig-data-json-structure .lightbox fig-align="center" width="80%"}

JSON documents are self-describing, the schema to make the data intelligible is built into the structures. It is also a highly flexible format that does not impose any structure on the data, except that it must comply with the JSON rules and data types.

![A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.](images/JSON_Example.png){#fig-data-json-example .lightbox fig-align="center" width="80%"}

As a human-readable, non-binary format, JSON shares some of the advantages and disadvantages with CSV files. You do not want to pass sensitive information in JSON format without encryption. The level of human readability is lower for JSON files. The format is intended to make algorithms interoperable, not to make human interpretation simple.

Since so much data is stored in JSON format, you need to get familiar and comfortable with working with JSON files. Data science projects are more likely consumers of JSON files rather than producer of files. 

<!---
download.file("https://raw.githubusercontent.com/datacarpentry/r-socialsci/main/episodes/data/SAFI.json","data/SAFI.json", mode = "wb")
--->

There are multiple packages for working with JSON files in `R`, for example, `jsonlite` and `rjson`.  Here we use the `jsonlite` package. The `toJSON` and `fromJSON` functions are used to convert `R` objects to/from JSON. `read_json` and `write_json` read and write JSON files. They are similar to `fromJSON` and `toJSON` but recognize a file path as input.

The following example is taken from [Datacarpentry.org](https://datacarpentry.org/r-socialsci/07-json.html). SAFI (Studying African Farmer-Led Irrigation) is a study of farming and irrigation methods in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017.

If you use `read_json` with the default settings, the JSON document is converted into a list (`simplifyVector=FALSE`). To convert JSON format into vectors and data frames, use `simplifyVector=TRUE`.

```{r readjson, warning=FALSE, message=FALSE}
library(jsonlite)
json_data <- read_json("data/SAFI.json", simplifyVector=TRUE)
```

To take a look at the data frame created with `read_json` we use the `glimpse` function from `tidyverse`. It works a bit like the `str` function in base `R` but has a more compact display for this case and shows more data points.

```{r glimpse, warning=FALSE, message=FALSE}
library(tidyverse)
glimpse(json_data)
```

Because of the deeply nested structure of JSON documents, flattening the data into a two-dimensional data frame can go only so far. Several of the columns are lists and some are lists of data frames.

```{r}
json_data %>%
    select(where(is.list)) %>%
    glimpse()
```
You access the data frames stored within the lists simply as any other list element in `R`.

```{r}
str(json_data$F_liv[[2]])
json_data$F_liv[[2]]
```


## Parquet Files

The [Apache Parquet](https://parquet.apache.org/) open-source file format is a binary format---data are not stored in plain text but in binary form. Originally conceived as a column-based file format in the Hadoop ecosystem, it has become popular as a general file format for analytical data inside and outside of Hadoop and its file system HDFS: for example, as an efficient analytic file format for data exported to data lakes or in data processing with Spark. Many organizations have switched to storing their data in Parquet files; loading Parquet files from AWS S3 buckets or from Google Cloud Storage or Microsoft Azure Blob storage has become a common access pattern.

Working with Parquet files for large data is an order of magnitude faster than working with CSV files. The drawbacks of CSV files discussed previously all melt away with Parquet files.

Parquet was designed from the ground up with complex data structures and read-heavy analytics in mind. It uses principally columnar storage but does it cleverly by storing chunks of columns in row groups rather than entire columns.

![The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. [Source](https://towardsdatascience.com/parquet-file-format-everything-you-need-to-know-4eed5c0019e7): Parquet File Format: Everything You Need to Know, by Nikola Ilic.](images/ParquetStorage.png){#fig-data-parquet .lightbox fig-align="center" width="80%"}

This hybrid storage model is very efficient when queries select specific columns and filter rows at the same time; a common pattern in data science: compute the correlation between `homeValue` and `NumberOfRooms` for homes where `ZipCode = 24060`.

Parquet stores metadata about the row chunks to speed access to rows, the metadata tells the reader which row chunks to skip. Also, a single write to the Parquet format can generate multiple `.parquet` files. The total data is divided into multiple files collected within a folder. Like NoSQL and NewSQL databases, data are partitioned, but since Parquet is a file format and not a database engine, the partitioning results in multiple files. This is advantageous for parallel processing frameworks like Spark that can work on multiple partitions (files) concurrently.

Parquet uses several compression techniques to reduce the size of the files such as run-length encoding, dictionary encoding, Snappy, GZip, LZO, LZ4, ZSTD. Because of columnar storage, compression methods can be specified on a per-column basis; Parquet files compress much more than text-oriented CSV files.

Because of its complex file structure, Parquet files are relatively slow to write. The file format is optimized for the WORM paradigm: write-once, read many times.

|                             |        CSV        |     JSON      |    Parquet    |
|-----------------------------|:-----------------:|:-------------:|:-------------:|
| **Columnar**                |        No         |      No       |      Yes      |
| **Compression**             |        Yes        |      Yes      |      Yes      |
| **Human Readable**          |        Yes        |      Yes      |      No       |
| **Nestable**                |        No         |      Yes      |      Yes      |
| **Complex Data Structures** |        No         |      Yes      |      Yes      |
| **Named Columns**           | Yes, if in header | Based on scan | Yes, metadata |
| **Data Types**              |   Based on scan   | Based on scan | Yes, metadata |

: Comparison of popular file formats in data science.

To read a file in Parquet format into an `R` session, install and load the [`arrow`](https://arrow.apache.org/docs/r/) package. 
[Apache Arrow](https://arrow.apache.org/docs/index.html) is an open-source development platform for in-memory analytics that supports many programming environments, including R and Python. The arrow libraries support reading and writing of the important file formats in this ecosystem.

```{r read_parquet, eval=FALSE}
#install.packages("arrow")
library(arrow)
read_parquet("somefile.parquet")
```


## Working with a Database

You interface with a database from `R` with the `DBI` package. It provides a common syntax and functions to connect to supported databases, and to send queries to the database. In addition to the `DBI` package you need the driver packages specific to the database you are working with (`RMySQL` for MySQL, `RSQLite` for SQLite, and so on). 

My favorite database for analytic work is duckDB, a highly efficient, embedded database designed for processing data analytically. The file `ads.ddb`, contains the database tables used in this and other courses in duckDB format. To add the duckDB driver package to your system, use

```{r, eval=FALSE}
install.packages("duckdb")
```

To read a table from that database into an `R` data frame, follow the steps in the next code snippet:

```{r, warning=FALSE, message=FALSE}
library("duckdb")                                           # <1>
con <- dbConnect(duckdb(),dbdir = "ads.ddb",read_only=TRUE) # <2>
fit <- dbGetQuery(con, "SELECT * FROM fitness")             # <3>
dbDisconnect(con)                                           # <4>
```
1. Load the `duckdb` library
2. Make a connection to duckDB through the DBI interface, specifying the database you want to work with, here `ads.ddb`
3. Send a `SELECT * FROM` query to read the contents of the target table and assign the result to a dataframe in `R`
4. Close the connection to the database when you are done querying it.

We can now work with the `fitness` data set as a dataframe in `R`:

```{r}
head(fit)
summary(fit)
```

The connection to the database can be left open until you are done working with the database. If you use the database to import tables at the beginning of a statistical program, it is recommended to close the connection as soon as that step is complete. 

Because we are reading many tables from the `ads.ddb` database, you can write a function to wrap the database operations. The following function loads the `duckdb` library unless it is already loaded, and reads a table, possibly selecting rows with a WHERE filter, and returns the `R` dataframe.

```{r, duckload}
duckload <- function(tableName, whereClause=NULL, dbName="ads.ddb") {
    if (!is.null(tableName)) {
        if (!("duckdb" %in% (.packages()))) {
            suppressWarnings(libary("duckdb"))
        }
        con <- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)
        query_string <- paste("SELECT * from ", tableName)
        if (!is.null(whereClause)) {
            query_string <- paste(query_string, " WHERE ", whereClause)
        }
        df_ <- dbGetQuery(con, query_string)
        dbDisconnect(con)
        return (df_)
    } else {
        return (NULL)
    }
}
```


```{r}
# Load the entire table
fit <- duckload("fitness")

# Load only the records where Age > 50
fit2 <- duckload("fitness",whereClause="Age > 50")

# Load only the records where Age = 40
fit3 <- duckload("fitness","Age = 40")
```

You can modify the function to load a **projection** of columns from the database table by replacing the `*` in the SELECT query with the selected columns.

## Saving and Loading `RData`

`R` has its own format (binary or ASCII) to read and write `R` objects. This is convenient to save a data frame to disk and later load it back into an `R` session. 

To save any `R` object, use the `save` or `saveRDS` function, to load it into an `R` session use the `load` function.
`save` can save one or more objects to a `.RData` file, while `saveRDS` creates an `.RDS` file from a single object.

```{r}
a <- matrix(rnorm(200),nrow=50,ncol=4)
b <- crossprod(a)
cor_mat <- cor(a)
save(a,b,cor_mat,file="data/matrixStuff.RData")
```

```{r}
load("data/matrixStuff.RData")
```

When you load `R` objects from a `.RData` file, you do not assign the result of the function. The objects are created in the work environment with their original names. 

:::{.callout-caution}
The `load` operation will overwrite any `R` objects in your environment by the same name as those in the `RData` file.  
:::

`.RData` files can contain many objects, for example, all the objects in your environment when it is saved upon closing an `R` session. If you load from an `.RDS` file, you can assign the result to an `R` object which helps avoid name collisions.

In general, it is better coding practice to use `saveRDS` and `readRDS` with single objects. The code is cleaner and easier to follow if single objects are saved/loaded and these are explicitly named in the code.

```{.default}
a = rnorm(100)
saveRDS(a, file = "stuff.RDS") 

normal_rvs <- readRDS("stuff.RDS")
```

